{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe043c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1512"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob(\"results/batch_*/run_*.json\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64bb33a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_extra = glob(\"results/redo/**/run_*.json\")\n",
    "len(files_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "631163c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_files = files + files_extra\n",
    "len(total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d76ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "collection = defaultdict(list)\n",
    "for file in total_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    collection[str(data[\"model\"] or \"unknown\")].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d6674ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grok-4 400\n",
      "gpt-5 400\n",
      "gemini-2.5-pro 400\n",
      "claude-sonnet-4-5 400\n"
     ]
    }
   ],
   "source": [
    "for model, file in collection.items():\n",
    "    print(model, len(file))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19936fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_model_succeed(f):\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    is_success = all(scenario[\"success\"] for scenario in data[\"scenarios\"][0][\"verifiers\"])\n",
    "    return is_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98a29c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOTAL SUCCESS RATES BY MODEL\n",
      "================================================================================\n",
      "\n",
      "grok-4:\n",
      "  Total runs: 400\n",
      "  Successful: 359\n",
      "  Failed: 41\n",
      "  Success rate: 89.75%\n",
      "\n",
      "gpt-5:\n",
      "  Total runs: 400\n",
      "  Successful: 319\n",
      "  Failed: 81\n",
      "  Success rate: 79.75%\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  Total runs: 400\n",
      "  Successful: 299\n",
      "  Failed: 101\n",
      "  Success rate: 74.75%\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  Total runs: 400\n",
      "  Successful: 361\n",
      "  Failed: 39\n",
      "  Success rate: 90.25%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'grok-4': {'total': 400, 'success': 359, 'failed': 41, 'success_rate': 89.75},\n",
       " 'gpt-5': {'total': 400, 'success': 319, 'failed': 81, 'success_rate': 79.75},\n",
       " 'gemini-2.5-pro': {'total': 400,\n",
       "  'success': 299,\n",
       "  'failed': 101,\n",
       "  'success_rate': 74.75},\n",
       " 'claude-sonnet-4-5': {'total': 400,\n",
       "  'success': 361,\n",
       "  'failed': 39,\n",
       "  'success_rate': 90.25}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate total success rate per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TOTAL SUCCESS RATES BY MODEL\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "model_success = {}\n",
    "for model, files_list in collection.items():\n",
    "    total_runs = len(files_list)\n",
    "    successful_runs = sum(1 for f in files_list if did_model_succeed(f))\n",
    "    success_rate = (successful_runs / total_runs * 100) if total_runs > 0 else 0\n",
    "    \n",
    "    model_success[model] = {\n",
    "        \"total\": total_runs,\n",
    "        \"success\": successful_runs,\n",
    "        \"failed\": total_runs - successful_runs,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    \n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Total runs: {total_runs}\")\n",
    "    print(f\"  Successful: {successful_runs}\")\n",
    "    print(f\"  Failed: {total_runs - successful_runs}\")\n",
    "    print(f\"  Success rate: {success_rate:.2f}%\")\n",
    "    print()\n",
    "\n",
    "model_success\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2914a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UNDERSTANDING FILE NAMING CONVENTION\n",
      "================================================================================\n",
      "\n",
      "Sample file names (grok-4):\n",
      "  run_task6_2_c1_p1_r8_v1_gpt5_v1_harness-grok-4-1.json\n",
      "  run_task24_8_c1_p1_r8_v1_gpt_v1_harness-grok-4-2.json\n",
      "  run_task21_10_c1_p1_r8_v6_harness-grok-4-6.json\n",
      "  run_task2_7_c1_p1_r8_v2_harness-grok-4-7.json\n",
      "  run_task10_7_c1_p1_r8_v3_harness-grok-4-6.json\n",
      "  run_task26_8_c1_p1_r8_v1_gpt_v1_harness-grok-4-3.json\n",
      "  run_new_sys_task_task16_6_c1_p1_r8_v3_harness-grok-4-7.json\n",
      "  run_task7_2_c1_p1_r8_v9_gpt5_v1_harness-grok-4-5.json\n",
      "  run_task7_2_c1_p1_r8_v9_gpt5_v1_harness-grok-4-4.json\n",
      "  run_new_sys_task_task16_6_c1_p1_r8_v3_harness-grok-4-6.json\n",
      "\n",
      "================================================================================\n",
      "PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Testing extraction on sample files:\n",
      "  Test: task6_2_c1_p1_r8_v1_gpt5_v1_harness, Run: 1\n",
      "  Test: task24_8_c1_p1_r8_v1_gpt_v1_harness, Run: 2\n",
      "  Test: task21_10_c1_p1_r8_v6_harness, Run: 6\n"
     ]
    }
   ],
   "source": [
    "# Understand the file naming convention for multiple runs\n",
    "import os\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"UNDERSTANDING FILE NAMING CONVENTION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Sample some files to understand the pattern\n",
    "sample_files = collection[\"grok-4\"][:10]\n",
    "print(\"Sample file names (grok-4):\")\n",
    "for f in sample_files:\n",
    "    basename = os.path.basename(f)\n",
    "    print(f\"  {basename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PATTERN ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# The pattern appears to be: run_<test_name>-<model>-<run_number>.json\n",
    "# Let's verify this and extract the test name\n",
    "\n",
    "def extract_test_info(filepath, model):\n",
    "    \"\"\"Extract test name and run number from filepath.\n",
    "    \n",
    "    Pattern: run_{test_name}-{model_name}-{run_number}.json\n",
    "    Example: run_new_sys_task10_3_c1_p1_r8_v2_harness-claude-sonnet-4-5-1.json\n",
    "    \n",
    "    The challenge is that model names contain hyphens (grok-4, gpt-5, gemini-2.5-pro, claude-sonnet-4-5)\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filepath)\n",
    "    \n",
    "    # Remove 'run_' prefix\n",
    "    if basename.startswith('run_'):\n",
    "        basename = basename[4:]\n",
    "    \n",
    "    # Remove '.json' suffix\n",
    "    if basename.endswith('.json'):\n",
    "        basename = basename[:-5]\n",
    "    \n",
    "    # The pattern is: {test_name}-{model}-{run_number}\n",
    "    # We know the model, so we can split on it\n",
    "    # But we need to be careful because model might be in the test name too\n",
    "    \n",
    "    # Strategy: find the model name followed by a hyphen and single/double digit number\n",
    "    import re\n",
    "    \n",
    "    # Escape special chars in model name for regex\n",
    "    model_escaped = re.escape(model)\n",
    "    \n",
    "    # Pattern: anything, then model, then hyphen, then 1-2 digits at the end\n",
    "    pattern = f'^(.+)-{model_escaped}-(\\\\d+)$'\n",
    "    match = re.match(pattern, basename)\n",
    "    \n",
    "    if match:\n",
    "        test_name = match.group(1)\n",
    "        run_number = match.group(2)\n",
    "        return test_name, run_number\n",
    "    \n",
    "    # Fallback: just extract last number\n",
    "    match = re.match(r'(.+)-(\\d+)$', basename)\n",
    "    if match:\n",
    "        test_name = match.group(1)\n",
    "        run_number = match.group(2)\n",
    "        return test_name, run_number\n",
    "    \n",
    "    return basename, \"unknown\"\n",
    "\n",
    "# Test the extraction\n",
    "print(\"Testing extraction on sample files:\")\n",
    "for f in sample_files[:3]:\n",
    "    test_name, run_num = extract_test_info(f, \"grok-4\")\n",
    "    print(f\"  Test: {test_name}, Run: {run_num}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6791213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUCCESS RATES PER TASK PER MODEL\n",
      "================================================================================\n",
      "\n",
      "Summary: Found unique tasks per model:\n",
      "  grok-4: 50 unique tasks\n",
      "  gpt-5: 50 unique tasks\n",
      "  gemini-2.5-pro: 50 unique tasks\n",
      "  claude-sonnet-4-5: 50 unique tasks\n",
      "\n",
      "================================================================================\n",
      "Verifying 8 runs per task:\n",
      "================================================================================\n",
      "\n",
      "\n",
      "grok-4:\n",
      "  ✅ All tasks have exactly 8 runs\n",
      "\n",
      "gpt-5:\n",
      "  ✅ All tasks have exactly 8 runs\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  ✅ All tasks have exactly 8 runs\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  ✅ All tasks have exactly 8 runs\n"
     ]
    }
   ],
   "source": [
    "# Group runs by task and analyze success per task per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"SUCCESS RATES PER TASK PER MODEL\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Structure: task_results[model][task_name] = {\"total\": X, \"success\": Y, \"runs\": [...]}\n",
    "def create_task_stats():\n",
    "    return {\"total\": 0, \"success\": 0, \"runs\": []}\n",
    "\n",
    "task_results = defaultdict(lambda: defaultdict(create_task_stats))\n",
    "\n",
    "for model, files_list in collection.items():\n",
    "    for file_path in files_list:\n",
    "        test_name, run_num = extract_test_info(file_path, model)\n",
    "        is_success = did_model_succeed(file_path)\n",
    "        \n",
    "        task_results[model][test_name][\"total\"] += 1\n",
    "        if is_success:\n",
    "            task_results[model][test_name][\"success\"] += 1\n",
    "        task_results[model][test_name][\"runs\"].append({\n",
    "            \"run_number\": run_num,\n",
    "            \"success\": is_success,\n",
    "            \"file\": file_path\n",
    "        })\n",
    "\n",
    "# Display summary\n",
    "print(f\"Summary: Found unique tasks per model:\")\n",
    "for model in task_results.keys():\n",
    "    unique_tasks = len(task_results[model])\n",
    "    print(f\"  {model}: {unique_tasks} unique tasks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Verifying 8 runs per task:\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for model, tasks in task_results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    non_eight_count = 0\n",
    "    for task_name, stats in sorted(tasks.items()):\n",
    "        if stats[\"total\"] != 8:\n",
    "            non_eight_count += 1\n",
    "            print(f\"  ⚠️  {task_name}: {stats['total']} runs (expected 8)\")\n",
    "    \n",
    "    if non_eight_count == 0:\n",
    "        print(f\"  ✅ All tasks have exactly 8 runs\")\n",
    "    else:\n",
    "        print(f\"  ❌ {non_eight_count} tasks don't have 8 runs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ad77208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAILED TASK SUCCESS RATES\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CLAUDE-SONNET-4-5\n",
      "================================================================================\n",
      "\n",
      "❌ new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 1, 6, 7, 8, 4, 5, 2, 3\n",
      "\n",
      "❌ new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 3, 2, 5, 4, 8, 7, 6, 1\n",
      "\n",
      "❌ new_sys_task9_7_c1_p1_r8_v3_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 1, 6, 7, 4, 8, 5, 2, 3\n",
      "\n",
      "⚠️  new_sys_task6_6_c1_p1_r8_v3_harness\n",
      "   Success: 4/8 (50.0%)\n",
      "   Failed runs: 8, 3, 1, 7\n",
      "\n",
      "⚠️  new_sys_task2_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 1, 5, 4\n",
      "\n",
      "⚠️  new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 1, 8\n",
      "\n",
      "⚠️  new_sys_task5_3_c1_p1_r8_v1_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 2, 7\n",
      "\n",
      "⚠️  claude-sonnet-4-5\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 2\n",
      "\n",
      "⚠️  new_sys_task15_3_c1_p1_r8_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 2\n",
      "\n",
      "⚠️  new_sys_task1_8_c1_p1_r8_v4_gpt5_v2_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 7\n",
      "\n",
      "⚠️  new_sys_task8_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 8\n",
      "\n",
      "✅ new_sys_task10_3_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task11_4_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task11_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task12_3_c1_p1_r8_v4_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task13_4_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task15_1_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task16_7_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_2_c1_p1_r8_v1_gpt_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_3_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_5_c1_p1_r8_v7_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_6_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_9_c1_p1_r8_v8_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task20_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task21_8_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task23_6_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task27_4_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task2_2_c1_p1_r8_v4_gpt5_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_2_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_4_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_8_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_4_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_9_c1_p1_r8_v6_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task5_2_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task5_4_c1_p1_r8_v15_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task6_3_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task7_6_c1_p1_r8_v1\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task_task16_6_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_task14_7_c1_p1_r8_v1_2_gpt2_5\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task10_7_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task1_10_c1_p1_r8_v5_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task21_10_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task24_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task26_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task2_7_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task6_2_c1_p1_r8_v1_gpt5_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task7_2_c1_p1_r8_v9_gpt5_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GEMINI-2.5-PRO\n",
      "================================================================================\n",
      "\n",
      "❌ gemini-2.5-pro\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 3, 2, 5, 8, 4, 7, 6, 1\n",
      "\n",
      "❌ new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 1, 7, 6, 5, 8, 4, 3, 2\n",
      "\n",
      "❌ new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 4, 8, 5, 2, 3, 1, 6, 7\n",
      "\n",
      "❌ new_sys_task7_6_c1_p1_r8_v1\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 1, 7, 6, 5, 4, 8, 3, 2\n",
      "\n",
      "❌ new_sys_task8_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 1/8 (12.5%)\n",
      "   Failed runs: 7, 6, 5, 8, 4, 3, 2\n",
      "\n",
      "❌ new_sys_task16_7_c1_p1_r8_v2_harness\n",
      "   Success: 2/8 (25.0%)\n",
      "   Failed runs: 4, 8, 3, 2, 7, 6\n",
      "\n",
      "❌ new_sys_task4_9_c1_p1_r8_v6_gpt_v1_harness\n",
      "   Success: 2/8 (25.0%)\n",
      "   Failed runs: 1, 7, 6, 8, 4, 3\n",
      "\n",
      "❌ new_sys_task6_6_c1_p1_r8_v3_harness\n",
      "   Success: 2/8 (25.0%)\n",
      "   Failed runs: 1, 6, 4, 5, 2, 3\n",
      "\n",
      "❌ new_task14_7_c1_p1_r8_v1_2_gpt2_5\n",
      "   Success: 2/8 (25.0%)\n",
      "   Failed runs: 6, 1, 2, 3, 4, 5\n",
      "\n",
      "⚠️  new_sys_task20_9_c1_p1_r8_v2_harness\n",
      "   Success: 4/8 (50.0%)\n",
      "   Failed runs: 1, 7, 5, 2\n",
      "\n",
      "⚠️  new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "   Success: 4/8 (50.0%)\n",
      "   Failed runs: 4, 8, 2, 1\n",
      "\n",
      "⚠️  new_sys_task4_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 4/8 (50.0%)\n",
      "   Failed runs: 6, 7, 1, 3\n",
      "\n",
      "⚠️  new_sys_task1_5_c1_p1_r8_v7_gpt_v1_harness\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 1, 5, 3\n",
      "\n",
      "⚠️  new_sys_task1_8_c1_p1_r8_v4_gpt5_v2_harness\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 2, 8, 7\n",
      "\n",
      "⚠️  task6_2_c1_p1_r8_v1_gpt5_v1_harness\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 4, 8, 7\n",
      "\n",
      "⚠️  new_sys_task11_4_c1_p1_r8_v2_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 5, 1\n",
      "\n",
      "⚠️  new_task13_7_c1_p1_r8_1_gpt2_5\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 1, 2\n",
      "\n",
      "⚠️  task1_10_c1_p1_r8_v5_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 1, 2\n",
      "\n",
      "⚠️  new_sys_task10_3_c1_p1_r8_v2_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 8\n",
      "\n",
      "⚠️  new_sys_task13_4_c1_p1_r8_v3_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 3\n",
      "\n",
      "⚠️  new_sys_task21_8_c1_p1_r8_v6_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 4\n",
      "\n",
      "⚠️  new_sys_task23_6_c1_p1_r8_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 8\n",
      "\n",
      "⚠️  new_sys_task27_4_c1_p1_r8_v6_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 3\n",
      "\n",
      "⚠️  new_sys_task2_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 6\n",
      "\n",
      "⚠️  new_sys_task3_2_c1_p1_r8_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 8\n",
      "\n",
      "⚠️  new_sys_task5_2_c1_p1_r8_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 2\n",
      "\n",
      "⚠️  new_sys_task6_3_c1_p1_r8_v3_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 4\n",
      "\n",
      "⚠️  task26_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 8\n",
      "\n",
      "⚠️  task7_2_c1_p1_r8_v9_gpt5_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 6\n",
      "\n",
      "✅ new_sys_task11_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task12_3_c1_p1_r8_v4_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task15_1_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task15_3_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_2_c1_p1_r8_v1_gpt_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_3_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_6_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_9_c1_p1_r8_v8_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task2_2_c1_p1_r8_v4_gpt5_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_4_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_8_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_4_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task5_3_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task5_4_c1_p1_r8_v15_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task_task16_6_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task10_7_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task21_10_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task24_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task2_7_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GPT-5\n",
      "================================================================================\n",
      "\n",
      "❌ new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 5, 4, 8, 3, 2, 1, 7, 6\n",
      "\n",
      "❌ new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 2, 3, 4, 8, 5, 6, 7, 1\n",
      "\n",
      "❌ new_sys_task9_7_c1_p1_r8_v3_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 1, 6, 7, 8, 4, 5, 2, 3\n",
      "\n",
      "❌ new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "   Success: 1/8 (12.5%)\n",
      "   Failed runs: 1, 6, 7, 4, 8, 2, 3\n",
      "\n",
      "❌ task24_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 1/8 (12.5%)\n",
      "   Failed runs: 2, 3, 8, 5, 6, 7, 1\n",
      "\n",
      "❌ task6_2_c1_p1_r8_v1_gpt5_v1_harness\n",
      "   Success: 1/8 (12.5%)\n",
      "   Failed runs: 2, 4, 8, 5, 6, 7, 1\n",
      "\n",
      "❌ new_sys_task1_6_c1_p1_r8_v1_harness\n",
      "   Success: 3/8 (37.5%)\n",
      "   Failed runs: 2, 3, 8, 7, 1\n",
      "\n",
      "⚠️  new_sys_task6_6_c1_p1_r8_v3_harness\n",
      "   Success: 4/8 (50.0%)\n",
      "   Failed runs: 7, 5, 2, 3\n",
      "\n",
      "⚠️  task1_10_c1_p1_r8_v5_harness\n",
      "   Success: 4/8 (50.0%)\n",
      "   Failed runs: 8, 4, 3, 1\n",
      "\n",
      "⚠️  new_sys_task1_2_c1_p1_r8_v1_gpt_v2_harness\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 8, 6, 7\n",
      "\n",
      "⚠️  new_sys_task15_3_c1_p1_r8_v1_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 8, 7\n",
      "\n",
      "⚠️  new_sys_task3_2_c1_p1_r8_v1_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 7, 1\n",
      "\n",
      "⚠️  new_sys_task3_9_c1_p1_r8_v2_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 8, 6\n",
      "\n",
      "⚠️  new_sys_task6_3_c1_p1_r8_v3_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 3, 7\n",
      "\n",
      "⚠️  task7_2_c1_p1_r8_v9_gpt5_v1_harness\n",
      "   Success: 6/8 (75.0%)\n",
      "   Failed runs: 2, 6\n",
      "\n",
      "⚠️  new_sys_task10_3_c1_p1_r8_v2_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 3\n",
      "\n",
      "⚠️  new_sys_task1_9_c1_p1_r8_v8_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 3\n",
      "\n",
      "⚠️  new_sys_task2_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 8\n",
      "\n",
      "⚠️  new_sys_task4_9_c1_p1_r8_v6_gpt_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 3\n",
      "\n",
      "⚠️  new_sys_task5_2_c1_p1_r8_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 5\n",
      "\n",
      "⚠️  new_sys_task5_3_c1_p1_r8_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 7\n",
      "\n",
      "⚠️  new_sys_task7_6_c1_p1_r8_v1\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 6\n",
      "\n",
      "⚠️  new_sys_task8_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 4\n",
      "\n",
      "⚠️  new_task14_7_c1_p1_r8_v1_2_gpt2_5\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 5\n",
      "\n",
      "⚠️  task10_7_c1_p1_r8_v3_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 2\n",
      "\n",
      "✅ new_sys_task11_4_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task11_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task12_3_c1_p1_r8_v4_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task13_4_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task15_1_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task16_7_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_3_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_5_c1_p1_r8_v7_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_8_c1_p1_r8_v4_gpt5_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task20_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task21_8_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task23_6_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task27_4_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task2_2_c1_p1_r8_v4_gpt5_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_4_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_8_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_4_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task5_4_c1_p1_r8_v15_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task_task16_6_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_task13_7_c1_p1_r8_1_gpt2_5\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task21_10_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task26_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task2_7_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GROK-4\n",
      "================================================================================\n",
      "\n",
      "❌ new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 6, 7, 1, 2, 3, 4, 8, 5\n",
      "\n",
      "❌ new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 7, 6, 1, 3, 2, 5, 4, 8\n",
      "\n",
      "❌ new_sys_task9_7_c1_p1_r8_v3_harness\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 1, 6, 7, 4, 8, 5, 2, 3\n",
      "\n",
      "❌ new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "   Success: 2/8 (25.0%)\n",
      "   Failed runs: 5, 2, 3, 1, 6, 7\n",
      "\n",
      "❌ task6_2_c1_p1_r8_v1_gpt5_v1_harness\n",
      "   Success: 2/8 (25.0%)\n",
      "   Failed runs: 7, 5, 4, 8, 3, 2\n",
      "\n",
      "⚠️  new_sys_task13_4_c1_p1_r8_v3_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 6\n",
      "\n",
      "⚠️  new_sys_task15_3_c1_p1_r8_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 2\n",
      "\n",
      "⚠️  new_sys_task1_6_c1_p1_r8_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 2\n",
      "\n",
      "⚠️  task1_10_c1_p1_r8_v5_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 6\n",
      "\n",
      "⚠️  task26_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 3\n",
      "\n",
      "✅ new_sys_task10_3_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task11_4_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task11_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task12_3_c1_p1_r8_v4_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task15_1_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task16_7_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_2_c1_p1_r8_v1_gpt_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_3_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_5_c1_p1_r8_v7_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_8_c1_p1_r8_v4_gpt5_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task1_9_c1_p1_r8_v8_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task20_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task21_8_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task23_6_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task27_4_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task2_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task2_2_c1_p1_r8_v4_gpt5_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_2_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_4_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_8_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task3_9_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_4_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task4_9_c1_p1_r8_v6_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task5_2_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task5_3_c1_p1_r8_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task5_4_c1_p1_r8_v15_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task6_3_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task6_6_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task7_6_c1_p1_r8_v1\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task8_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_sys_task_task16_6_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_task13_7_c1_p1_r8_1_gpt2_5\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ new_task14_7_c1_p1_r8_v1_2_gpt2_5\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task10_7_c1_p1_r8_v3_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task21_10_c1_p1_r8_v6_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task24_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task2_7_c1_p1_r8_v2_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task7_2_c1_p1_r8_v9_gpt5_v1_harness\n",
      "   Success: 8/8 (100.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed success rate per task per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"DETAILED TASK SUCCESS RATES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for model in sorted(task_results.keys()):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    tasks = task_results[model]\n",
    "    \n",
    "    # Sort tasks by success rate (lowest first to highlight problems)\n",
    "    sorted_tasks = sorted(tasks.items(), key=lambda x: (x[1][\"success\"] / x[1][\"total\"], x[0]))\n",
    "    \n",
    "    for task_name, stats in sorted_tasks:\n",
    "        success_rate = (stats[\"success\"] / stats[\"total\"] * 100) if stats[\"total\"] > 0 else 0\n",
    "        \n",
    "        # Use different symbols based on success rate\n",
    "        if success_rate == 100:\n",
    "            symbol = \"✅\"\n",
    "        elif success_rate >= 50:\n",
    "            symbol = \"⚠️ \"\n",
    "        else:\n",
    "            symbol = \"❌\"\n",
    "        \n",
    "        print(f\"{symbol} {task_name}\")\n",
    "        print(f\"   Success: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
    "        \n",
    "        # Show which specific runs failed\n",
    "        failed_runs = [r for r in stats[\"runs\"] if not r[\"success\"]]\n",
    "        if failed_runs:\n",
    "            failed_run_nums = [r[\"run_number\"] for r in failed_runs]\n",
    "            print(f\"   Failed runs: {', '.join(failed_run_nums)}\")\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba69fdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING RESULTS TO CSV\n",
      "================================================================================\n",
      "\n",
      "✅ Exported: model_summary.csv\n",
      "✅ Exported: task_results_detailed.csv\n",
      "✅ Exported: run_details.csv\n",
      "\n",
      "================================================================================\n",
      "EXPORT COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  1. model_summary.csv - Overall model statistics\n",
      "  2. task_results_detailed.csv - Success rates per task per model\n",
      "  3. run_details.csv - Individual run details\n"
     ]
    }
   ],
   "source": [
    "# Export results to CSV for analysis\n",
    "import csv\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"EXPORTING RESULTS TO CSV\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Export 1: Summary by model\n",
    "with open(\"model_summary.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Total Runs\", \"Successful\", \"Failed\", \"Success Rate %\"])\n",
    "    \n",
    "    for model, stats in sorted(model_success.items()):\n",
    "        writer.writerow([\n",
    "            model,\n",
    "            stats[\"total\"],\n",
    "            stats[\"success\"],\n",
    "            stats[\"failed\"],\n",
    "            f\"{stats['success_rate']:.2f}\"\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: model_summary.csv\")\n",
    "\n",
    "# Export 2: Detailed task results\n",
    "with open(\"task_results_detailed.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Task\", \"Total Runs\", \"Successful\", \"Failed\", \"Success Rate %\", \"Failed Run Numbers\"])\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        for task_name, stats in sorted(task_results[model].items()):\n",
    "            success_rate = (stats[\"success\"] / stats[\"total\"] * 100) if stats[\"total\"] > 0 else 0\n",
    "            failed_runs = [r for r in stats[\"runs\"] if not r[\"success\"]]\n",
    "            failed_run_nums = \", \".join([r[\"run_number\"] for r in failed_runs])\n",
    "            \n",
    "            writer.writerow([\n",
    "                model,\n",
    "                task_name,\n",
    "                stats[\"total\"],\n",
    "                stats[\"success\"],\n",
    "                stats[\"total\"] - stats[\"success\"],\n",
    "                f\"{success_rate:.1f}\",\n",
    "                failed_run_nums\n",
    "            ])\n",
    "\n",
    "print(\"✅ Exported: task_results_detailed.csv\")\n",
    "\n",
    "# Export 3: Per-run details\n",
    "with open(\"run_details.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Task\", \"Run Number\", \"Success\", \"File Path\"])\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        for task_name, stats in sorted(task_results[model].items()):\n",
    "            for run in stats[\"runs\"]:\n",
    "                writer.writerow([\n",
    "                    model,\n",
    "                    task_name,\n",
    "                    run[\"run_number\"],\n",
    "                    \"Yes\" if run[\"success\"] else \"No\",\n",
    "                    run[\"file\"]\n",
    "                ])\n",
    "\n",
    "print(\"✅ Exported: run_details.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPORT COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  1. model_summary.csv - Overall model statistics\")\n",
    "print(f\"  2. task_results_detailed.csv - Success rates per task per model\")\n",
    "print(f\"  3. run_details.csv - Individual run details\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "86b008f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-MODEL TASK DIFFICULTY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total unique tasks: 52\n",
      "\n",
      "MOST DIFFICULT TASKS (lowest success rate):\n",
      "================================================================================\n",
      "\n",
      "1. gemini-2.5-pro\n",
      "   Overall success rate: 0.0%\n",
      "   Per model:\n",
      "     - gemini-2.5-pro: 0/8 (0.0%)\n",
      "\n",
      "2. new_sys_task9_7_c1_p1_r8_v3_harness\n",
      "   Overall success rate: 0.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 0/8 (0.0%)\n",
      "     - gpt-5: 0/8 (0.0%)\n",
      "     - grok-4: 0/8 (0.0%)\n",
      "\n",
      "3. new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "   Overall success rate: 0.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 0/8 (0.0%)\n",
      "     - gemini-2.5-pro: 0/8 (0.0%)\n",
      "     - gpt-5: 0/8 (0.0%)\n",
      "     - grok-4: 0/8 (0.0%)\n",
      "\n",
      "4. new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "   Overall success rate: 6.2%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 0/8 (0.0%)\n",
      "     - gemini-2.5-pro: 0/8 (0.0%)\n",
      "     - gpt-5: 0/8 (0.0%)\n",
      "     - grok-4: 2/8 (25.0%)\n",
      "\n",
      "5. new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "   Overall success rate: 34.4%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 6/8 (75.0%)\n",
      "     - gemini-2.5-pro: 4/8 (50.0%)\n",
      "     - gpt-5: 1/8 (12.5%)\n",
      "     - grok-4: 0/8 (0.0%)\n",
      "\n",
      "6. task6_2_c1_p1_r8_v1_gpt5_v1_harness\n",
      "   Overall success rate: 50.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 5/8 (62.5%)\n",
      "     - gpt-5: 1/8 (12.5%)\n",
      "     - grok-4: 2/8 (25.0%)\n",
      "\n",
      "7. new_sys_task6_6_c1_p1_r8_v3_harness\n",
      "   Overall success rate: 56.2%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 4/8 (50.0%)\n",
      "     - gemini-2.5-pro: 2/8 (25.0%)\n",
      "     - gpt-5: 4/8 (50.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "8. new_sys_task7_6_c1_p1_r8_v1\n",
      "   Overall success rate: 71.9%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 0/8 (0.0%)\n",
      "     - gpt-5: 7/8 (87.5%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "9. new_sys_task8_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Overall success rate: 71.9%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 7/8 (87.5%)\n",
      "     - gemini-2.5-pro: 1/8 (12.5%)\n",
      "     - gpt-5: 7/8 (87.5%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "10. task1_10_c1_p1_r8_v5_harness\n",
      "   Overall success rate: 78.1%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 6/8 (75.0%)\n",
      "     - gpt-5: 4/8 (50.0%)\n",
      "     - grok-4: 7/8 (87.5%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EASIEST TASKS (highest success rate):\n",
      "================================================================================\n",
      "\n",
      "1. new_sys_task12_3_c1_p1_r8_v4_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "2. new_sys_task5_4_c1_p1_r8_v15_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "3. new_sys_task2_2_c1_p1_r8_v4_gpt5_v1_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "4. new_sys_task11_9_c1_p1_r8_v2_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "5. task2_7_c1_p1_r8_v2_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "6. new_sys_task1_3_c1_p1_r8_v2_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "7. new_sys_task1_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "8. new_sys_task15_1_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "9. task21_10_c1_p1_r8_v6_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n",
      "10. new_sys_task4_4_c1_p1_r8_v3_harness\n",
      "   Overall success rate: 100.0%\n",
      "   Per model:\n",
      "     - claude-sonnet-4-5: 8/8 (100.0%)\n",
      "     - gemini-2.5-pro: 8/8 (100.0%)\n",
      "     - gpt-5: 8/8 (100.0%)\n",
      "     - grok-4: 8/8 (100.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross-model task comparison - which tasks are hardest?\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"CROSS-MODEL TASK DIFFICULTY ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Get all unique tasks across all models\n",
    "all_tasks = set()\n",
    "for model, tasks in task_results.items():\n",
    "    all_tasks.update(tasks.keys())\n",
    "\n",
    "print(f\"Total unique tasks: {len(all_tasks)}\\n\")\n",
    "\n",
    "# Build comparison matrix\n",
    "task_comparison = {}\n",
    "for task in all_tasks:\n",
    "    task_comparison[task] = {}\n",
    "    total_success = 0\n",
    "    total_runs = 0\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            task_comparison[task][model] = {\n",
    "                \"success\": stats[\"success\"],\n",
    "                \"total\": stats[\"total\"],\n",
    "                \"rate\": (stats[\"success\"] / stats[\"total\"] * 100) if stats[\"total\"] > 0 else 0\n",
    "            }\n",
    "            total_success += stats[\"success\"]\n",
    "            total_runs += stats[\"total\"]\n",
    "        else:\n",
    "            task_comparison[task][model] = {\"success\": 0, \"total\": 0, \"rate\": 0}\n",
    "    \n",
    "    task_comparison[task][\"overall_rate\"] = (total_success / total_runs * 100) if total_runs > 0 else 0\n",
    "\n",
    "# Sort tasks by overall difficulty (lowest success rate first)\n",
    "sorted_tasks_by_difficulty = sorted(task_comparison.items(), key=lambda x: x[1][\"overall_rate\"])\n",
    "\n",
    "print(\"MOST DIFFICULT TASKS (lowest success rate):\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, (task, stats) in enumerate(sorted_tasks_by_difficulty[:10], 1):\n",
    "    print(f\"{i}. {task}\")\n",
    "    print(f\"   Overall success rate: {stats['overall_rate']:.1f}%\")\n",
    "    print(f\"   Per model:\")\n",
    "    for model in sorted(task_results.keys()):\n",
    "        model_stats = stats[model]\n",
    "        if model_stats[\"total\"] > 0:\n",
    "            print(f\"     - {model}: {model_stats['success']}/{model_stats['total']} ({model_stats['rate']:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EASIEST TASKS (highest success rate):\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, (task, stats) in enumerate(reversed(sorted_tasks_by_difficulty[-10:]), 1):\n",
    "    print(f\"{i}. {task}\")\n",
    "    print(f\"   Overall success rate: {stats['overall_rate']:.1f}%\")\n",
    "    print(f\"   Per model:\")\n",
    "    for model in sorted(task_results.keys()):\n",
    "        model_stats = stats[model]\n",
    "        if model_stats[\"total\"] > 0:\n",
    "            print(f\"     - {model}: {model_stats['success']}/{model_stats['total']} ({model_stats['rate']:.1f}%)\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97f1fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING TASK COMPARISON MATRIX\n",
      "================================================================================\n",
      "\n",
      "✅ Exported: task_comparison_matrix.csv\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Summary of findings:\n",
      "  - Total models: 4\n",
      "  - Total unique tasks: 52\n",
      "  - Total runs analyzed: 1600\n",
      "\n",
      "Success rates by model:\n",
      "  claude-sonnet-4-5: 90.25% (361/400)\n",
      "  grok-4: 89.75% (359/400)\n",
      "  gpt-5: 79.75% (319/400)\n",
      "  gemini-2.5-pro: 74.75% (299/400)\n",
      "\n",
      "Exported files:\n",
      "  1. model_summary.csv\n",
      "  2. task_results_detailed.csv\n",
      "  3. run_details.csv\n",
      "  4. task_comparison_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# Export task comparison matrix\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"EXPORTING TASK COMPARISON MATRIX\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "with open(\"task_comparison_matrix.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Header\n",
    "    models = sorted(task_results.keys())\n",
    "    header = [\"Task\", \"Overall Success Rate %\"]\n",
    "    for model in models:\n",
    "        header.extend([f\"{model} Success\", f\"{model} Total\", f\"{model} Rate %\"])\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Sort by overall rate (hardest first)\n",
    "    for task, stats in sorted_tasks_by_difficulty:\n",
    "        row = [task, f\"{stats['overall_rate']:.1f}\"]\n",
    "        \n",
    "        for model in models:\n",
    "            model_stats = stats[model]\n",
    "            row.extend([\n",
    "                model_stats[\"success\"],\n",
    "                model_stats[\"total\"],\n",
    "                f\"{model_stats['rate']:.1f}\" if model_stats[\"total\"] > 0 else \"N/A\"\n",
    "            ])\n",
    "        \n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"✅ Exported: task_comparison_matrix.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Summary of findings:\")\n",
    "print(f\"  - Total models: {len(task_results)}\")\n",
    "print(f\"  - Total unique tasks: {len(all_tasks)}\")\n",
    "print(f\"  - Total runs analyzed: {sum(m['total'] for m in model_success.values())}\")\n",
    "print(f\"\\nSuccess rates by model:\")\n",
    "for model, stats in sorted(model_success.items(), key=lambda x: x[1]['success_rate'], reverse=True):\n",
    "    print(f\"  {model}: {stats['success_rate']:.2f}% ({stats['success']}/{stats['total']})\")\n",
    "\n",
    "print(f\"\\nExported files:\")\n",
    "print(f\"  1. model_summary.csv\")\n",
    "print(f\"  2. task_results_detailed.csv\")\n",
    "print(f\"  3. run_details.csv\")\n",
    "print(f\"  4. task_comparison_matrix.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fbc69e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT DIFFICULTY CATEGORIZATION BY MODEL\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CLAUDE-SONNET-4-5\n",
      "================================================================================\n",
      "\n",
      "✅ EASY (0-1 failures): 43 prompts\n",
      "⚠️  MEDIUM (2-4 failures): 4 prompts\n",
      "❌ HARD (5-8 failures): 3 prompts\n",
      "\n",
      "Total prompts: 50\n",
      "\n",
      "Distribution:\n",
      "  Easy: 86.0%\n",
      "  Medium: 8.0%\n",
      "  Hard: 6.0%\n",
      "\n",
      "❌ HARD prompts (showing up to 5):\n",
      "  - new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "    Failures: 8/8\n",
      "  - new_sys_task9_7_c1_p1_r8_v3_harness\n",
      "    Failures: 8/8\n",
      "  - new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "    Failures: 8/8\n",
      "\n",
      "⚠️  MEDIUM prompts (showing up to 3):\n",
      "  - new_sys_task6_6_c1_p1_r8_v3_harness\n",
      "    Failures: 4/8\n",
      "  - new_sys_task2_1_c1_p1_r8_v1_claude_v1_harness\n",
      "    Failures: 3/8\n",
      "  - new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "    Failures: 2/8\n",
      "\n",
      "================================================================================\n",
      "GEMINI-2.5-PRO\n",
      "================================================================================\n",
      "\n",
      "✅ EASY (0-1 failures): 32 prompts\n",
      "⚠️  MEDIUM (2-4 failures): 9 prompts\n",
      "❌ HARD (5-8 failures): 9 prompts\n",
      "\n",
      "Total prompts: 50\n",
      "\n",
      "Distribution:\n",
      "  Easy: 64.0%\n",
      "  Medium: 18.0%\n",
      "  Hard: 18.0%\n",
      "\n",
      "❌ HARD prompts (showing up to 5):\n",
      "  - gemini-2.5-pro\n",
      "    Failures: 8/8\n",
      "  - new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "    Failures: 8/8\n",
      "  - new_sys_task7_6_c1_p1_r8_v1\n",
      "    Failures: 8/8\n",
      "  - new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "    Failures: 8/8\n",
      "  - new_sys_task8_1_c1_p1_r8_v1_claude_v1_harness\n",
      "    Failures: 7/8\n",
      "\n",
      "⚠️  MEDIUM prompts (showing up to 3):\n",
      "  - new_sys_task20_9_c1_p1_r8_v2_harness\n",
      "    Failures: 4/8\n",
      "  - new_sys_task4_1_c1_p1_r8_v1_claude_v1_harness\n",
      "    Failures: 4/8\n",
      "  - new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "    Failures: 4/8\n",
      "\n",
      "================================================================================\n",
      "GPT-5\n",
      "================================================================================\n",
      "\n",
      "✅ EASY (0-1 failures): 35 prompts\n",
      "⚠️  MEDIUM (2-4 failures): 8 prompts\n",
      "❌ HARD (5-8 failures): 7 prompts\n",
      "\n",
      "Total prompts: 50\n",
      "\n",
      "Distribution:\n",
      "  Easy: 70.0%\n",
      "  Medium: 16.0%\n",
      "  Hard: 14.0%\n",
      "\n",
      "❌ HARD prompts (showing up to 5):\n",
      "  - new_sys_task9_7_c1_p1_r8_v3_harness\n",
      "    Failures: 8/8\n",
      "  - new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "    Failures: 8/8\n",
      "  - new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "    Failures: 8/8\n",
      "  - task24_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "    Failures: 7/8\n",
      "  - task6_2_c1_p1_r8_v1_gpt5_v1_harness\n",
      "    Failures: 7/8\n",
      "\n",
      "⚠️  MEDIUM prompts (showing up to 3):\n",
      "  - task1_10_c1_p1_r8_v5_harness\n",
      "    Failures: 4/8\n",
      "  - new_sys_task6_6_c1_p1_r8_v3_harness\n",
      "    Failures: 4/8\n",
      "  - new_sys_task1_2_c1_p1_r8_v1_gpt_v2_harness\n",
      "    Failures: 3/8\n",
      "\n",
      "================================================================================\n",
      "GROK-4\n",
      "================================================================================\n",
      "\n",
      "✅ EASY (0-1 failures): 45 prompts\n",
      "⚠️  MEDIUM (2-4 failures): 0 prompts\n",
      "❌ HARD (5-8 failures): 5 prompts\n",
      "\n",
      "Total prompts: 50\n",
      "\n",
      "Distribution:\n",
      "  Easy: 90.0%\n",
      "  Medium: 0.0%\n",
      "  Hard: 10.0%\n",
      "\n",
      "❌ HARD prompts (showing up to 5):\n",
      "  - new_sys_task9_7_c1_p1_r8_v3_harness\n",
      "    Failures: 8/8\n",
      "  - new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "    Failures: 8/8\n",
      "  - new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "    Failures: 8/8\n",
      "  - task6_2_c1_p1_r8_v1_gpt5_v1_harness\n",
      "    Failures: 6/8\n",
      "  - new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "    Failures: 6/8\n",
      "\n",
      "================================================================================\n",
      "SUMMARY ACROSS ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "Model                Easy       Medium     Hard       Total     \n",
      "------------------------------------------------------------\n",
      "claude-sonnet-4-5    43         4          3          50        \n",
      "gemini-2.5-pro       32         9          9          50        \n",
      "gpt-5                35         8          7          50        \n",
      "grok-4               45         0          5          50        \n"
     ]
    }
   ],
   "source": [
    "# Categorize prompts by difficulty based on failure rate per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"PROMPT DIFFICULTY CATEGORIZATION BY MODEL\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Categories:\n",
    "# Easy: 0-1 failures (7-8 successes out of 8)\n",
    "# Medium: 2-4 failures (4-6 successes out of 8)\n",
    "# Hard: 5-8 failures (0-3 successes out of 8)\n",
    "\n",
    "def categorize_difficulty(success_count, total_runs=8):\n",
    "    \"\"\"Categorize based on number of failures\"\"\"\n",
    "    failures = total_runs - success_count\n",
    "    if failures <= 1:\n",
    "        return \"easy\"\n",
    "    elif failures <= 4:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"hard\"\n",
    "\n",
    "# Structure: difficulty_by_model[model][category] = [list of tasks]\n",
    "difficulty_by_model = {}\n",
    "\n",
    "for model in sorted(task_results.keys()):\n",
    "    difficulty_by_model[model] = {\n",
    "        \"easy\": [],\n",
    "        \"medium\": [],\n",
    "        \"hard\": []\n",
    "    }\n",
    "    \n",
    "    for task_name, stats in task_results[model].items():\n",
    "        category = categorize_difficulty(stats[\"success\"], stats[\"total\"])\n",
    "        difficulty_by_model[model][category].append({\n",
    "            \"task\": task_name,\n",
    "            \"success\": stats[\"success\"],\n",
    "            \"total\": stats[\"total\"],\n",
    "            \"failures\": stats[\"total\"] - stats[\"success\"]\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "for model in sorted(difficulty_by_model.keys()):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    categories = difficulty_by_model[model]\n",
    "    \n",
    "    print(f\"✅ EASY (0-1 failures): {len(categories['easy'])} prompts\")\n",
    "    print(f\"⚠️  MEDIUM (2-4 failures): {len(categories['medium'])} prompts\")\n",
    "    print(f\"❌ HARD (5-8 failures): {len(categories['hard'])} prompts\")\n",
    "    \n",
    "    total_prompts = len(categories['easy']) + len(categories['medium']) + len(categories['hard'])\n",
    "    print(f\"\\nTotal prompts: {total_prompts}\")\n",
    "    \n",
    "    if total_prompts > 0:\n",
    "        easy_pct = len(categories['easy']) / total_prompts * 100\n",
    "        medium_pct = len(categories['medium']) / total_prompts * 100\n",
    "        hard_pct = len(categories['hard']) / total_prompts * 100\n",
    "        \n",
    "        print(f\"\\nDistribution:\")\n",
    "        print(f\"  Easy: {easy_pct:.1f}%\")\n",
    "        print(f\"  Medium: {medium_pct:.1f}%\")\n",
    "        print(f\"  Hard: {hard_pct:.1f}%\")\n",
    "    \n",
    "    # Show some examples from each category\n",
    "    if categories['hard']:\n",
    "        print(f\"\\n❌ HARD prompts (showing up to 5):\")\n",
    "        for item in sorted(categories['hard'], key=lambda x: x['failures'], reverse=True)[:5]:\n",
    "            print(f\"  - {item['task']}\")\n",
    "            print(f\"    Failures: {item['failures']}/{item['total']}\")\n",
    "    \n",
    "    if categories['medium']:\n",
    "        print(f\"\\n⚠️  MEDIUM prompts (showing up to 3):\")\n",
    "        for item in sorted(categories['medium'], key=lambda x: x['failures'], reverse=True)[:3]:\n",
    "            print(f\"  - {item['task']}\")\n",
    "            print(f\"    Failures: {item['failures']}/{item['total']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY ACROSS ALL MODELS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Aggregate summary\n",
    "summary_table = []\n",
    "for model in sorted(difficulty_by_model.keys()):\n",
    "    categories = difficulty_by_model[model]\n",
    "    total = len(categories['easy']) + len(categories['medium']) + len(categories['hard'])\n",
    "    summary_table.append({\n",
    "        \"model\": model,\n",
    "        \"easy\": len(categories['easy']),\n",
    "        \"medium\": len(categories['medium']),\n",
    "        \"hard\": len(categories['hard']),\n",
    "        \"total\": total\n",
    "    })\n",
    "\n",
    "# Print as table\n",
    "print(f\"{'Model':<20} {'Easy':<10} {'Medium':<10} {'Hard':<10} {'Total':<10}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for row in summary_table:\n",
    "    print(f\"{row['model']:<20} {row['easy']:<10} {row['medium']:<10} {row['hard']:<10} {row['total']:<10}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c92bd2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING DIFFICULTY CATEGORIZATION\n",
      "================================================================================\n",
      "\n",
      "✅ Exported: difficulty_summary_by_model.csv\n",
      "✅ Exported: prompt_difficulty_detailed.csv\n",
      "\n",
      "Analyzing cross-model difficulty patterns...\n",
      "✅ Exported: prompt_difficulty_cross_model.csv\n",
      "\n",
      "================================================================================\n",
      "EXPORT COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Generated difficulty analysis files:\n",
      "  1. difficulty_summary_by_model.csv - Summary table of easy/medium/hard counts per model\n",
      "  2. prompt_difficulty_detailed.csv - Detailed list of all prompts with difficulty ratings per model\n",
      "  3. prompt_difficulty_cross_model.csv - Cross-model comparison showing which prompts are hard for which models\n"
     ]
    }
   ],
   "source": [
    "# Export difficulty categorization to CSV\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"EXPORTING DIFFICULTY CATEGORIZATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Export 1: Difficulty summary by model\n",
    "with open(\"difficulty_summary_by_model.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Easy (0-1 failures)\", \"Medium (2-4 failures)\", \"Hard (5-8 failures)\", \"Total Prompts\", \"Easy %\", \"Medium %\", \"Hard %\"])\n",
    "    \n",
    "    for model in sorted(difficulty_by_model.keys()):\n",
    "        categories = difficulty_by_model[model]\n",
    "        total = len(categories['easy']) + len(categories['medium']) + len(categories['hard'])\n",
    "        \n",
    "        easy_pct = len(categories['easy']) / total * 100 if total > 0 else 0\n",
    "        medium_pct = len(categories['medium']) / total * 100 if total > 0 else 0\n",
    "        hard_pct = len(categories['hard']) / total * 100 if total > 0 else 0\n",
    "        \n",
    "        writer.writerow([\n",
    "            model,\n",
    "            len(categories['easy']),\n",
    "            len(categories['medium']),\n",
    "            len(categories['hard']),\n",
    "            total,\n",
    "            f\"{easy_pct:.1f}\",\n",
    "            f\"{medium_pct:.1f}\",\n",
    "            f\"{hard_pct:.1f}\"\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: difficulty_summary_by_model.csv\")\n",
    "\n",
    "# Export 2: Detailed prompt difficulty per model\n",
    "with open(\"prompt_difficulty_detailed.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Prompt/Task\", \"Difficulty\", \"Successes\", \"Failures\", \"Total Runs\", \"Success Rate %\"])\n",
    "    \n",
    "    for model in sorted(difficulty_by_model.keys()):\n",
    "        for category in ['easy', 'medium', 'hard']:\n",
    "            for item in sorted(difficulty_by_model[model][category], key=lambda x: x['failures'], reverse=True):\n",
    "                success_rate = (item['success'] / item['total'] * 100) if item['total'] > 0 else 0\n",
    "                writer.writerow([\n",
    "                    model,\n",
    "                    item['task'],\n",
    "                    category.upper(),\n",
    "                    item['success'],\n",
    "                    item['failures'],\n",
    "                    item['total'],\n",
    "                    f\"{success_rate:.1f}\"\n",
    "                ])\n",
    "\n",
    "print(\"✅ Exported: prompt_difficulty_detailed.csv\")\n",
    "\n",
    "# Export 3: Cross-model difficulty comparison (which prompts are hard for which models)\n",
    "print(\"\\nAnalyzing cross-model difficulty patterns...\")\n",
    "\n",
    "# Get all unique tasks\n",
    "all_tasks_set = set()\n",
    "for model in task_results.keys():\n",
    "    all_tasks_set.update(task_results[model].keys())\n",
    "\n",
    "with open(\"prompt_difficulty_cross_model.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Header\n",
    "    header = [\"Prompt/Task\"]\n",
    "    for model in sorted(task_results.keys()):\n",
    "        header.extend([f\"{model} Difficulty\", f\"{model} Failures\"])\n",
    "    header.append(\"Models Finding This Hard\")\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # For each task, show difficulty across models\n",
    "    for task in sorted(all_tasks_set):\n",
    "        row = [task]\n",
    "        hard_count = 0\n",
    "        \n",
    "        for model in sorted(task_results.keys()):\n",
    "            if task in task_results[model]:\n",
    "                stats = task_results[model][task]\n",
    "                category = categorize_difficulty(stats[\"success\"], stats[\"total\"])\n",
    "                failures = stats[\"total\"] - stats[\"success\"]\n",
    "                \n",
    "                row.extend([category.upper(), failures])\n",
    "                \n",
    "                if category == \"hard\":\n",
    "                    hard_count += 1\n",
    "            else:\n",
    "                row.extend([\"N/A\", \"N/A\"])\n",
    "        \n",
    "        row.append(hard_count)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"✅ Exported: prompt_difficulty_cross_model.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPORT COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Generated difficulty analysis files:\")\n",
    "print(f\"  1. difficulty_summary_by_model.csv - Summary table of easy/medium/hard counts per model\")\n",
    "print(f\"  2. prompt_difficulty_detailed.csv - Detailed list of all prompts with difficulty ratings per model\")\n",
    "print(f\"  3. prompt_difficulty_cross_model.csv - Cross-model comparison showing which prompts are hard for which models\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0334c811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UNIVERSAL vs MODEL-SPECIFIC DIFFICULTY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🔴 UNIVERSAL HARD PROMPTS (Hard for ALL models): 4\n",
      "\n",
      "1. new_sys_task17_9_c1_p1_r8_v4_harness\n",
      "   Hard for: claude-sonnet-4-5, gemini-2.5-pro, gpt-5, grok-4\n",
      "\n",
      "2. gemini-2.5-pro\n",
      "   Hard for: gemini-2.5-pro\n",
      "\n",
      "3. new_sys_task9_7_c1_p1_r8_v3_harness\n",
      "   Hard for: claude-sonnet-4-5, gpt-5, grok-4\n",
      "\n",
      "4. new_sys_task6_4_c1_p1_r8_v3_harness\n",
      "   Hard for: claude-sonnet-4-5, gemini-2.5-pro, gpt-5, grok-4\n",
      "\n",
      "================================================================================\n",
      "🟠 MOSTLY HARD PROMPTS (Hard for 75%+ of models): 0\n",
      "\n",
      "================================================================================\n",
      "🟡 SOME HARD PROMPTS (Hard for multiple but not all models): 2\n",
      "\n",
      "1. new_sys_task2_8_c1_p1_r8_v2_harness\n",
      "   Hard for: gpt-5, grok-4 (2/4)\n",
      "\n",
      "2. task6_2_c1_p1_r8_v1_gpt5_v1_harness\n",
      "   Hard for: gpt-5, grok-4 (2/4)\n",
      "\n",
      "================================================================================\n",
      "🔵 MODEL-SPECIFIC HARD PROMPTS (Hard for only ONE model)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "gemini-2.5-pro: 6 unique hard prompts\n",
      "  Top 3 (by failures):\n",
      "    - new_sys_task7_6_c1_p1_r8_v1\n",
      "      Failures: 8/8\n",
      "    - new_sys_task8_1_c1_p1_r8_v1_claude_v1_harness\n",
      "      Failures: 7/8\n",
      "    - new_sys_task6_6_c1_p1_r8_v3_harness\n",
      "      Failures: 6/8\n",
      "\n",
      "gpt-5: 2 unique hard prompts\n",
      "  Top 3 (by failures):\n",
      "    - task24_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "      Failures: 7/8\n",
      "    - new_sys_task1_6_c1_p1_r8_v1_harness\n",
      "      Failures: 5/8\n",
      "\n",
      "================================================================================\n",
      "DIFFICULTY PATTERN SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total unique prompts analyzed: 52\n",
      "\n",
      "Difficulty patterns:\n",
      "  🔴 Universal (all models): 4 (7.7%)\n",
      "  🟠 Mostly hard (75%+ models): 0 (0.0%)\n",
      "  🟡 Some hard (2-3 models): 2 (3.8%)\n",
      "  🔵 Model-specific (1 model): 8 (15.4%)\n",
      "  ✅ Easy for all models: 23 (44.2%)\n"
     ]
    }
   ],
   "source": [
    "# Identify universally difficult prompts and model-specific challenges\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"UNIVERSAL vs MODEL-SPECIFIC DIFFICULTY ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Get all unique tasks\n",
    "all_tasks_set = set()\n",
    "for model in task_results.keys():\n",
    "    all_tasks_set.update(task_results[model].keys())\n",
    "\n",
    "# Analyze each task across all models\n",
    "universal_hard = []\n",
    "mostly_hard = []\n",
    "some_hard = []\n",
    "model_specific_hard = {}\n",
    "\n",
    "for task in all_tasks_set:\n",
    "    hard_models = []\n",
    "    medium_models = []\n",
    "    easy_models = []\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            category = categorize_difficulty(stats[\"success\"], stats[\"total\"])\n",
    "            \n",
    "            if category == \"hard\":\n",
    "                hard_models.append(model)\n",
    "            elif category == \"medium\":\n",
    "                medium_models.append(model)\n",
    "            else:\n",
    "                easy_models.append(model)\n",
    "    \n",
    "    num_hard = len(hard_models)\n",
    "    total_models = len(hard_models) + len(medium_models) + len(easy_models)\n",
    "    \n",
    "    if num_hard == total_models and total_models > 0:\n",
    "        # All models find this hard\n",
    "        universal_hard.append({\n",
    "            \"task\": task,\n",
    "            \"hard_count\": num_hard,\n",
    "            \"models\": hard_models\n",
    "        })\n",
    "    elif num_hard >= total_models * 0.75 and total_models > 0:\n",
    "        # Most models find this hard (75%+)\n",
    "        mostly_hard.append({\n",
    "            \"task\": task,\n",
    "            \"hard_count\": num_hard,\n",
    "            \"total\": total_models,\n",
    "            \"hard_models\": hard_models,\n",
    "            \"other_models\": medium_models + easy_models\n",
    "        })\n",
    "    elif num_hard > 0:\n",
    "        # Some models find this hard\n",
    "        if num_hard == 1:\n",
    "            # Model-specific difficulty\n",
    "            model = hard_models[0]\n",
    "            if model not in model_specific_hard:\n",
    "                model_specific_hard[model] = []\n",
    "            model_specific_hard[model].append({\n",
    "                \"task\": task,\n",
    "                \"failures\": task_results[model][task][\"total\"] - task_results[model][task][\"success\"]\n",
    "            })\n",
    "        else:\n",
    "            some_hard.append({\n",
    "                \"task\": task,\n",
    "                \"hard_count\": num_hard,\n",
    "                \"total\": total_models,\n",
    "                \"hard_models\": hard_models\n",
    "            })\n",
    "\n",
    "print(f\"🔴 UNIVERSAL HARD PROMPTS (Hard for ALL models): {len(universal_hard)}\")\n",
    "if universal_hard:\n",
    "    for i, item in enumerate(universal_hard, 1):\n",
    "        print(f\"\\n{i}. {item['task']}\")\n",
    "        print(f\"   Hard for: {', '.join(item['models'])}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"🟠 MOSTLY HARD PROMPTS (Hard for 75%+ of models): {len(mostly_hard)}\")\n",
    "if mostly_hard:\n",
    "    for i, item in enumerate(mostly_hard[:5], 1):\n",
    "        print(f\"\\n{i}. {item['task']}\")\n",
    "        print(f\"   Hard for: {', '.join(item['hard_models'])} ({item['hard_count']}/{item['total']})\")\n",
    "        if item['other_models']:\n",
    "            print(f\"   Other models: {', '.join(item['other_models'])}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"🟡 SOME HARD PROMPTS (Hard for multiple but not all models): {len(some_hard)}\")\n",
    "if some_hard:\n",
    "    for i, item in enumerate(sorted(some_hard, key=lambda x: x['hard_count'], reverse=True)[:5], 1):\n",
    "        print(f\"\\n{i}. {item['task']}\")\n",
    "        print(f\"   Hard for: {', '.join(item['hard_models'])} ({item['hard_count']}/{item['total']})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"🔵 MODEL-SPECIFIC HARD PROMPTS (Hard for only ONE model)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for model in sorted(model_specific_hard.keys()):\n",
    "    prompts = model_specific_hard[model]\n",
    "    print(f\"\\n{model}: {len(prompts)} unique hard prompts\")\n",
    "    if prompts:\n",
    "        print(f\"  Top 3 (by failures):\")\n",
    "        for item in sorted(prompts, key=lambda x: x['failures'], reverse=True)[:3]:\n",
    "            print(f\"    - {item['task']}\")\n",
    "            print(f\"      Failures: {item['failures']}/8\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DIFFICULTY PATTERN SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "total_analyzed = len(all_tasks_set)\n",
    "print(f\"Total unique prompts analyzed: {total_analyzed}\")\n",
    "print(f\"\\nDifficulty patterns:\")\n",
    "print(f\"  🔴 Universal (all models): {len(universal_hard)} ({len(universal_hard)/total_analyzed*100:.1f}%)\")\n",
    "print(f\"  🟠 Mostly hard (75%+ models): {len(mostly_hard)} ({len(mostly_hard)/total_analyzed*100:.1f}%)\")\n",
    "print(f\"  🟡 Some hard (2-3 models): {len(some_hard)} ({len(some_hard)/total_analyzed*100:.1f}%)\")\n",
    "\n",
    "model_specific_total = sum(len(prompts) for prompts in model_specific_hard.values())\n",
    "print(f\"  🔵 Model-specific (1 model): {model_specific_total} ({model_specific_total/total_analyzed*100:.1f}%)\")\n",
    "\n",
    "# Calculate prompts that are easy for all\n",
    "all_easy_count = 0\n",
    "for task in all_tasks_set:\n",
    "    all_easy = True\n",
    "    for model in task_results.keys():\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            category = categorize_difficulty(stats[\"success\"], stats[\"total\"])\n",
    "            if category != \"easy\":\n",
    "                all_easy = False\n",
    "                break\n",
    "    if all_easy:\n",
    "        all_easy_count += 1\n",
    "\n",
    "print(f\"  ✅ Easy for all models: {all_easy_count} ({all_easy_count/total_analyzed*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e8f095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PERFECT SUCCESS RATE ANALYSIS (8/8 runs passing)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  Tasks with 8/8 success: 39/50\n",
      "  Percentage: 78.0%\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  Tasks with 8/8 success: 21/50\n",
      "  Percentage: 42.0%\n",
      "\n",
      "gpt-5:\n",
      "  Tasks with 8/8 success: 25/50\n",
      "  Percentage: 50.0%\n",
      "\n",
      "grok-4:\n",
      "  Tasks with 8/8 success: 40/50\n",
      "  Percentage: 80.0%\n",
      "\n",
      "================================================================================\n",
      "SUMMARY TABLE\n",
      "================================================================================\n",
      "\n",
      "Model                     Perfect Tasks (8/8)  Total Tasks     Perfect %      \n",
      "---------------------------------------------------------------------------\n",
      "claude-sonnet-4-5         39                   50              78.0%\n",
      "gemini-2.5-pro            21                   50              42.0%\n",
      "gpt-5                     25                   50              50.0%\n",
      "grok-4                    40                   50              80.0%\n",
      "\n",
      "================================================================================\n",
      "TASKS PERFECT FOR ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "Tasks with 8/8 success across ALL models: 13/52\n",
      "Percentage: 25.0%\n",
      "\n",
      "These tasks are perfect for all models:\n",
      "  1. new_sys_task11_9_c1_p1_r8_v2_harness\n",
      "  2. new_sys_task12_3_c1_p1_r8_v4_harness\n",
      "  3. new_sys_task15_1_c1_p1_r8_v1_gpt_v1_harness\n",
      "  4. new_sys_task1_1_c1_p1_r8_v1_claude_v1_harness\n",
      "  5. new_sys_task1_3_c1_p1_r8_v2_harness\n",
      "  6. new_sys_task2_2_c1_p1_r8_v4_gpt5_v1_harness\n",
      "  7. new_sys_task3_4_c1_p1_r8_v6_harness\n",
      "  8. new_sys_task3_8_c1_p1_r8_v1_harness\n",
      "  9. new_sys_task4_4_c1_p1_r8_v3_harness\n",
      "  10. new_sys_task5_4_c1_p1_r8_v15_harness\n",
      "  11. new_sys_task_task16_6_c1_p1_r8_v3_harness\n",
      "  12. task21_10_c1_p1_r8_v6_harness\n",
      "  13. task2_7_c1_p1_r8_v2_harness\n",
      "\n",
      "================================================================================\n",
      "TASKS WITH MIXED PERFECT SCORES\n",
      "================================================================================\n",
      "\n",
      "Tasks with mixed perfect scores: 33\n",
      "\n",
      "Top examples (most models perfect):\n",
      "\n",
      "1. new_sys_task16_7_c1_p1_r8_v2_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gpt-5, grok-4\n",
      "   Not perfect for: gemini-2.5-pro\n",
      "\n",
      "2. new_sys_task20_9_c1_p1_r8_v2_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gpt-5, grok-4\n",
      "   Not perfect for: gemini-2.5-pro\n",
      "\n",
      "3. new_sys_task4_1_c1_p1_r8_v1_claude_v1_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gpt-5, grok-4\n",
      "   Not perfect for: gemini-2.5-pro\n",
      "\n",
      "4. new_sys_task11_4_c1_p1_r8_v2_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gpt-5, grok-4\n",
      "   Not perfect for: gemini-2.5-pro\n",
      "\n",
      "5. new_sys_task1_5_c1_p1_r8_v7_gpt_v1_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gpt-5, grok-4\n",
      "   Not perfect for: gemini-2.5-pro\n",
      "\n",
      "6. new_sys_task3_9_c1_p1_r8_v2_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gemini-2.5-pro, grok-4\n",
      "   Not perfect for: gpt-5\n",
      "\n",
      "7. new_sys_task1_2_c1_p1_r8_v1_gpt_v2_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gemini-2.5-pro, grok-4\n",
      "   Not perfect for: gpt-5\n",
      "\n",
      "8. new_sys_task27_4_c1_p1_r8_v6_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gpt-5, grok-4\n",
      "   Not perfect for: gemini-2.5-pro\n",
      "\n",
      "9. task24_8_c1_p1_r8_v1_gpt_v1_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gemini-2.5-pro, grok-4\n",
      "   Not perfect for: gpt-5\n",
      "\n",
      "10. new_sys_task21_8_c1_p1_r8_v6_harness\n",
      "   Perfect for (3/4): claude-sonnet-4-5, gpt-5, grok-4\n",
      "   Not perfect for: gemini-2.5-pro\n"
     ]
    }
   ],
   "source": [
    "# Find tasks with perfect 8/8 success rate per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"PERFECT SUCCESS RATE ANALYSIS (8/8 runs passing)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "perfect_tasks_by_model = {}\n",
    "\n",
    "for model in sorted(task_results.keys()):\n",
    "    perfect_tasks = []\n",
    "    \n",
    "    for task_name, stats in task_results[model].items():\n",
    "        if stats[\"success\"] == stats[\"total\"] and stats[\"total\"] == 8:\n",
    "            perfect_tasks.append(task_name)\n",
    "    \n",
    "    perfect_tasks_by_model[model] = perfect_tasks\n",
    "    \n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  Tasks with 8/8 success: {len(perfect_tasks)}/{len(task_results[model])}\")\n",
    "    print(f\"  Percentage: {len(perfect_tasks)/len(task_results[model])*100:.1f}%\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY TABLE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"{'Model':<25} {'Perfect Tasks (8/8)':<20} {'Total Tasks':<15} {'Perfect %':<15}\")\n",
    "print(f\"{'-'*75}\")\n",
    "\n",
    "for model in sorted(perfect_tasks_by_model.keys()):\n",
    "    perfect_count = len(perfect_tasks_by_model[model])\n",
    "    total_count = len(task_results[model])\n",
    "    perfect_pct = perfect_count / total_count * 100 if total_count > 0 else 0\n",
    "    \n",
    "    print(f\"{model:<25} {perfect_count:<20} {total_count:<15} {perfect_pct:.1f}%\")\n",
    "\n",
    "# Compare which tasks are perfect across all models\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TASKS PERFECT FOR ALL MODELS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "all_tasks_set = set()\n",
    "for model in task_results.keys():\n",
    "    all_tasks_set.update(task_results[model].keys())\n",
    "\n",
    "perfect_for_all = []\n",
    "for task in all_tasks_set:\n",
    "    is_perfect_for_all = True\n",
    "    \n",
    "    for model in task_results.keys():\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            if stats[\"success\"] != 8 or stats[\"total\"] != 8:\n",
    "                is_perfect_for_all = False\n",
    "                break\n",
    "        else:\n",
    "            is_perfect_for_all = False\n",
    "            break\n",
    "    \n",
    "    if is_perfect_for_all:\n",
    "        perfect_for_all.append(task)\n",
    "\n",
    "print(f\"Tasks with 8/8 success across ALL models: {len(perfect_for_all)}/{len(all_tasks_set)}\")\n",
    "print(f\"Percentage: {len(perfect_for_all)/len(all_tasks_set)*100:.1f}%\\n\")\n",
    "\n",
    "if perfect_for_all:\n",
    "    print(f\"These tasks are perfect for all models:\")\n",
    "    for i, task in enumerate(sorted(perfect_for_all), 1):\n",
    "        print(f\"  {i}. {task}\")\n",
    "\n",
    "# Find tasks that are perfect for some models but not all\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TASKS WITH MIXED PERFECT SCORES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "mixed_perfect = {}\n",
    "for task in all_tasks_set:\n",
    "    perfect_models = []\n",
    "    imperfect_models = []\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            if stats[\"success\"] == 8 and stats[\"total\"] == 8:\n",
    "                perfect_models.append(model)\n",
    "            else:\n",
    "                imperfect_models.append(model)\n",
    "    \n",
    "    # Only include if some (but not all) models have perfect score\n",
    "    if perfect_models and imperfect_models:\n",
    "        mixed_perfect[task] = {\n",
    "            \"perfect\": perfect_models,\n",
    "            \"imperfect\": imperfect_models\n",
    "        }\n",
    "\n",
    "print(f\"Tasks with mixed perfect scores: {len(mixed_perfect)}\\n\")\n",
    "\n",
    "# Show top examples (tasks where only 1-2 models fail)\n",
    "mixed_sorted = sorted(mixed_perfect.items(), \n",
    "                     key=lambda x: len(x[1][\"perfect\"]), \n",
    "                     reverse=True)\n",
    "\n",
    "print(f\"Top examples (most models perfect):\")\n",
    "for i, (task, data) in enumerate(mixed_sorted[:10], 1):\n",
    "    print(f\"\\n{i}. {task}\")\n",
    "    print(f\"   Perfect for ({len(data['perfect'])}/4): {', '.join(data['perfect'])}\")\n",
    "    print(f\"   Not perfect for: {', '.join(data['imperfect'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c00cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING PERFECT SCORE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "✅ Exported: perfect_score_summary.csv\n",
      "✅ Exported: perfect_tasks_by_model.csv\n",
      "✅ Exported: perfect_score_cross_model.csv\n",
      "✅ Exported: universal_perfect_tasks.csv\n",
      "\n",
      "================================================================================\n",
      "PERFECT SCORE EXPORT COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  1. perfect_score_summary.csv - Summary of perfect scores per model\n",
      "  2. perfect_tasks_by_model.csv - List of all perfect tasks per model\n",
      "  3. perfect_score_cross_model.csv - Matrix showing which tasks are perfect for which models\n",
      "  4. universal_perfect_tasks.csv - Tasks perfect for all vs some models\n"
     ]
    }
   ],
   "source": [
    "# Export perfect score analysis\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"EXPORTING PERFECT SCORE ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Export 1: Perfect score summary by model\n",
    "with open(\"perfect_score_summary.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Perfect Tasks (8/8)\", \"Total Tasks\", \"Perfect %\", \"Imperfect Tasks\"])\n",
    "    \n",
    "    for model in sorted(perfect_tasks_by_model.keys()):\n",
    "        perfect_count = len(perfect_tasks_by_model[model])\n",
    "        total_count = len(task_results[model])\n",
    "        imperfect_count = total_count - perfect_count\n",
    "        perfect_pct = perfect_count / total_count * 100 if total_count > 0 else 0\n",
    "        \n",
    "        writer.writerow([\n",
    "            model,\n",
    "            perfect_count,\n",
    "            total_count,\n",
    "            f\"{perfect_pct:.1f}\",\n",
    "            imperfect_count\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: perfect_score_summary.csv\")\n",
    "\n",
    "# Export 2: Perfect tasks per model\n",
    "with open(\"perfect_tasks_by_model.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Task Name\"])\n",
    "    \n",
    "    for model in sorted(perfect_tasks_by_model.keys()):\n",
    "        for task in sorted(perfect_tasks_by_model[model]):\n",
    "            writer.writerow([model, task])\n",
    "\n",
    "print(\"✅ Exported: perfect_tasks_by_model.csv\")\n",
    "\n",
    "# Export 3: Cross-model perfect score matrix\n",
    "with open(\"perfect_score_cross_model.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Header\n",
    "    models_sorted = sorted(task_results.keys())\n",
    "    header = [\"Task Name\"] + [f\"{model} (8/8?)\" for model in models_sorted] + [\"Perfect for # Models\", \"Perfect for All?\"]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # For each task\n",
    "    for task in sorted(all_tasks_set):\n",
    "        row = [task]\n",
    "        perfect_count = 0\n",
    "        \n",
    "        for model in models_sorted:\n",
    "            if task in task_results[model]:\n",
    "                stats = task_results[model][task]\n",
    "                is_perfect = stats[\"success\"] == 8 and stats[\"total\"] == 8\n",
    "                row.append(\"Yes\" if is_perfect else \"No\")\n",
    "                if is_perfect:\n",
    "                    perfect_count += 1\n",
    "            else:\n",
    "                row.append(\"N/A\")\n",
    "        \n",
    "        row.append(perfect_count)\n",
    "        row.append(\"Yes\" if perfect_count == len(models_sorted) else \"No\")\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"✅ Exported: perfect_score_cross_model.csv\")\n",
    "\n",
    "# Export 4: Universal perfect tasks\n",
    "with open(\"universal_perfect_tasks.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Task Name\", \"Status\"])\n",
    "    \n",
    "    writer.writerow([\"=== PERFECT FOR ALL MODELS ===\", \"\"])\n",
    "    for task in sorted(perfect_for_all):\n",
    "        writer.writerow([task, \"Perfect for all\"])\n",
    "    \n",
    "    writer.writerow([\"\", \"\"])\n",
    "    writer.writerow([\"=== MIXED PERFECT SCORES ===\", \"\"])\n",
    "    writer.writerow([\"Task Name\", \"Perfect Models\", \"Imperfect Models\"])\n",
    "    \n",
    "    for task, data in mixed_sorted:\n",
    "        writer.writerow([\n",
    "            task,\n",
    "            \", \".join(data[\"perfect\"]),\n",
    "            \", \".join(data[\"imperfect\"])\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: universal_perfect_tasks.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PERFECT SCORE EXPORT COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Generated files:\")\n",
    "print(f\"  1. perfect_score_summary.csv - Summary of perfect scores per model\")\n",
    "print(f\"  2. perfect_tasks_by_model.csv - List of all perfect tasks per model\")\n",
    "print(f\"  3. perfect_score_cross_model.csv - Matrix showing which tasks are perfect for which models\")\n",
    "print(f\"  4. universal_perfect_tasks.csv - Tasks perfect for all vs some models\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f84c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
