{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCP Benchmark SDK - Harness Example\n",
        "\n",
        "This notebook demonstrates how to use the MCP Benchmark SDK to run a test harness and save results.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The SDK provides a simple API for:\n",
        "- Loading test scenarios from JSON harness files\n",
        "- Running benchmarks across multiple models\n",
        "- Collecting and saving detailed results\n",
        "- Analyzing success rates and conversation history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ipriyam26/Programming/agent_harness/packages/mcp_benchmark_sdk/src/mcp_benchmark_sdk/harness/agent_factory.py:117: UserWarning: Claude thinking mode requires temperature=1.0. Your temperature=0.1 will be overridden. Set temperature=1.0 or enable_thinking=False to suppress this warning.\n",
            "  return ClaudeAgent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ pm review task - claude-sonnet-4-5\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "from pathlib import Path\n",
        "from mcp_benchmark_sdk import TestHarness, TestHarnessConfig, MCPConfig, create_agent\n",
        "load_dotenv()\n",
        "# Configure MCP and harness\n",
        "config = TestHarnessConfig(\n",
        "    mcps=[MCPConfig(name=\"jira\", url=\"http://localhost:8015/mcp\", transport=\"streamable_http\")],\n",
        "    sql_runner_url=\"http://localhost:8015/api/sql-runner\"\n",
        ")\n",
        "\n",
        "# Load harness and run\n",
        "harness = TestHarness(harness_path=Path(\"9_tasks/task1.json\"), config=config)\n",
        "results = await harness.run(models=[\"claude-sonnet-4-5\"], agent_factory=create_agent)\n",
        "\n",
        "# Check results\n",
        "for result in results:\n",
        "    print(f\"{'✅' if result.success else '❌'} {result.scenario_name} - {result.model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, import the necessary modules from the SDK:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from mcp_benchmark_sdk import (\n",
        "    TestHarness,\n",
        "    TestHarnessConfig,\n",
        "    MCPConfig,\n",
        "    create_agent,\n",
        "    RunObserver,\n",
        ")\n",
        "\n",
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Configure the MCP server(s) and test harness settings:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration complete!\n",
            "MCP Server: http://localhost:8015/mcp\n",
            "Models: claude-sonnet-4-5\n",
            "Harness: 9_tasks/task1.json\n"
          ]
        }
      ],
      "source": [
        "# Configure MCP Server (adjust URL to match your setup)\n",
        "mcp_config = MCPConfig(\n",
        "    name=\"jira\",\n",
        "    url=\"http://localhost:8015/mcp\",\n",
        "    transport=\"streamable_http\",\n",
        ")\n",
        "\n",
        "# Configure test harness\n",
        "config = TestHarnessConfig(\n",
        "    mcps=[mcp_config],\n",
        "    sql_runner_url=\"http://localhost:8015/api/sql-runner\",  # For database verifiers\n",
        "    max_steps=1000,\n",
        "    tool_call_limit=1000,\n",
        "    temperature=0.1,\n",
        "    runs_per_scenario=1,  # How many times to run each scenario\n",
        "    max_concurrent_runs=5,  # Limit concurrent executions\n",
        ")\n",
        "\n",
        "# Models to test (adjust based on your API keys)\n",
        "models = [\n",
        "    \"claude-sonnet-4-5\",\n",
        "    # \"gpt-4o\",\n",
        "    # \"gemini-2.5-pro\",\n",
        "    # \"grok-4\",\n",
        "]\n",
        "\n",
        "# Harness file path\n",
        "harness_path = Path(\"9_tasks/task1.json\")\n",
        "\n",
        "print(f\"Configuration complete!\")\n",
        "print(f\"MCP Server: {mcp_config.url}\")\n",
        "print(f\"Models: {', '.join(models)}\")\n",
        "print(f\"Harness: {harness_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Harness Example\n",
        "\n",
        "Let's start with the simplest possible example - creating and running a minimal harness:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created simple harness: simple_harness.json\n",
            "\n",
            "Harness structure:\n",
            "  - Scenarios: 1\n",
            "  - Scenario ID: simple_test\n",
            "  - Prompts: 1\n",
            "  - Verifiers: 1\n"
          ]
        }
      ],
      "source": [
        "# Create a simple harness file\n",
        "simple_harness = {\n",
        "    \"scenarios\": [\n",
        "        {\n",
        "            \"scenario_id\": \"simple_test\",\n",
        "            \"name\": \"Simple Issue Creation Test\",\n",
        "            \"description\": \"Test creating a single issue\",\n",
        "            \"prompts\": [\n",
        "                {\n",
        "                    \"prompt_text\": \"Create a bug issue in project DEMO with title 'Test Bug' and description 'This is a test'\",\n",
        "                    \"expected_tools\": [\"create_issue\"],\n",
        "                    \"verifier\": [\n",
        "                        {\n",
        "                            \"verifier_type\": \"database_state\",\n",
        "                            \"name\": \"issue_created\",\n",
        "                            \"validation_config\": {\n",
        "                                \"query\": \"SELECT COUNT(*) FROM issue WHERE summary = 'Test Bug'\",\n",
        "                                \"expected_value\": 1,\n",
        "                                \"comparison_type\": \"equals\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "simple_harness_path = Path(\"simple_harness.json\")\n",
        "with open(simple_harness_path, \"w\") as f:\n",
        "    json.dump(simple_harness, f, indent=2)\n",
        "\n",
        "print(f\"✅ Created simple harness: {simple_harness_path}\")\n",
        "print(f\"\\nHarness structure:\")\n",
        "print(f\"  - Scenarios: {len(simple_harness['scenarios'])}\")\n",
        "print(f\"  - Scenario ID: {simple_harness['scenarios'][0]['scenario_id']}\")\n",
        "print(f\"  - Prompts: {len(simple_harness['scenarios'][0]['prompts'])}\")\n",
        "print(f\"  - Verifiers: {len(simple_harness['scenarios'][0]['prompts'][0]['verifier'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Simple Harness\n",
        "\n",
        "Now let's run this simple harness with minimal code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple example ready (uncomment to run)\n"
          ]
        }
      ],
      "source": [
        "async def run_simple_example():\n",
        "    \"\"\"Minimal example - just 3 lines of code!\"\"\"\n",
        "    \n",
        "    # 1. Create harness\n",
        "    harness = TestHarness(harness_path=simple_harness_path, config=config)\n",
        "    \n",
        "    # 2. Run it\n",
        "    results = await harness.run(models=[\"claude-sonnet-4-5\"], agent_factory=create_agent)\n",
        "    \n",
        "    # 3. Check results\n",
        "    for result in results:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Model: {result.model}\")\n",
        "        print(f\"Scenario: {result.scenario_name}\")\n",
        "        print(f\"Success: {'✅ PASSED' if result.success else '❌ FAILED'}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Show verifier results\n",
        "        for vr in result.verifier_results:\n",
        "            status = \"✅\" if vr.success else \"❌\"\n",
        "            msg = vr.error if vr.error else f\"Expected: {vr.expected_value}, Got: {vr.actual_value}\"\n",
        "            print(f\"{status} {vr.name}: {msg}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Uncomment to run the simple example:\n",
        "# simple_results = await run_simple_example()\n",
        "print(\"Simple example ready (uncomment to run)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Simple Harness Format\n",
        "\n",
        "The minimal harness file contains:\n",
        "\n",
        "**Required Fields:**\n",
        "- `scenarios`: Array of test scenarios\n",
        "  - `scenario_id`: Unique identifier\n",
        "  - `name`: Human-readable name\n",
        "  - `description`: What this scenario tests\n",
        "  - `prompts`: Array of prompts to send to the agent\n",
        "    - `prompt_text`: The instruction for the agent\n",
        "    - `verifier`: Array of checks to validate success\n",
        "      - `verifier_type`: Type of verification (e.g., \"database_state\")\n",
        "      - `validation_config`: Configuration for the verifier\n",
        "        - `query`: SQL query to check state\n",
        "        - `expected_value`: What value we expect\n",
        "        - `comparison_type`: How to compare (equals, greater_than, etc.)\n",
        "\n",
        "**Optional Fields:**\n",
        "- `expected_tools`: List of tools the agent should use\n",
        "- `restricted_tools`: Tools the agent cannot use\n",
        "- `system_prompt`: Custom system prompt (overrides default)\n",
        "- `metadata`: Additional metadata about the scenario\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More Simple Harness Examples\n",
        "\n",
        "Here are a few more minimal harness examples for different scenarios:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created 3 example harness structures:\n",
            "  1. assignment_harness - Assign an issue\n",
            "  2. comment_harness - Add a comment\n",
            "  3. multi_check_harness - Multiple verifiers\n",
            "\n",
            "Each can be saved to a file and run the same way!\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Simple issue assignment\n",
        "assignment_harness = {\n",
        "    \"scenarios\": [{\n",
        "        \"scenario_id\": \"assign_issue\",\n",
        "        \"name\": \"Assign Issue\",\n",
        "        \"description\": \"Test assigning an issue to a user\",\n",
        "        \"prompts\": [{\n",
        "            \"prompt_text\": \"Assign issue WEB-1 to user Alice\",\n",
        "            \"expected_tools\": [\"get_issue\", \"update_issue\"],\n",
        "            \"verifier\": [{\n",
        "                \"verifier_type\": \"database_state\",\n",
        "                \"name\": \"issue_assigned\",\n",
        "                \"validation_config\": {\n",
        "                    \"query\": \"SELECT assignee_id FROM issue WHERE key = 'WEB-1'\",\n",
        "                    \"expected_value\": 1,  # Assuming Alice has ID 1\n",
        "                    \"comparison_type\": \"equals\"\n",
        "                }\n",
        "            }]\n",
        "        }]\n",
        "    }]\n",
        "}\n",
        "\n",
        "# Example 2: Simple comment addition\n",
        "comment_harness = {\n",
        "    \"scenarios\": [{\n",
        "        \"scenario_id\": \"add_comment\",\n",
        "        \"name\": \"Add Comment\",\n",
        "        \"description\": \"Test adding a comment to an issue\",\n",
        "        \"prompts\": [{\n",
        "            \"prompt_text\": \"Add a comment to WEB-1 saying 'Working on this now'\",\n",
        "            \"expected_tools\": [\"add_issue_comment\"],\n",
        "            \"verifier\": [{\n",
        "                \"verifier_type\": \"database_state\",\n",
        "                \"name\": \"comment_added\",\n",
        "                \"validation_config\": {\n",
        "                    \"query\": \"SELECT COUNT(*) FROM comment c JOIN issue i ON c.issue_id = i.id WHERE i.key = 'WEB-1' AND c.body LIKE '%Working on this now%'\",\n",
        "                    \"expected_value\": 1,\n",
        "                    \"comparison_type\": \"equals\"\n",
        "                }\n",
        "            }]\n",
        "        }]\n",
        "    }]\n",
        "}\n",
        "\n",
        "# Example 3: Multiple verifiers (checking multiple things)\n",
        "multi_check_harness = {\n",
        "    \"scenarios\": [{\n",
        "        \"scenario_id\": \"create_with_label\",\n",
        "        \"name\": \"Create Issue with Label\",\n",
        "        \"description\": \"Create issue and verify both creation and label\",\n",
        "        \"prompts\": [{\n",
        "            \"prompt_text\": \"Create a bug issue 'Login Error' in DEMO project with label 'urgent'\",\n",
        "            \"expected_tools\": [\"create_issue\", \"update_issue\"],\n",
        "            \"verifier\": [\n",
        "                {\n",
        "                    \"verifier_type\": \"database_state\",\n",
        "                    \"name\": \"issue_created\",\n",
        "                    \"validation_config\": {\n",
        "                        \"query\": \"SELECT COUNT(*) FROM issue WHERE summary = 'Login Error'\",\n",
        "                        \"expected_value\": 1,\n",
        "                        \"comparison_type\": \"equals\"\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"verifier_type\": \"database_state\",\n",
        "                    \"name\": \"label_added\",\n",
        "                    \"validation_config\": {\n",
        "                        \"query\": \"SELECT COUNT(*) FROM issue_label_association ila JOIN issue_label il ON ila.label_id = il.id JOIN issue i ON ila.issue_id = i.id WHERE i.summary = 'Login Error' AND il.name = 'urgent'\",\n",
        "                        \"expected_value\": 1,\n",
        "                        \"comparison_type\": \"equals\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }]\n",
        "    }]\n",
        "}\n",
        "\n",
        "print(\"✅ Created 3 example harness structures:\")\n",
        "print(\"  1. assignment_harness - Assign an issue\")\n",
        "print(\"  2. comment_harness - Add a comment\")\n",
        "print(\"  3. multi_check_harness - Multiple verifiers\")\n",
        "print(\"\\nEach can be saved to a file and run the same way!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Reference: Running Any Harness\n",
        "\n",
        "The pattern for running ANY harness is always the same:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick reference function defined!\n"
          ]
        }
      ],
      "source": [
        "# Quick reference template - copy and modify this!\n",
        "async def run_any_harness(harness_file_path, model_name=\"claude-sonnet-4-5\"):\n",
        "    \"\"\"\n",
        "    Universal harness runner - works with any harness file!\n",
        "    \n",
        "    Args:\n",
        "        harness_file_path: Path to your harness JSON file\n",
        "        model_name: Model to test (default: claude-sonnet-4-5)\n",
        "    \"\"\"\n",
        "    # Create harness\n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(harness_file_path),\n",
        "        config=config\n",
        "    )\n",
        "    \n",
        "    # Run and return results\n",
        "    results = await harness.run(\n",
        "        models=[model_name],\n",
        "        agent_factory=create_agent\n",
        "    )\n",
        "    \n",
        "    # Print summary\n",
        "    for result in results:\n",
        "        status = \"✅ PASSED\" if result.success else \"❌ FAILED\"\n",
        "        print(f\"{status} | {result.scenario_name} | {result.model}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Usage examples:\n",
        "# results = await run_any_harness(\"simple_harness.json\")\n",
        "# results = await run_any_harness(\"9_tasks/task1.json\", \"gpt-4o\")\n",
        "# results = await run_any_harness(\"my_custom_test.json\", \"gemini-2.5-pro\")\n",
        "\n",
        "print(\"Quick reference function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Create a Progress Observer\n",
        "\n",
        "You can create a custom observer to track progress during execution:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observer class defined!\n"
          ]
        }
      ],
      "source": [
        "class SimpleProgressObserver(RunObserver):\n",
        "    \"\"\"Simple observer that prints progress updates.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.tool_count = 0\n",
        "        self.message_count = 0\n",
        "    \n",
        "    async def on_message(self, role, content, metadata=None):\n",
        "        \"\"\"Called when a message is sent/received.\"\"\"\n",
        "        self.message_count += 1\n",
        "        # Truncate long messages for cleaner output\n",
        "        content_preview = content[:80] + \"...\" if len(content) > 80 else content\n",
        "        print(f\"  [{role}] {content_preview}\")\n",
        "    \n",
        "    async def on_tool_call(self, tool_name, arguments, result, is_error=False):\n",
        "        \"\"\"Called when a tool is executed.\"\"\"\n",
        "        self.tool_count += 1\n",
        "        status = \"❌\" if is_error else \"✅\"\n",
        "        print(f\"  {status} Tool: {tool_name}\")\n",
        "    \n",
        "    async def on_status(self, message, level=\"info\"):\n",
        "        \"\"\"Called for status updates.\"\"\"\n",
        "        print(f\"  [{level.upper()}] {message}\")\n",
        "    \n",
        "    async def on_complete(self, success, metadata=None):\n",
        "        \"\"\"Called when execution completes.\"\"\"\n",
        "        print(f\"\\n  Completed: {success}\")\n",
        "        print(f\"  Total messages: {self.message_count}\")\n",
        "        print(f\"  Total tool calls: {self.tool_count}\")\n",
        "\n",
        "print(\"Observer class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create and Run Test Harness\n",
        "\n",
        "Now we'll create the test harness and run the benchmarks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "INITIALIZING TEST HARNESS\n",
            "================================================================================\n",
            "\n",
            "Loaded 1 scenario(s) from harness\n",
            "  1. pm review task (pm_review_task_1)\n",
            "\n",
            "================================================================================\n",
            "RUNNING BENCHMARKS\n",
            "================================================================================\n",
            "  [INFO] Initialized with 49 tools from 1 MCP(s)\n",
            "  [system] You are an autonomous project-management agent operating inside an MCP server.\n",
            "Y...\n",
            "  [user] I'm Emily Davis. I just reviewed the testing results on WEB-3 and it's working c...\n",
            "  [INFO] Step 1/1000\n",
            "  [assistant] I'll help you complete these tasks for WEB-3 and WEB-9. Let me start by gatherin...\n",
            "  ✅ Tool: get_transitions\n",
            "  ✅ Tool: find_users\n",
            "  [INFO] Step 2/1000\n",
            "  [assistant] Perfect! Now I'll proceed with the tasks:\n",
            "  ❌ Tool: do_transition\n",
            "  ✅ Tool: update_issue\n",
            "  ✅ Tool: add_issue_comment\n",
            "  ✅ Tool: assign_issue\n",
            "  ✅ Tool: add_issue_comment\n",
            "  [INFO] Step 3/1000\n",
            "  [assistant] The transition requires a comment. Let me retry with the comment included:\n",
            "  ✅ Tool: do_transition\n",
            "  [INFO] Step 4/1000\n",
            "  [assistant] Excellent! All tasks have been completed successfully:\n",
            "\n",
            "## ✅ WEB-3 Updates Compl...\n",
            "  [INFO] Agent completed\n",
            "\n",
            "================================================================================\n",
            "BENCHMARK COMPLETE\n",
            "================================================================================\n",
            "Total runs completed: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "async def run_harness():\n",
        "    \"\"\"Run the test harness and return results.\"\"\"\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"INITIALIZING TEST HARNESS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Create test harness\n",
        "    harness = TestHarness(\n",
        "        harness_path=harness_path,\n",
        "        config=config,\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nLoaded {len(harness.scenarios)} scenario(s) from harness\")\n",
        "    for i, scenario in enumerate(harness.scenarios, 1):\n",
        "        print(f\"  {i}. {scenario.name} ({scenario.scenario_id})\")\n",
        "    \n",
        "    # Optional: Add observer for progress tracking\n",
        "    harness.add_observer_factory(lambda: SimpleProgressObserver())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RUNNING BENCHMARKS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Run the benchmarks\n",
        "    results = await harness.run(\n",
        "        models=models,\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total runs completed: {len(results)}\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the harness\n",
        "results = await run_harness()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Results\n",
        "\n",
        "Let's examine the results and calculate success rates:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "RESULTS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "claude-sonnet-4-5:\n",
            "  Total runs: 1\n",
            "  Successful: 1 ✅\n",
            "  Failed: 0 ❌\n",
            "  Success rate: 100.0%\n",
            "    ✅ pm review task (run #1)\n",
            "        ✅ DatabaseVerifier: Expected: 3, Got: 3\n",
            "        ✅ DatabaseVerifier: Expected: 1, Got: 1\n",
            "        ✅ DatabaseVerifier: Expected: 1, Got: 1\n",
            "        ✅ DatabaseVerifier: Expected: 1, Got: 1\n",
            "        ✅ DatabaseVerifier: Expected: 2, Got: 2\n",
            "        ✅ DatabaseVerifier: Expected: 1, Got: 1\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Group results by model\n",
        "results_by_model = {}\n",
        "for result in results:\n",
        "    if result.model not in results_by_model:\n",
        "        results_by_model[result.model] = []\n",
        "    results_by_model[result.model].append(result)\n",
        "\n",
        "# Calculate statistics per model\n",
        "for model, model_results in results_by_model.items():\n",
        "    total = len(model_results)\n",
        "    successful = sum(1 for r in model_results if r.success)\n",
        "    failed = total - successful\n",
        "    success_rate = (successful / total * 100) if total > 0 else 0\n",
        "    \n",
        "    print(f\"\\n{model}:\")\n",
        "    print(f\"  Total runs: {total}\")\n",
        "    print(f\"  Successful: {successful} ✅\")\n",
        "    print(f\"  Failed: {failed} ❌\")\n",
        "    print(f\"  Success rate: {success_rate:.1f}%\")\n",
        "    \n",
        "    # Show details for each run\n",
        "    for result in model_results:\n",
        "        status = \"✅\" if result.success else \"❌\"\n",
        "        print(f\"    {status} {result.scenario_name} (run #{result.run_number})\")\n",
        "        \n",
        "        # Show verifier results\n",
        "        for vr in result.verifier_results:\n",
        "            vr_status = \"✅\" if vr.success else \"❌\"\n",
        "            vr_name = vr.name or \"unnamed\"\n",
        "            msg = vr.error if vr.error else f\"Expected: {vr.expected_value}, Got: {vr.actual_value}\"\n",
        "            print(f\"        {vr_status} {vr_name}: {msg}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results to JSON\n",
        "\n",
        "Save the detailed results to JSON files for later analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving results to: sdk_results_20251106_202239\n",
            "\n",
            "✅ Saved: pm_review_task_1_claude-sonnet-4-5_1.json\n",
            "\n",
            "✅ Saved summary: summary.json\n",
            "\n",
            "================================================================================\n",
            "All results saved to: sdk_results_20251106_202239\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create results directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_dir = Path(f\"sdk_results_{timestamp}\")\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Saving results to: {results_dir}\\n\")\n",
        "\n",
        "# Save individual result files\n",
        "for result in results:\n",
        "    # Create filename: {scenario_id}_{model}_{run_number}.json\n",
        "    filename = f\"{result.scenario_id}_{result.model.replace('/', '-')}_{result.run_number}.json\"\n",
        "    filepath = results_dir / filename\n",
        "    \n",
        "    # Convert result to dictionary (includes conversation history)\n",
        "    result_dict = result.to_dict()\n",
        "    \n",
        "    # Save to JSON\n",
        "    with open(filepath, \"w\") as f:\n",
        "        json.dump(result_dict, f, indent=2)\n",
        "    \n",
        "    status = \"✅\" if result.success else \"❌\"\n",
        "    print(f\"{status} Saved: {filename}\")\n",
        "\n",
        "# Create summary file\n",
        "summary = {\n",
        "    \"timestamp\": timestamp,\n",
        "    \"harness_file\": str(harness_path),\n",
        "    \"models\": models,\n",
        "    \"total_runs\": len(results),\n",
        "    \"results_by_model\": {},\n",
        "}\n",
        "\n",
        "for model, model_results in results_by_model.items():\n",
        "    total = len(model_results)\n",
        "    successful = sum(1 for r in model_results if r.success)\n",
        "    summary[\"results_by_model\"][model] = {\n",
        "        \"total\": total,\n",
        "        \"successful\": successful,\n",
        "        \"failed\": total - successful,\n",
        "        \"success_rate\": (successful / total * 100) if total > 0 else 0,\n",
        "        \"scenarios\": [\n",
        "            {\n",
        "                \"scenario_id\": r.scenario_id,\n",
        "                \"scenario_name\": r.scenario_name,\n",
        "                \"run_number\": r.run_number,\n",
        "                \"success\": r.success,\n",
        "            }\n",
        "            for r in model_results\n",
        "        ],\n",
        "    }\n",
        "\n",
        "summary_file = results_dir / \"summary.json\"\n",
        "with open(summary_file, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ Saved summary: {summary_file.name}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"All results saved to: {results_dir}\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examine Conversation History\n",
        "\n",
        "Let's look at the conversation history for one of the runs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CONVERSATION HISTORY: pm review task\n",
            "Model: claude-sonnet-4-5\n",
            "Success: True\n",
            "================================================================================\n",
            "\n",
            "Total conversation entries: 30\n",
            "\n",
            "\n",
            "[1] Type: message\n",
            "    Role: system\n",
            "    Content: You are an autonomous project-management agent operating inside an MCP server.\n",
            "You receive: (a) a user request and (b) a set of tool definitions (schemas, params, return types).\n",
            "Your goal is to comple...\n",
            "\n",
            "[2] Type: message\n",
            "    Role: user\n",
            "    Content: I'm Emily Davis. I just reviewed the testing results on WEB-3 and it's working correctly now. I need to mark it as resolved, add 'ready-for-deployment' label to it and leave a comment 'Registration fe...\n",
            "\n",
            "[3] Type: message\n",
            "    Role: assistant\n",
            "    Content: I'll help you complete these tasks for WEB-3 and WEB-9. Let me start by gathering the necessary information.\n",
            "\n",
            "[4] Type: tool_call\n",
            "    Tool: get_transitions\n",
            "    Args: {'issueIdOrKey': 'WEB-3'}\n",
            "\n",
            "[5] Type: tool_call\n",
            "    Tool: find_users\n",
            "    Args: {'query': 'Sarah Johnson'}\n",
            "\n",
            "[6] Type: tool_call\n",
            "    Tool: get_transitions\n",
            "    Args: {'issueIdOrKey': 'WEB-3'}\n",
            "\n",
            "[7] Type: tool_call\n",
            "    Tool: find_users\n",
            "    Args: {'query': 'Sarah Johnson'}\n",
            "\n",
            "[8] Type: tool_result\n",
            "    Tool: get_transitions\n",
            "    Output: {'transitions': [{'id': '11', 'name': 'Resolve with Fix', 'to': {'id': '3', 'name': 'Resolved', 'statusCategory': {'id': 3, 'key': 'done', 'name': 'Done'}}, 'hasScreen': True, 'isGlobal': False, 'isIn...\n",
            "\n",
            "[9] Type: tool_result\n",
            "    Tool: find_users\n",
            "    Output: [{'accountId': 'acc-dev-001', 'accountType': 'atlassian', 'displayName': 'Sarah Johnson', 'active': True, 'emailAddress': 'sarah.johnson@company.com', 'key': '', 'name': '', 'avatarUrls': {'48x48': 'h...\n",
            "\n",
            "[10] Type: message\n",
            "    Role: assistant\n",
            "    Content: Perfect! Now I'll proceed with the tasks:\n",
            "\n",
            "... (20 more entries)\n"
          ]
        }
      ],
      "source": [
        "# Pick the first result to examine\n",
        "if results:\n",
        "    example_result = results[0]\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(f\"CONVERSATION HISTORY: {example_result.scenario_name}\")\n",
        "    print(f\"Model: {example_result.model}\")\n",
        "    print(f\"Success: {example_result.success}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Get conversation history\n",
        "    conversation = example_result.get_conversation_history()\n",
        "    \n",
        "    print(f\"\\nTotal conversation entries: {len(conversation)}\\n\")\n",
        "    \n",
        "    # Display first few entries\n",
        "    for i, entry in enumerate(conversation[:10], 1):\n",
        "        print(f\"\\n[{i}] Type: {entry.get('type')}\")\n",
        "        \n",
        "        if entry[\"type\"] == \"message\":\n",
        "            role = entry.get(\"role\", \"unknown\")\n",
        "            content = entry.get(\"content\", \"\")\n",
        "            # Truncate long content\n",
        "            content_preview = content[:200] + \"...\" if len(content) > 200 else content\n",
        "            print(f\"    Role: {role}\")\n",
        "            print(f\"    Content: {content_preview}\")\n",
        "        \n",
        "        elif entry[\"type\"] == \"tool_call\":\n",
        "            tool = entry.get(\"tool\", \"unknown\")\n",
        "            args = entry.get(\"args\", {})\n",
        "            print(f\"    Tool: {tool}\")\n",
        "            print(f\"    Args: {args}\")\n",
        "        \n",
        "        elif entry[\"type\"] == \"tool_result\":\n",
        "            tool = entry.get(\"tool\", \"unknown\")\n",
        "            output = entry.get(\"output\", {})\n",
        "            # Truncate large outputs\n",
        "            output_str = str(output)[:200] + \"...\" if len(str(output)) > 200 else str(output)\n",
        "            print(f\"    Tool: {tool}\")\n",
        "            print(f\"    Output: {output_str}\")\n",
        "    \n",
        "    if len(conversation) > 10:\n",
        "        print(f\"\\n... ({len(conversation) - 10} more entries)\")\n",
        "else:\n",
        "    print(\"No results to display.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Analysis\n",
        "\n",
        "You can perform additional analysis on the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "RESULTS DATAFRAME\n",
            "================================================================================\n",
            "               model        scenario  run  success  tool_calls  messages  \\\n",
            "0  claude-sonnet-4-5  pm review task    1     True          16         6   \n",
            "\n",
            "   verifiers_passed  verifiers_total  \n",
            "0                 6                6  \n",
            "\n",
            "================================================================================\n",
            "STATISTICS BY MODEL\n",
            "================================================================================\n",
            "                  success            tool_calls         messages        \n",
            "                      sum count mean       mean min max     mean min max\n",
            "model                                                                   \n",
            "claude-sonnet-4-5       1     1  1.0       16.0  16  16      6.0   6   6\n",
            "\n",
            "✅ Saved analysis to: sdk_results_20251106_202239/results_analysis.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame for easy analysis\n",
        "data = []\n",
        "for result in results:\n",
        "    # Count conversation elements\n",
        "    conversation = result.get_conversation_history()\n",
        "    tool_calls = sum(1 for entry in conversation if entry[\"type\"] == \"tool_call\")\n",
        "    messages = sum(1 for entry in conversation if entry[\"type\"] == \"message\")\n",
        "    \n",
        "    # Count verifier results\n",
        "    verifiers_passed = sum(1 for vr in result.verifier_results if vr.success)\n",
        "    verifiers_total = len(result.verifier_results)\n",
        "    \n",
        "    data.append({\n",
        "        \"model\": result.model,\n",
        "        \"scenario\": result.scenario_name,\n",
        "        \"run\": result.run_number,\n",
        "        \"success\": result.success,\n",
        "        \"tool_calls\": tool_calls,\n",
        "        \"messages\": messages,\n",
        "        \"verifiers_passed\": verifiers_passed,\n",
        "        \"verifiers_total\": verifiers_total,\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RESULTS DATAFRAME\")\n",
        "print(\"=\"*80)\n",
        "print(df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATISTICS BY MODEL\")\n",
        "print(\"=\"*80)\n",
        "print(df.groupby(\"model\").agg({\n",
        "    \"success\": [\"sum\", \"count\", \"mean\"],\n",
        "    \"tool_calls\": [\"mean\", \"min\", \"max\"],\n",
        "    \"messages\": [\"mean\", \"min\", \"max\"],\n",
        "}))\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "csv_file = results_dir / \"results_analysis.csv\"\n",
        "df.to_csv(csv_file, index=False)\n",
        "print(f\"\\n✅ Saved analysis to: {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running Multiple Harness Files\n",
        "\n",
        "You can also run multiple harness files by pointing to a directory:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory harness example ready (uncomment to run)\n"
          ]
        }
      ],
      "source": [
        "async def run_directory_harness():\n",
        "    \"\"\"Run all harness files in a directory.\"\"\"\n",
        "    \n",
        "    harness_dir = Path(\"9_tasks\")  # Directory with multiple .json files\n",
        "    \n",
        "    if not harness_dir.exists():\n",
        "        print(f\"Directory not found: {harness_dir}\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"Running harness directory: {harness_dir}\")\n",
        "    \n",
        "    harness = TestHarness(\n",
        "        harness_path=harness_dir,\n",
        "        config=config,\n",
        "    )\n",
        "    \n",
        "    print(f\"Loaded {len(harness.scenarios)} total scenarios from directory\")\n",
        "    print(f\"From {len(harness.file_map)} harness files:\")\n",
        "    for filename, scenarios in harness.file_map.items():\n",
        "        print(f\"  - {filename}: {len(scenarios)} scenario(s)\")\n",
        "    \n",
        "    # Run all scenarios\n",
        "    results = await harness.run(\n",
        "        models=models,\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nCompleted {len(results)} total runs\")\n",
        "    return results\n",
        "\n",
        "# Uncomment to run:\n",
        "directory_results = await run_directory_harness()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Custom Agents\n",
        "\n",
        "The SDK allows you to create custom agents for any LLM provider. Here's an example based on the Qwen implementation from the CLI:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Any, Optional\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_core.messages import AIMessage, BaseMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from mcp_benchmark_sdk import Agent, AgentResponse\n",
        "from mcp_benchmark_sdk.parsers import OpenAIResponseParser, ResponseParser\n",
        "from mcp_benchmark_sdk.utils import retry_with_backoff\n",
        "\n",
        "\n",
        "class QwenAgent(Agent):\n",
        "    \"\"\"Agent implementation for Alibaba Cloud Qwen models.\n",
        "\n",
        "    Uses OpenAI-compatible API from DashScope.\n",
        "    Requires DASHSCOPE_API_KEY environment variable.\n",
        "\n",
        "    Supported models:\n",
        "    - qwen-plus (default)\n",
        "    - qwen3-14b\n",
        "    - qwen-turbo\n",
        "    - qwen-max\n",
        "    - qwen2.5-72b-instruct\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"qwen-plus\",\n",
        "        temperature: float = 0.1,\n",
        "        max_output_tokens: Optional[int] = None,\n",
        "        tool_call_limit: int = 1000,\n",
        "        system_prompt: Optional[str] = None,\n",
        "        base_url: Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Initialize Qwen agent.\n",
        "\n",
        "        Args:\n",
        "            model: Qwen model name\n",
        "            temperature: Sampling temperature\n",
        "            max_output_tokens: Maximum output tokens\n",
        "            tool_call_limit: Maximum tool calls per run\n",
        "            system_prompt: Optional system prompt for the agent\n",
        "            base_url: Optional custom base URL (default: Singapore endpoint)\n",
        "            **kwargs: Additional arguments for ChatOpenAI\n",
        "        \"\"\"\n",
        "        # Pass system_prompt and tool_call_limit to parent Agent class\n",
        "        super().__init__(system_prompt=system_prompt, tool_call_limit=tool_call_limit)\n",
        "        \n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_output_tokens = max_output_tokens\n",
        "        \n",
        "        # Map common model names to actual DashScope model IDs\n",
        "        model_map = {\n",
        "            \"qwen-14b\": \"qwen3-14b\",\n",
        "            \"qwen3-14b\": \"qwen3-14b\",\n",
        "            \"qwen-plus\": \"qwen-plus\",\n",
        "            \"qwen-turbo\": \"qwen-turbo\",\n",
        "            \"qwen-max\": \"qwen-max\",\n",
        "            \"qwen2.5-72b-instruct\": \"qwen2.5-72b-instruct\",\n",
        "        }\n",
        "        \n",
        "        self.actual_model = model_map.get(model.lower(), model)\n",
        "        \n",
        "        # Get base URL from environment or use default (Singapore)\n",
        "        self.base_url = base_url or os.environ.get(\n",
        "            \"DASHSCOPE_BASE_URL\",\n",
        "            \"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
        "        )\n",
        "        \n",
        "        # Get API key\n",
        "        self.api_key = os.environ.get(\"DASHSCOPE_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise EnvironmentError(\n",
        "                \"DASHSCOPE_API_KEY is not set. Export the API key before running Qwen models.\\n\"\n",
        "                \"Get your API key from: https://modelstudio.console.alibabacloud.com/?tab=model#/api-key\\n\"\n",
        "                \"Set it with: export DASHSCOPE_API_KEY='your-key-here'\"\n",
        "            )\n",
        "        \n",
        "        self.extra_kwargs = kwargs\n",
        "\n",
        "    def _build_llm(self) -> BaseChatModel:\n",
        "        \"\"\"Build Qwen model using OpenAI-compatible interface.\"\"\"\n",
        "        config: dict[str, Any] = {\n",
        "            \"model\": self.actual_model,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"timeout\": None,\n",
        "            \"max_retries\": 3,\n",
        "            \"base_url\": self.base_url,\n",
        "            \"api_key\": self.api_key,\n",
        "        }\n",
        "\n",
        "        if self.max_output_tokens is not None:\n",
        "            config[\"max_completion_tokens\"] = self.max_output_tokens\n",
        "\n",
        "        # Pass through any additional kwargs\n",
        "        config.update(self.extra_kwargs)\n",
        "        \n",
        "        # Disable thinking in non-streaming mode\n",
        "        extra_body = config.get(\"extra_body\", {})\n",
        "        extra_body[\"enable_thinking\"] = False\n",
        "        config[\"extra_body\"] = extra_body\n",
        "\n",
        "        llm = ChatOpenAI(**config)\n",
        "        return llm.bind_tools(self._tools) if self._tools else llm\n",
        "\n",
        "    async def get_response(self, messages: list[BaseMessage]) -> tuple[AgentResponse, AIMessage]:\n",
        "        \"\"\"Get Qwen model response with retry logic.\"\"\"\n",
        "        if not self._llm:\n",
        "            raise RuntimeError(\"LLM not initialized. Call initialize() first.\")\n",
        "\n",
        "        async def _invoke():\n",
        "            return await self._llm.ainvoke(messages)\n",
        "\n",
        "        ai_message = await retry_with_backoff(\n",
        "            _invoke,\n",
        "            max_retries=2,\n",
        "            timeout_seconds=600.0,\n",
        "            on_retry=lambda attempt, exc, delay: None,\n",
        "        )\n",
        "\n",
        "        # Parse response (use OpenAI parser since it's compatible)\n",
        "        parser = self.get_response_parser()\n",
        "        parsed = parser.parse(ai_message)\n",
        "\n",
        "        agent_response = AgentResponse(\n",
        "            content=parsed.content,\n",
        "            tool_calls=parsed.tool_calls,\n",
        "            reasoning=\"\\n\".join(parsed.reasoning) if parsed.reasoning else None,\n",
        "            done=not bool(parsed.tool_calls),\n",
        "            info={\"raw_reasoning\": parsed.raw_reasoning},\n",
        "        )\n",
        "\n",
        "        return agent_response, ai_message\n",
        "\n",
        "    def get_response_parser(self) -> ResponseParser:\n",
        "        \"\"\"Get OpenAI-compatible response parser.\"\"\"\n",
        "        return OpenAIResponseParser()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Custom Agents with TestHarness\n",
        "\n",
        "Now let's create a custom agent factory that uses our `QwenAgent`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_agent_factory(\n",
        "    model: str,\n",
        "    temperature: float = 0.1,\n",
        "    max_output_tokens: int | None = None,\n",
        "    tool_call_limit: int = 1000,\n",
        "    system_prompt: str | None = None,\n",
        "    **kwargs,\n",
        ") -> Agent:\n",
        "    \"\"\"Custom agent factory that supports Qwen models.\n",
        "    \n",
        "    This factory checks the model name and creates the appropriate agent:\n",
        "    - Qwen models -> QwenAgent\n",
        "    - Other models -> Use SDK's default create_agent\n",
        "    \n",
        "    Args:\n",
        "        model: Model name (e.g., \"qwen-plus\", \"claude-sonnet-4-5\", \"gpt-4o\")\n",
        "        temperature: Sampling temperature\n",
        "        max_output_tokens: Maximum output tokens\n",
        "        tool_call_limit: Maximum tool calls per run\n",
        "        system_prompt: Optional system prompt\n",
        "        **kwargs: Additional model-specific arguments\n",
        "    \n",
        "    Returns:\n",
        "        Agent instance for the specified model\n",
        "    \"\"\"\n",
        "    model_lower = model.lower()\n",
        "    \n",
        "    # Check if it's a Qwen model\n",
        "    if model_lower.startswith(\"qwen\"):\n",
        "        print(f\"Creating QwenAgent for model: {model}\")\n",
        "        return QwenAgent(\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            max_output_tokens=max_output_tokens,\n",
        "            tool_call_limit=tool_call_limit,\n",
        "            system_prompt=system_prompt,\n",
        "            **kwargs,\n",
        "        )\n",
        "    \n",
        "    # For other models, use the SDK's default factory\n",
        "    print(f\"Creating default agent for model: {model}\")\n",
        "    return create_agent(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        tool_call_limit=tool_call_limit,\n",
        "        system_prompt=system_prompt,\n",
        "        **kwargs,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running with Custom Agent\n",
        "\n",
        "Now you can use your custom agent factory with the test harness:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_with_custom_agent():\n",
        "    \"\"\"Example: Run harness with custom agent factory.\"\"\"\n",
        "    \n",
        "    # Create harness\n",
        "    harness = TestHarness(\n",
        "        harness_path=simple_harness_path,\n",
        "        config=config,\n",
        "    )\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"RUNNING WITH CUSTOM AGENT FACTORY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Run with multiple models including Qwen\n",
        "    # NOTE: You need DASHSCOPE_API_KEY set to run Qwen models\n",
        "    models_to_test = [\n",
        "        \"claude-sonnet-4-5\",  # Uses default ClaudeAgent\n",
        "        # \"qwen-plus\",           # Uses custom QwenAgent (uncomment if you have API key)\n",
        "        # \"gpt-4o\",              # Uses default OpenAIAgent\n",
        "    ]\n",
        "    \n",
        "    results = await harness.run(\n",
        "        models=models_to_test,\n",
        "        agent_factory=custom_agent_factory,  # Use custom factory instead of create_agent\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for result in results:\n",
        "        status = \"✅ PASSED\" if result.success else \"❌ FAILED\"\n",
        "        print(f\"{status} | {result.model} | {result.scenario_name}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Example: Run with custom agent\n",
        "# Uncomment to run:\n",
        "# custom_results = await run_with_custom_agent()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Concepts for Custom Agents\n",
        "\n",
        "When creating a custom agent, you need to implement:\n",
        "\n",
        "1. **`__init__()`**: Initialize your agent\n",
        "   - Call `super().__init__(system_prompt, tool_call_limit)` \n",
        "   - Store model configuration (API keys, base URLs, etc.)\n",
        "   - Validate environment variables\n",
        "\n",
        "2. **`_build_llm()`**: Build the LangChain model\n",
        "   - Create and configure your LLM (e.g., `ChatOpenAI`, `ChatAnthropic`)\n",
        "   - Bind tools to the model: `llm.bind_tools(self._tools)`\n",
        "   - Return the configured model\n",
        "\n",
        "3. **`get_response()`**: Get model responses\n",
        "   - Call `await self._llm.ainvoke(messages)`\n",
        "   - Add retry logic using `retry_with_backoff()`\n",
        "   - Parse the response and return `AgentResponse`\n",
        "\n",
        "4. **`get_response_parser()`**: Parse model responses\n",
        "   - Return appropriate parser (e.g., `OpenAIResponseParser()`)\n",
        "   - Or create custom parser for your model's response format\n",
        "\n",
        "**Agent Factory Pattern:**\n",
        "\n",
        "The agent factory is a function that creates agents based on the model name:\n",
        "\n",
        "```python\n",
        "def my_factory(model: str, **kwargs) -> Agent:\n",
        "    if \"custom-model\" in model:\n",
        "        return CustomAgent(model=model, **kwargs)\n",
        "    return create_agent(model=model, **kwargs)  # Fallback to SDK default\n",
        "```\n",
        "\n",
        "This allows you to:\n",
        "- Support multiple LLM providers\n",
        "- Use different configurations per model\n",
        "- Mix custom and built-in agents\n",
        "- Keep agent creation logic centralized\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. ✅ **Configuration**: Setting up MCP servers and harness configuration\n",
        "2. ✅ **Running**: Executing benchmarks across models\n",
        "3. ✅ **Observers**: Using custom observers for progress tracking\n",
        "4. ✅ **Results**: Analyzing success rates and verifier results\n",
        "5. ✅ **Saving**: Persisting results to JSON files\n",
        "6. ✅ **Conversation History**: Examining agent interactions\n",
        "7. ✅ **Analysis**: Using pandas for additional insights\n",
        "8. ✅ **Custom Agents**: Creating custom agents for new LLM providers (Qwen example)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Create your own harness files with custom scenarios\n",
        "- Add custom verifiers for domain-specific validation\n",
        "- Implement custom observers for rich UI feedback\n",
        "- Compare results across different models and configurations\n",
        "- Create custom agents for additional LLM providers\n",
        "- Use the SDK in your own benchmarking pipeline\n",
        "\n",
        "For more information, see:\n",
        "- SDK Documentation: `packages/mcp_benchmark_sdk/README.md`\n",
        "- Harness Design: `packages/mcp_benchmark_sdk/src/mcp_benchmark_sdk/harness/README.md`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
