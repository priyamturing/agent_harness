{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCP Benchmark SDK - Complete Usage Guide\n",
        "\n",
        "This notebook demonstrates **all usage patterns** for the MCP Benchmark SDK, from the simplest harness to custom agents and verifiers.\n",
        "\n",
        "**Table of Contents:**\n",
        "1. [Setup](#setup)\n",
        "2. [Pattern 1: Simplest Harness](#pattern-1-simplest-harness)\n",
        "3. [Pattern 2: Using Agents Without Harness](#pattern-2-using-agents-without-harness)\n",
        "4. [Pattern 3: Custom Agent (Qwen Example)](#pattern-3-custom-agent-qwen-example)\n",
        "5. [Pattern 4: Custom Verifier](#pattern-4-custom-verifier)\n",
        "6. [Pattern 5: Full Workflow with Harness + Custom Components](#pattern-5-full-workflow-with-harness--custom-components)\n",
        "7. [Pattern 6: Multiple Models Comparison](#pattern-6-multiple-models-comparison)\n",
        "8. [Pattern 7: Observers and Progress Tracking](#pattern-7-observers-and-progress-tracking)\n",
        "\n",
        "**Focus**: The SDK is **harness-first**. The harness orchestrates everything - you define scenarios and let it run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Reference\n",
        "\n",
        "**Minimal harness usage** - The simplest way to get started:\n",
        "\n",
        "```python\n",
        "from pathlib import Path\n",
        "from mcp_benchmark_sdk import TestHarness, TestHarnessConfig, MCPConfig, create_agent\n",
        "\n",
        "harness = TestHarness(\n",
        "    harness_path=Path(\"task.json\"),\n",
        "    config=TestHarnessConfig(\n",
        "        mcp=MCPConfig(name=\"jira\", url=\"http://localhost:8015/mcp\", transport=\"streamable_http\")\n",
        "    )\n",
        ")\n",
        "\n",
        "results = await harness.run(models=[\"gpt-4o\"], agent_factory=create_agent)\n",
        "```\n",
        "\n",
        "**That's it!** The harness handles the rest: agent creation, MCP connections, execution, verification, and metrics.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the SDK and set up API keys:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Install SDK (if not already installed)\n",
        "# !pip install mcp-benchmark-sdk\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up API keys (replace with your actual keys or load from .env)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your-google-key\"\n",
        "# os.environ[\"DASHSCOPE_API_KEY\"] = \"your-dashscope-key\"  # For Qwen\n",
        "\n",
        "print(\"‚úì Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 1: Simplest Harness\n",
        "\n",
        "**The recommended way to use the SDK.** Create a harness file and let the SDK handle everything.\n",
        "\n",
        "### Step 1: Create a Simple Harness File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Created simple_task.json\n"
          ]
        }
      ],
      "source": [
        "# Create a simple harness file\n",
        "simple_harness = {\n",
        "    \"scenarios\": [\n",
        "        {\n",
        "            \"scenario_id\": \"create_bug\",\n",
        "            \"name\": \"Create Bug Issue\",\n",
        "            \"description\": \"Test if agent can create a bug issue\",\n",
        "            \"prompts\": [\n",
        "                {\n",
        "                    \"prompt_text\": \"Create a bug issue in project DEMO with summary 'Login button not working' and description 'Users cannot click the login button'\",\n",
        "                    \"expected_tools\": [\"create_issue\"],\n",
        "                    \"verifier\": {\n",
        "                        \"verifier_type\": \"database_state\",\n",
        "                        \"validation_config\": {\n",
        "                            \"query\": \"SELECT COUNT(*) FROM issue WHERE summary = 'Login button not working'\",\n",
        "                            \"expected_value\": 1,\n",
        "                            \"comparison_type\": \"equals\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ],\n",
        "            \"metadata\": {\"difficulty\": \"easy\"},\n",
        "            \"conversation_mode\": False\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "with open(\"simple_task.json\", \"w\") as f:\n",
        "    json.dump(simple_harness, f, indent=2)\n",
        "\n",
        "print(\"‚úì Created simple_task.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Run the Harness\n",
        "\n",
        "This is the **core pattern** - everything else is built on top of this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 scenario(s)\n",
            "\n",
            "gpt-4o - create_bug: ‚úó FAIL\n",
            "  Steps: 2\n",
            "  Database ID: 99073ca8-9f71-4204-98a6-0082182611dc\n",
            "  Error: Verifiers failed: DatabaseVerifier\n",
            "  Verifiers:\n",
            "    ‚úó DatabaseVerifier: Expected 1, Got 0\n",
            "\n",
            "‚úì Harness completed! Pass rate: 0/1\n"
          ]
        }
      ],
      "source": [
        "from mcp_benchmark_sdk import TestHarness, TestHarnessConfig, MCPConfig, create_agent\n",
        "\n",
        "async def run_simple_harness():\n",
        "    # Configure MCP server\n",
        "    mcp_config = MCPConfig(\n",
        "        name=\"jira\",\n",
        "        url=\"http://localhost:8015/mcp\",\n",
        "        transport=\"streamable_http\"\n",
        "    )\n",
        "    \n",
        "    # Create harness\n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(\"simple_task.json\"),\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=mcp_config,\n",
        "            max_steps=50,\n",
        "            tool_call_limit=100,\n",
        "            runs_per_scenario=1,\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    print(f\"Loaded {len(harness.scenarios)} scenario(s)\")\n",
        "    \n",
        "    # Run benchmarks\n",
        "    results = await harness.run(\n",
        "        models=[\"gpt-4o\"],\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    \n",
        "    # Print results\n",
        "    for result in results:\n",
        "        status = \"‚úì PASS\" if result.success else \"‚úó FAIL\"\n",
        "        print(f\"\\n{result.model} - {result.scenario_id}: {status}\")\n",
        "        print(f\"  Steps: {result.result.metadata.get('steps')}\")\n",
        "        print(f\"  Database ID: {result.result.database_id}\")\n",
        "        \n",
        "        if result.error:\n",
        "            print(f\"  Error: {result.error}\")\n",
        "        \n",
        "        # Show verifier results\n",
        "        if result.verifier_results:\n",
        "            print(f\"  Verifiers:\")\n",
        "            for v in result.verifier_results:\n",
        "                v_status = \"‚úì\" if v.success else \"‚úó\"\n",
        "                print(f\"    {v_status} {v.name}: Expected {v.expected_value}, Got {v.actual_value}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run it\n",
        "results = await run_simple_harness()\n",
        "print(f\"\\n‚úì Harness completed! Pass rate: {sum(r.success for r in results)}/{len(results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**That's it!** The harness handled:\n",
        "- Loading the scenario\n",
        "- Creating the agent\n",
        "- Connecting to MCP\n",
        "- Running the task\n",
        "- Verifying the result\n",
        "- Collecting metrics\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 2: Using Agents Without Harness\n",
        "\n",
        "**Best for:** One-off tasks, interactive testing, custom workflows\n",
        "\n",
        "You can use agents directly without the harness:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gw/zj2q1xwn18nd9922p394wrb00000gn/T/ipykernel_59153/2260787270.py:5: UserWarning: Claude thinking mode requires temperature=1.0. Your temperature=0.1 will be overridden. Set temperature=1.0 or enable_thinking=False to suppress this warning.\n",
            "  agent = ClaudeAgent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success: True\n",
            "Steps: 3\n",
            "Database ID: 691041b0-252a-43df-aca4-e65c61e0b23a\n",
            "\n",
            "Conversation (10 entries):\n",
            "  1. user: Create a bug issue in project DEMO titled 'Homepage not load...\n",
            "  2. assistant: I'll create a bug issue in the DEMO project with the specifi...\n",
            "  3. Tool: create_issue\n",
            "  4. Tool: create_issue\n",
            "  5. Tool result: create_issue\n",
            "\n",
            "‚úì Direct agent execution complete!\n"
          ]
        }
      ],
      "source": [
        "from mcp_benchmark_sdk import ClaudeAgent, Task, MCPConfig\n",
        "\n",
        "async def run_agent_directly():\n",
        "    # Create agent\n",
        "    agent = ClaudeAgent(\n",
        "        model=\"claude-sonnet-4-5\",\n",
        "        temperature=0.1,\n",
        "        tool_call_limit=100,\n",
        "    )\n",
        "    \n",
        "    # Define task\n",
        "    task = Task(\n",
        "        prompt=\"Create a bug issue in project DEMO titled 'Homepage not loading' with description 'Users report 500 error'\",\n",
        "        mcp=MCPConfig(\n",
        "            name=\"jira\",\n",
        "            url=\"http://localhost:8015/mcp\",\n",
        "            transport=\"streamable_http\"\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Run task\n",
        "    result = await agent.run(task, max_steps=50)\n",
        "    \n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Steps: {result.metadata.get('steps')}\")\n",
        "    print(f\"Database ID: {result.database_id}\")\n",
        "    \n",
        "    if result.error:\n",
        "        print(f\"Error: {result.error}\")\n",
        "    \n",
        "    # Access conversation history\n",
        "    conversation = result.get_conversation_history()\n",
        "    print(f\"\\nConversation ({len(conversation)} entries):\")\n",
        "    for i, entry in enumerate(conversation[:5]):  # Show first 5\n",
        "        if entry[\"type\"] == \"message\":\n",
        "            print(f\"  {i+1}. {entry['role']}: {entry['content'][:60]}...\")\n",
        "        elif entry[\"type\"] == \"tool_call\":\n",
        "            print(f\"  {i+1}. Tool: {entry['tool']}\")\n",
        "        elif entry[\"type\"] == \"tool_result\":\n",
        "            print(f\"  {i+1}. Tool result: {entry['tool']}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "result = await run_agent_directly()\n",
        "print(\"\\n‚úì Direct agent execution complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manual Verification\n",
        "\n",
        "When using agents directly, you can manually verify results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mcp_benchmark_sdk import DatabaseVerifier\n",
        "\n",
        "async def verify_result(result):\n",
        "    # Create verifier\n",
        "    verifier = DatabaseVerifier(\n",
        "        query=\"SELECT COUNT(*) FROM issue WHERE summary = 'Homepage not loading'\",\n",
        "        expected_value=1,\n",
        "        mcp_url=\"http://localhost:8015/mcp\",\n",
        "        database_id=result.database_id,  # Use same database as task\n",
        "        comparison=\"equals\"\n",
        "    )\n",
        "    \n",
        "    # Run verification\n",
        "    verifier_result = await verifier.verify()\n",
        "    \n",
        "    print(f\"Verified: {verifier_result.success}\")\n",
        "    print(f\"Expected: {verifier_result.expected_value}, Got: {verifier_result.actual_value}\")\n",
        "    if verifier_result.error:\n",
        "        print(f\"Error: {verifier_result.error}\")\n",
        "    \n",
        "    return verifier_result\n",
        "\n",
        "# Verify the previous result\n",
        "verifier_result = await verify_result(result)\n",
        "print(\"\\n‚úì Verification complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mcp_benchmark_sdk import Agent, AgentResponse\n",
        "from mcp_benchmark_sdk.parsers import OpenAIResponseParser, ResponseParser\n",
        "from mcp_benchmark_sdk.utils import retry_with_backoff\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import BaseMessage, AIMessage\n",
        "\n",
        "class QwenAgent(Agent):\n",
        "    \"\"\"Custom agent for Alibaba Cloud Qwen models.\n",
        "    \n",
        "    Uses OpenAI-compatible API from DashScope.\n",
        "    Requires DASHSCOPE_API_KEY environment variable.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"qwen-plus\",\n",
        "        temperature: float = 0.1,\n",
        "        max_output_tokens: int | None = None,\n",
        "        tool_call_limit: int = 1000,\n",
        "        system_prompt: str | None = None,\n",
        "    ):\n",
        "        super().__init__(system_prompt=system_prompt, tool_call_limit=tool_call_limit)\n",
        "        \n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_output_tokens = max_output_tokens\n",
        "        \n",
        "        # Model mapping\n",
        "        model_map = {\n",
        "            \"qwen-14b\": \"qwen3-14b\",\n",
        "            \"qwen-plus\": \"qwen-plus\",\n",
        "            \"qwen-turbo\": \"qwen-turbo\",\n",
        "            \"qwen-max\": \"qwen-max\",\n",
        "        }\n",
        "        self.actual_model = model_map.get(model.lower(), model)\n",
        "        \n",
        "        # Get base URL and API key\n",
        "        self.base_url = os.environ.get(\n",
        "            \"DASHSCOPE_BASE_URL\",\n",
        "            \"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
        "        )\n",
        "        self.api_key = os.environ.get(\"DASHSCOPE_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            print(\"‚ö†Ô∏è  DASHSCOPE_API_KEY not set - agent will fail at runtime\")\n",
        "    \n",
        "    def _build_llm(self):\n",
        "        \"\"\"Build LLM client (called during agent.initialize()).\"\"\"\n",
        "        config = {\n",
        "            \"model\": self.actual_model,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"timeout\": None,\n",
        "            \"max_retries\": 3,\n",
        "            \"base_url\": self.base_url,\n",
        "            \"api_key\": self.api_key,\n",
        "        }\n",
        "        \n",
        "        if self.max_output_tokens is not None:\n",
        "            config[\"max_completion_tokens\"] = self.max_output_tokens\n",
        "        \n",
        "        # Disable thinking mode for non-streaming\n",
        "        config[\"extra_body\"] = {\"enable_thinking\": False}\n",
        "        \n",
        "        llm = ChatOpenAI(**config)\n",
        "        # Bind tools to LLM (self._tools is set during initialize())\n",
        "        return llm.bind_tools(self._tools) if self._tools else llm\n",
        "    \n",
        "    async def get_response(self, messages: list[BaseMessage]) -> tuple[AgentResponse, AIMessage]:\n",
        "        \"\"\"Get model response with retry logic.\"\"\"\n",
        "        if not self._llm:\n",
        "            raise RuntimeError(\"LLM not initialized. Call initialize() first.\")\n",
        "        \n",
        "        # Call LLM with retry logic\n",
        "        async def _invoke():\n",
        "            return await self._llm.ainvoke(messages)\n",
        "        \n",
        "        ai_message = await retry_with_backoff(\n",
        "            _invoke,\n",
        "            max_retries=2,\n",
        "            timeout_seconds=600.0,\n",
        "            on_retry=lambda attempt, exc, delay: None,\n",
        "        )\n",
        "        \n",
        "        # Parse response using OpenAI parser (compatible format)\n",
        "        parser = self.get_response_parser()\n",
        "        parsed = parser.parse(ai_message)\n",
        "        \n",
        "        # Convert to AgentResponse\n",
        "        agent_response = AgentResponse(\n",
        "            content=parsed.content,\n",
        "            tool_calls=parsed.tool_calls,\n",
        "            reasoning=\"\\n\".join(parsed.reasoning) if parsed.reasoning else None,\n",
        "            done=not bool(parsed.tool_calls),\n",
        "        )\n",
        "        \n",
        "        return agent_response, ai_message\n",
        "    \n",
        "    def get_response_parser(self) -> ResponseParser:\n",
        "        \"\"\"Get OpenAI-compatible response parser.\"\"\"\n",
        "        return OpenAIResponseParser()\n",
        "\n",
        "print(\"‚úì QwenAgent class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_agent_factory(model: str, **kwargs):\n",
        "    \"\"\"Factory function for creating agents (including custom ones).\"\"\"\n",
        "    if model.startswith(\"qwen\"):\n",
        "        return QwenAgent(model=model, **kwargs)\n",
        "    else:\n",
        "        # Fall back to built-in agents\n",
        "        return create_agent(model, **kwargs)\n",
        "\n",
        "async def run_harness_with_custom_agent():\n",
        "    \"\"\"Run harness with custom agent factory.\"\"\"\n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(\"simple_task.json\"),\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=MCPConfig(\n",
        "                name=\"jira\",\n",
        "                url=\"http://localhost:8015/mcp\",\n",
        "                transport=\"streamable_http\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Run with both custom (Qwen) and built-in (GPT) agents\n",
        "    results = await harness.run(\n",
        "        models=[\"gpt-4o\"],  # Add \"qwen-plus\" if you have DASHSCOPE_API_KEY\n",
        "        agent_factory=custom_agent_factory,\n",
        "    )\n",
        "    \n",
        "    for result in results:\n",
        "        status = \"‚úì PASS\" if result.success else \"‚úó FAIL\"\n",
        "        print(f\"{result.model}: {status}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run it\n",
        "# results = await run_harness_with_custom_agent()\n",
        "print(\"‚úì Custom agent factory ready! Now the harness can use ANY agent you create.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 4: Custom Verifier\n",
        "\n",
        "**Best for:** Complex validation logic, custom result checks\n",
        "\n",
        "Beyond database queries, you can create verifiers for any validation logic:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mcp_benchmark_sdk import Verifier, VerifierResult\n",
        "import httpx\n",
        "\n",
        "class APIResponseVerifier(Verifier):\n",
        "    \"\"\"Verify that an API endpoint returns expected data.\n",
        "    \n",
        "    Example: Check if an issue was created by querying the API directly.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        endpoint: str,\n",
        "        expected_field: str,\n",
        "        expected_value: any,\n",
        "        name: str | None = None\n",
        "    ):\n",
        "        super().__init__(name or \"APIResponseVerifier\")\n",
        "        self.endpoint = endpoint\n",
        "        self.expected_field = expected_field\n",
        "        self.expected_value = expected_value\n",
        "    \n",
        "    async def verify(self) -> VerifierResult:\n",
        "        \"\"\"Execute verification.\"\"\"\n",
        "        try:\n",
        "            async with httpx.AsyncClient() as client:\n",
        "                response = await client.get(self.endpoint)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                \n",
        "                actual_value = data.get(self.expected_field)\n",
        "                success = actual_value == self.expected_value\n",
        "                \n",
        "                return VerifierResult(\n",
        "                    name=self.name,\n",
        "                    success=success,\n",
        "                    expected_value=self.expected_value,\n",
        "                    actual_value=actual_value,\n",
        "                    comparison_type=\"equals\",\n",
        "                    error=None if success else \"Value mismatch\",\n",
        "                )\n",
        "        except Exception as exc:\n",
        "            return VerifierResult(\n",
        "                name=self.name,\n",
        "                success=False,\n",
        "                expected_value=self.expected_value,\n",
        "                actual_value=None,\n",
        "                comparison_type=\"equals\",\n",
        "                error=str(exc),\n",
        "            )\n",
        "\n",
        "# Test it\n",
        "async def test_custom_verifier():\n",
        "    verifier = APIResponseVerifier(\n",
        "        endpoint=\"https://jsonplaceholder.typicode.com/todos/1\",\n",
        "        expected_field=\"userId\",\n",
        "        expected_value=1,\n",
        "        name=\"Check User ID\"\n",
        "    )\n",
        "    \n",
        "    result = await verifier.verify()\n",
        "    print(f\"‚úì {result.name}: {result.success}\")\n",
        "    print(f\"  Expected: {result.expected_value}, Got: {result.actual_value}\")\n",
        "\n",
        "await test_custom_verifier()\n",
        "print(\"\\n‚úì Custom verifier works!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 5: Observers for Progress Tracking\n",
        "\n",
        "**Best for:** Real-time monitoring, debugging, custom logging\n",
        "\n",
        "Add observers to track execution in real-time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mcp_benchmark_sdk import RunObserver\n",
        "\n",
        "class DetailedObserver(RunObserver):\n",
        "    \"\"\"Observer that tracks everything with nice formatting.\"\"\"\n",
        "    \n",
        "    def __init__(self, label: str):\n",
        "        self.label = label\n",
        "        self.stats = {\n",
        "            \"messages\": 0,\n",
        "            \"tool_calls\": 0,\n",
        "            \"tool_errors\": 0,\n",
        "        }\n",
        "    \n",
        "    async def on_message(self, role: str, content: str, metadata=None):\n",
        "        self.stats[\"messages\"] += 1\n",
        "        if role == \"assistant\":\n",
        "            print(f\"[{self.label}] üí¨ Agent: {content[:80]}...\")\n",
        "    \n",
        "    async def on_tool_call(self, tool_name, arguments, result, is_error=False):\n",
        "        self.stats[\"tool_calls\"] += 1\n",
        "        if is_error:\n",
        "            self.stats[\"tool_errors\"] += 1\n",
        "        status = \"‚úó\" if is_error else \"‚úì\"\n",
        "        print(f\"[{self.label}] üîß Tool {status}: {tool_name}\")\n",
        "    \n",
        "    async def on_status(self, message: str, level: str = \"info\"):\n",
        "        emoji = {\"info\": \"‚ÑπÔ∏è\", \"warning\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\"}.get(level, \"‚ÑπÔ∏è\")\n",
        "        print(f\"[{self.label}] {emoji} {message}\")\n",
        "    \n",
        "    def print_stats(self):\n",
        "        print(f\"\\nüìä [{self.label}] Statistics:\")\n",
        "        print(f\"   Messages: {self.stats['messages']}\")\n",
        "        print(f\"   Tool calls: {self.stats['tool_calls']}\")\n",
        "        print(f\"   Tool errors: {self.stats['tool_errors']}\")\n",
        "\n",
        "print(\"‚úì DetailedObserver class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Observer with Harness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_with_observer():\n",
        "    \"\"\"Run harness with detailed progress tracking.\"\"\"\n",
        "    \n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(\"simple_task.json\"),\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=MCPConfig(\n",
        "                name=\"jira\",\n",
        "                url=\"http://localhost:8015/mcp\",\n",
        "                transport=\"streamable_http\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Create observer (one per run)\n",
        "    observer = DetailedObserver(\"benchmark\")\n",
        "    harness.add_observer_factory(lambda: observer)\n",
        "    \n",
        "    print(\"üöÄ Starting benchmark with observer...\\n\")\n",
        "    \n",
        "    # Run\n",
        "    results = await harness.run(\n",
        "        models=[\"gpt-4o\"],\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    \n",
        "    # Print statistics\n",
        "    observer.print_stats()\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nüìä Results:\")\n",
        "    for result in results:\n",
        "        status = \"‚úì PASS\" if result.success else \"‚úó FAIL\"\n",
        "        print(f\"  {result.scenario_id}: {status}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run with tracking\n",
        "# results = await run_with_observer()\n",
        "print(\"‚úì Observer pattern ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 6: Multiple Models Comparison\n",
        "\n",
        "**Best for:** Benchmarking across different LLM providers\n",
        "\n",
        "Compare how different models perform on the same scenarios:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "async def compare_models():\n",
        "    \"\"\"Compare multiple models on the same scenarios.\"\"\"\n",
        "    \n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(\"simple_task.json\"),\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=MCPConfig(\n",
        "                name=\"jira\",\n",
        "                url=\"http://localhost:8015/mcp\",\n",
        "                transport=\"streamable_http\"\n",
        "            ),\n",
        "            runs_per_scenario=3,  # Run each 3 times for reliability\n",
        "            max_concurrent_runs=10,\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Compare multiple models\n",
        "    models = [\n",
        "        \"gpt-4o\",\n",
        "        \"gpt-4o-mini\",\n",
        "        \"claude-sonnet-4-5\",\n",
        "        \"gemini-2.0-flash-exp\",\n",
        "    ]\n",
        "    \n",
        "    print(f\"üèÅ Comparing {len(models)} models...\\n\")\n",
        "    \n",
        "    results = await harness.run(\n",
        "        models=models,\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    \n",
        "    # Aggregate by model\n",
        "    model_stats = defaultdict(lambda: {\"total\": 0, \"passed\": 0, \"steps\": []})\n",
        "    \n",
        "    for result in results:\n",
        "        model_stats[result.model][\"total\"] += 1\n",
        "        if result.success:\n",
        "            model_stats[result.model][\"passed\"] += 1\n",
        "        model_stats[result.model][\"steps\"].append(result.result.metadata.get(\"steps\", 0))\n",
        "    \n",
        "    # Print comparison\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä MODEL COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"{'Model':<30} {'Pass Rate':<15} {'Avg Steps':<10}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    for model, stats in sorted(model_stats.items()):\n",
        "        pass_rate = stats[\"passed\"] / stats[\"total\"] * 100\n",
        "        avg_steps = sum(stats[\"steps\"]) / len(stats[\"steps\"]) if stats[\"steps\"] else 0\n",
        "        print(f\"{model:<30} {pass_rate:>6.1f}%          {avg_steps:>6.1f}\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return results, model_stats\n",
        "\n",
        "# Run comparison\n",
        "# results, stats = await compare_models()\n",
        "print(\"‚úì Model comparison ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 7: Export Results\n",
        "\n",
        "**Best for:** Analysis, reporting, debugging\n",
        "\n",
        "Export results to JSON for further analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_results(results, filename=\"results.json\"):\n",
        "    \"\"\"Export results to JSON file with full details.\"\"\"\n",
        "    data = [r.to_dict() for r in results]\n",
        "    \n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "    \n",
        "    print(f\"‚úì Exported {len(results)} results to {filename}\")\n",
        "    \n",
        "    # Show what's included\n",
        "    if results:\n",
        "        sample = results[0].to_dict()\n",
        "        print(f\"\\nüì¶ Each result includes:\")\n",
        "        print(f\"   - model, scenario_id, scenario_name\")\n",
        "        print(f\"   - success, error\")\n",
        "        print(f\"   - steps, database_id\")\n",
        "        print(f\"   - conversation ({len(sample.get('conversation', []))} entries)\")\n",
        "        print(f\"   - verifier_results\")\n",
        "        print(f\"   - reasoning_traces (if available)\")\n",
        "        \n",
        "        # Show conversation structure\n",
        "        if sample.get('conversation'):\n",
        "            print(f\"\\nüí¨ Conversation format:\")\n",
        "            entry = sample['conversation'][0]\n",
        "            print(f\"   Type: {entry.get('type')}\")\n",
        "            print(f\"   Keys: {list(entry.keys())}\")\n",
        "\n",
        "# Example usage:\n",
        "# async def save_results():\n",
        "#     results = await run_simple_harness()\n",
        "#     export_results(results, \"benchmark_results.json\")\n",
        "#     return results\n",
        "\n",
        "print(\"‚úì Export function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**üéâ You've learned all the core patterns!**\n",
        "\n",
        "### The SDK Philosophy: **Harness-First**\n",
        "\n",
        "The TestHarness is the main component. It orchestrates:\n",
        "- ‚úÖ Agent creation\n",
        "- ‚úÖ MCP connections\n",
        "- ‚úÖ Task execution\n",
        "- ‚úÖ Result verification\n",
        "- ‚úÖ Metrics collection\n",
        "\n",
        "### Usage Patterns Recap\n",
        "\n",
        "1. **Simplest Harness** ‚≠ê - Start here!\n",
        "   - Create JSON file with scenarios\n",
        "   - Run with `harness.run(models, agent_factory)`\n",
        "   - Get results with verification\n",
        "\n",
        "2. **Direct Agent Usage** - For one-off tasks\n",
        "   - `agent.run(task)` without harness\n",
        "   - Manual verification with `DatabaseVerifier`\n",
        "\n",
        "3. **Custom Agents** - Integrate any LLM\n",
        "   - Subclass `Agent`\n",
        "   - Implement `_build_llm()`, `get_response()`, `get_response_parser()`\n",
        "   - Use with harness via custom factory\n",
        "\n",
        "4. **Custom Verifiers** - Complex validation\n",
        "   - Subclass `Verifier`\n",
        "   - Implement `verify()` method\n",
        "   - Use programmatically (harness integration requires extending loader)\n",
        "\n",
        "5. **Observers** - Real-time tracking\n",
        "   - Subclass `RunObserver`\n",
        "   - Track messages, tool calls, status updates\n",
        "   - Add to harness with `add_observer_factory()`\n",
        "\n",
        "6. **Model Comparison** - Systematic benchmarking\n",
        "   - Pass multiple models to `harness.run()`\n",
        "   - Aggregate and compare results\n",
        "   - Statistical analysis (multiple runs per scenario)\n",
        "\n",
        "7. **Export Results** - Analysis and reporting\n",
        "   - `result.to_dict()` for JSON serialization\n",
        "   - Includes conversation, verifiers, reasoning traces\n",
        "   - Ready for pandas, matplotlib, etc.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "‚úÖ **Start with the harness** - It's the recommended approach  \n",
        "‚úÖ **The harness orchestrates everything** - Agents, MCP, verification  \n",
        "‚úÖ **Custom agents integrate seamlessly** - Just implement 3 methods  \n",
        "‚úÖ **Observers provide visibility** - Real-time progress tracking  \n",
        "‚úÖ **Results are export-ready** - JSON with full details  \n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Read the full README for API reference\n",
        "2. Create your own harness files (see `9_tasks/task1.json` for examples)\n",
        "3. Build custom agents for your LLM providers\n",
        "4. Create custom verifiers for your use cases\n",
        "5. Run large-scale benchmarks!\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- **README.md** - Complete API reference\n",
        "- **simple_harness_example.py** - Basic example script\n",
        "- **9_tasks/** - Real benchmark scenarios\n",
        "- **QwenAgent** - Example custom agent implementation\n",
        "\n",
        "Happy benchmarking! üéØ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
