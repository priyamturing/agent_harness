{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCP Benchmark SDK - Usage Showcase\n",
        "\n",
        "This notebook demonstrates all the major usage patterns of the MCP Benchmark SDK:\n",
        "\n",
        "1. **Using Agents Standalone** - Run agents without the harness\n",
        "2. **Custom Agents** - Create your own agent (Qwen example)\n",
        "3. **Custom Verifiers** - Build custom verification logic\n",
        "4. **Test Harness** - Run benchmarks across multiple models\n",
        "5. **Observers** - Monitor execution in real-time\n",
        "6. **Advanced Patterns** - Tracing, custom prompts, and more\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have:\n",
        "- An MCP server running (e.g., Jira MCP on `http://localhost:8015/mcp`)\n",
        "- API keys set as environment variables\n",
        "- The SDK installed: `pip install -e .`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Jupyter\n",
            "SDK imported successfully âœ“\n"
          ]
        }
      ],
      "source": [
        "# Setup and Imports\n",
        "\n",
        "import asyncio\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Core SDK imports\n",
        "from mcp_benchmark_sdk import (\n",
        "    # Agents\n",
        "    ClaudeAgent,\n",
        "    GPTAgent,\n",
        "    GeminiAgent,\n",
        "    GrokAgent,\n",
        "    # Task/Result\n",
        "    Task,\n",
        "    Result,\n",
        "    # MCP\n",
        "    MCPConfig,\n",
        "    # Verifiers\n",
        "    Verifier,\n",
        "    DatabaseVerifier,\n",
        "    VerifierResult,\n",
        "    # Runtime\n",
        "    RunContext,\n",
        "    RunObserver,\n",
        "    # Harness\n",
        "    TestHarness,\n",
        "    TestHarnessConfig,\n",
        "    create_agent,\n",
        ")\n",
        "\n",
        "# Check if we're in Jupyter\n",
        "def is_jupyter():\n",
        "    try:\n",
        "        from IPython.core.getipython import get_ipython\n",
        "        return get_ipython() is not None\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "print(f\"Running in {'Jupyter' if is_jupyter() else 'Python script'}\")\n",
        "print(f\"SDK imported successfully âœ“\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Using Agents Standalone (Without Harness)\n",
        "\n",
        "Agents can be used directly without the test harness for single-task execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gw/zj2q1xwn18nd9922p394wrb00000gn/T/ipykernel_52548/1002770565.py:14: UserWarning: Claude thinking mode requires temperature=1.0. Your temperature=0.1 will be overridden. Set temperature=1.0 or enable_thinking=False to suppress this warning.\n",
            "  agent = ClaudeAgent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent...\n",
            "\n",
            "============================================================\n",
            "Success: True\n",
            "Steps taken: 4\n",
            "Database ID: 9bd53603-b91d-4ad8-9d96-2eca67abac39\n",
            "\n",
            "Last 3 messages:\n",
            "  ai: [{'signature': 'EpQLCkYICRgCKkDXze33UI5sqjyWOapdWKFrPf4fY4argQmBAyq1OYKgfW8EvmJy2Hd8BVscTVFQTDfW52en...\n",
            "  tool: {\"isLast\": true, \"maxResults\": 100, \"nextPage\": null, \"self\": \"http://localhost:8015/rest/api/3/proj...\n",
            "  ai: Great! Here are all **6 projects** currently available in the system:\n",
            "\n",
            "## ðŸ“Š Projects Summary\n",
            "\n",
            "| # | ...\n"
          ]
        }
      ],
      "source": [
        "### Example 1.1: Basic Agent Usage\n",
        "\n",
        "async def example_basic_agent():\n",
        "    \"\"\"Run a simple agent task.\"\"\"\n",
        "    \n",
        "    # Configure MCP server\n",
        "    mcp_config = MCPConfig(\n",
        "        name=\"jira\",\n",
        "        url=\"http://localhost:8015/mcp\",\n",
        "        transport=\"streamable_http\"\n",
        "    )\n",
        "    \n",
        "    # Create agent\n",
        "    agent = ClaudeAgent(\n",
        "        model=\"claude-sonnet-4-5\",\n",
        "        temperature=0.1,\n",
        "        tool_call_limit=1000\n",
        "    )\n",
        "    \n",
        "    # Define task\n",
        "    task = Task(\n",
        "        prompt=\"List all projects available in the system\",\n",
        "        mcp=mcp_config,\n",
        "        max_steps=100\n",
        "    )\n",
        "    \n",
        "    # Run agent\n",
        "    print(\"Running agent...\")\n",
        "    result = await agent.run(task)\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Steps taken: {len(result.messages)}\")\n",
        "    print(f\"Database ID: {result.database_id}\")\n",
        "    \n",
        "    if result.error:\n",
        "        print(f\"Error: {result.error}\")\n",
        "    \n",
        "    # Show last few messages\n",
        "    print(f\"\\nLast 3 messages:\")\n",
        "    for msg in result.messages[-3:]:\n",
        "        print(f\"  {msg.type}: {str(msg.content)[:100]}...\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Run the example\n",
        "# Uncomment to execute:\n",
        "result = await example_basic_agent()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gw/zj2q1xwn18nd9922p394wrb00000gn/T/ipykernel_52548/1486894685.py:20: UserWarning: Claude thinking mode requires temperature=1.0. Your temperature=0.1 will be overridden. Set temperature=1.0 or enable_thinking=False to suppress this warning.\n",
            "  \"Claude Sonnet\": ClaudeAgent(model=\"claude-sonnet-4-5\", temperature=0.1),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trying GPT-4o...\n",
            "Steps: 4\n",
            "\n",
            "Trying Claude Sonnet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n",
            "Key 'additionalProperties' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steps: 6\n",
            "\n",
            "Trying Gemini 2.5 Pro...\n",
            "Steps: 2\n",
            "GPT-4o: {'steps': 4, 'error': None}\n",
            "Claude Sonnet: {'steps': 6, 'error': None}\n",
            "Gemini 2.5 Pro: {'steps': 2, 'error': None}\n"
          ]
        }
      ],
      "source": [
        "### Example 1.2: Using Different Agent Providers\n",
        "\n",
        "async def example_multiple_providers():\n",
        "    \"\"\"Compare different LLM providers.\"\"\"\n",
        "    \n",
        "    mcp_config = MCPConfig(\n",
        "        name=\"jira\",\n",
        "        url=\"http://localhost:8015/mcp\",\n",
        "        transport=\"streamable_http\"\n",
        "    )\n",
        "    \n",
        "    task = Task(\n",
        "        prompt=\"Create a test issue in the DEMO project\",\n",
        "        mcp=mcp_config\n",
        "    )\n",
        "    \n",
        "    # Try different providers\n",
        "    agents = {\n",
        "        \"GPT-4o\": GPTAgent(model=\"gpt-4o\", temperature=0.1),\n",
        "        \"Claude Sonnet\": ClaudeAgent(model=\"claude-sonnet-4-5\", temperature=0.1), # will give a warning as claude-sonnet-4-5 temperature needs to be 1\n",
        "        \"Gemini 2.5 Pro\": GeminiAgent(model=\"gemini-2.5-pro\", temperature=0.1),\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    for name, agent in agents.items():\n",
        "        print(f\"\\nTrying {name}...\")\n",
        "        try:\n",
        "            result = await agent.run(task)\n",
        "            results[name] = {\n",
        "                \"steps\": len(result.messages),\n",
        "                \"error\": result.error\n",
        "            }\n",
        "            print(f\"Steps: {len(result.messages)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  âœ— Error: {e}\")\n",
        "            results[name] = {\"success\": False, \"error\": str(e)}\n",
        "    \n",
        "    return results\n",
        "\n",
        "results = await example_multiple_providers()\n",
        "for provider, result in results.items():\n",
        "    print(f\"{provider}: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEMO] INFO: Initialized with 49 tools from MCP server\n",
            "[DEMO] INFO: Step 1/1000\n",
            "[DEMO] Agent: I'll retrieve the first 3 projects for you....\n",
            "[DEMO] Tool called: get_paginated_projects: {\n",
            "  \"maxResults\": 3,\n",
            "  \"startAt\": 0\n",
            "}\n",
            "[DEMO] Tool result: {\n",
            "  \"isLast\": false,\n",
            "  \"maxResults\": 3,\n",
            "  \"nextPage\": \"http://localhost:8015/rest/api/3/project/search?startAt=3&maxResults=3\",\n",
            "  \"self\": \"http://localhost:8015/rest/api/3/project/search?startAt=0&maxResults=3\",\n",
            "  \"startAt\": 0,\n",
            "  \"total\": 6,\n",
            "  \"values\": [\n",
            "    {\n",
            "      \"id\": \"4\",\n",
            "      \"key\": \"AS3\",\n",
            "      \"name\": \"apple-s3\",\n",
            "      \"simplified\": false,\n",
            "      \"style\": \"classic\",\n",
            "      \"self\": \"http://localhost:8015/rest/api/3/project/AS3\",\n",
            "      \"avatarUrls\": {\n",
            "        \"16x16\": \"http://localhost:8015/secure/projectavatar?size=xsmall&pid=4\",\n",
            "        \"24x24\": \"http://localhost:8015/secure/projectavatar?size=small&pid=4\",\n",
            "        \"32x32\": \"http://localhost:8015/secure/projectavatar?size=medium&pid=4\",\n",
            "        \"48x48\": \"http://localhost:8015/secure/projectavatar?size=large&pid=4\"\n",
            "      },\n",
            "      \"insight\": {\n",
            "        \"totalIssueCount\": 0,\n",
            "        \"lastIssueUpdateTime\": \"2025-11-06T21:06:04.131266Z\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"3\",\n",
            "      \"key\": \"INFRA\",\n",
            "      \"name\": \"Infrastructure\",\n",
            "      \"simplified\": false,\n",
            "      \"style\": \"classic\",\n",
            "      \"self\": \"http://localhost:8015/rest/api/3/project/INFRA\",\n",
            "      \"avatarUrls\": {\n",
            "        \"16x16\": \"http://localhost:8015/secure/projectavatar?size=xsmall&pid=3\",\n",
            "        \"24x24\": \"http://localhost:8015/secure/projectavatar?size=small&pid=3\",\n",
            "        \"32x32\": \"http://localhost:8015/secure/projectavatar?size=medium&pid=3\",\n",
            "        \"48x48\": \"http://localhost:8015/secure/projectavatar?size=large&pid=3\"\n",
            "      },\n",
            "      \"projectCategory\": {\n",
            "        \"id\": \"3\",\n",
            "        \"name\": \"Infrastructure\",\n",
            "        \"description\": \"System and infrastructure projects\",\n",
            "        \"self\": \"http://localhost:8015/rest/api/3/projectCategory/3\"\n",
            "      },\n",
            "      \"insight\": {\n",
            "        \"totalIssueCount\": 3,\n",
            "        \"lastIssueUpdateTime\": \"2025-11-06T21:06:04.131636Z\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"6\",\n",
            "      \"key\": \"KANX\",\n",
            "      \"name\": \"Moblie app 2\",\n",
            "      \"simplified\": false,\n",
            "      \"style\": \"classic\",\n",
            "      \"self\": \"http://localhost:8015/rest/api/3/project/KANX\",\n",
            "      \"avatarUrls\": {\n",
            "        \"16x16\": \"http://localhost:8015/secure/projectavatar?size=xsmall&pid=6\",\n",
            "        \"24x24\": \"http://localhost:8015/secure/projectavatar?size=small&pid=6\",\n",
            "        \"32x32\": \"http://localhost:8015/secure/projectavatar?size=medium&pid=6\",\n",
            "        \"48x48\": \"http://localhost:8015/secure/projectavatar?size=large&pid=6\"\n",
            "      },\n",
            "      \"insight\": {\n",
            "        \"totalIssueCount\": 2,\n",
            "        \"lastIssueUpdateTime\": \"2025-11-06T21:06:04.131955Z\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "[DEMO] INFO: Step 2/1000\n",
            "[DEMO] Agent: Here are the first 3 projects in the system:\n",
            "\n",
            "1. **apple-s3** (AS3)\n",
            "   - ID: 4\n",
            "   - Style: Classic\n",
            "   - Total Issues: 0\n",
            "   - Last Updated: November 6, 2025\n",
            "\n",
            "2. **Infrastructure** (INFRA)\n",
            "   - ID: 3\n",
            "   - Style: Classic\n",
            "   - Category: Infrastructure - System and infrastructure projects\n",
            "   - Total Issues: 3\n",
            "   - Last Updated: November 6, 2025\n",
            "\n",
            "3. **Moblie app 2** (KANX)\n",
            "   - ID: 6\n",
            "   - Style: Classic\n",
            "   - Total Issues: 2\n",
            "   - Last Updated: November 6, 2025\n",
            "\n",
            "There are **6 total projects** in the system. Would you like to see the remaining projects or get more details about any of these?...\n",
            "[DEMO] INFO: Agent completed\n",
            "\n",
            "Execution complete!\n",
            "  Success: True\n",
            "  Tool calls: 1\n",
            "  Errors: 0\n"
          ]
        }
      ],
      "source": [
        "### Example 1.3: Using Observers for Real-time Monitoring\n",
        "import json\n",
        "class SimpleConsoleObserver(RunObserver):\n",
        "    \"\"\"Simple console observer to track execution.\"\"\"\n",
        "    \n",
        "    def __init__(self, label=\"Agent\"):\n",
        "        self.label = label\n",
        "        self.tool_count = 0\n",
        "        self.error_count = 0\n",
        "    \n",
        "    async def on_message(self, role: str, content: str, metadata=None):\n",
        "        if role == \"assistant\":\n",
        "            print(f\"[{self.label}] Agent: {content}...\")\n",
        "    \n",
        "    async def on_tool_call(self, tool_name, arguments, result, is_error=False):\n",
        "        self.tool_count += 1\n",
        "        if is_error:\n",
        "            self.error_count += 1\n",
        "            print(f\"[{self.label}] Tool failed: {tool_name}\")\n",
        "            print(f\"[{self.label}] Tool error: {result}\")\n",
        "        else:\n",
        "            print(f\"[{self.label}] Tool called: {tool_name}: {json.dumps(arguments, indent=2)}\")\n",
        "            print(f\"[{self.label}] Tool result: {json.dumps(result, indent=2)}\")\n",
        "    \n",
        "    async def on_status(self, message: str, level: str = \"info\"):\n",
        "        print(f\"[{self.label}] {level.upper()}: {message}\")\n",
        "\n",
        "\n",
        "async def example_with_observer():\n",
        "    \"\"\"Use an observer to monitor agent execution.\"\"\"\n",
        "    \n",
        "    # Create run context with observer\n",
        "    async with RunContext() as ctx:\n",
        "        observer = SimpleConsoleObserver(label=\"DEMO\")\n",
        "        ctx.add_observer(observer)\n",
        "        \n",
        "        # Create agent and task\n",
        "        agent = ClaudeAgent(model=\"claude-sonnet-4-5\")\n",
        "        task = Task(\n",
        "            prompt=\"List the first 3 projects in the system\",\n",
        "            mcp=MCPConfig(\n",
        "                name=\"jira\",\n",
        "                url=\"http://localhost:8015/mcp\",\n",
        "                transport=\"streamable_http\"\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # Run with observer\n",
        "        result = await agent.run(task, run_context=ctx)\n",
        "        \n",
        "        print(f\"\\nExecution complete!\")\n",
        "        print(f\"  Success: {result.success}\")\n",
        "        print(f\"  Tool calls: {observer.tool_count}\")\n",
        "        print(f\"  Errors: {observer.error_count}\")\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Uncomment to execute:\n",
        "result = await example_with_observer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Creating Custom Agents\n",
        "\n",
        "You can create custom agents by subclassing Agent. Here is the Qwen agent as an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QwenAgent class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "### Example 2.1: QwenAgent Implementation\n",
        "\n",
        "from typing import Optional\n",
        "from langchain_core.messages import BaseMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from mcp_benchmark_sdk import Agent, AgentResponse\n",
        "from mcp_benchmark_sdk.parsers import OpenAIResponseParser, ResponseParser\n",
        "from mcp_benchmark_sdk.utils import retry_with_backoff\n",
        "\n",
        "\n",
        "class QwenAgent(Agent):\n",
        "    \"\"\"Custom agent for Alibaba Cloud Qwen models.\n",
        "    \n",
        "    Uses OpenAI-compatible API from DashScope.\n",
        "    Requires DASHSCOPE_API_KEY environment variable.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"qwen-plus\",\n",
        "        temperature: float = 0.1,\n",
        "        max_output_tokens: Optional[int] = None,\n",
        "        tool_call_limit: int = 1000,\n",
        "        system_prompt: Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(system_prompt=system_prompt, tool_call_limit=tool_call_limit)\n",
        "        \n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_output_tokens = max_output_tokens\n",
        "        \n",
        "        # Get API key\n",
        "        self.api_key = os.environ.get(\"DASHSCOPE_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise EnvironmentError(\n",
        "                \"DASHSCOPE_API_KEY not set. \"\n",
        "                \"Get your key from: https://modelstudio.console.alibabacloud.com/\"\n",
        "            )\n",
        "        \n",
        "        self.base_url = \"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
        "    \n",
        "    def _build_llm(self):\n",
        "        \"\"\"Build the LLM instance.\"\"\"\n",
        "        config = {\n",
        "            \"model\": self.model,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"timeout\": None,\n",
        "            \"max_retries\": 3,\n",
        "            \"base_url\": self.base_url,\n",
        "            \"api_key\": self.api_key,\n",
        "        }\n",
        "        \n",
        "        if self.max_output_tokens:\n",
        "            config[\"max_completion_tokens\"] = self.max_output_tokens\n",
        "        \n",
        "        llm = ChatOpenAI(**config)\n",
        "        return llm.bind_tools(self._tools) if self._tools else llm\n",
        "    \n",
        "    async def get_response(self, messages: list[BaseMessage]) -> tuple[AgentResponse, AIMessage]:\n",
        "        \"\"\"Get model response with retry logic.\"\"\"\n",
        "        if not self._llm:\n",
        "            raise RuntimeError(\"LLM not initialized\")\n",
        "        \n",
        "        async def _invoke():\n",
        "            return await self._llm.ainvoke(messages)\n",
        "        \n",
        "        ai_message = await retry_with_backoff(\n",
        "            _invoke, \n",
        "            max_retries=2, \n",
        "            timeout_seconds=600.0\n",
        "        )\n",
        "        \n",
        "        # Parse response\n",
        "        parser = self.get_response_parser()\n",
        "        parsed = parser.parse(ai_message)\n",
        "        \n",
        "        agent_response = AgentResponse(\n",
        "            content=parsed.content,\n",
        "            tool_calls=parsed.tool_calls,\n",
        "            reasoning=\"\\n\".join(parsed.reasoning) if parsed.reasoning else None,\n",
        "            done=not bool(parsed.tool_calls),\n",
        "        )\n",
        "        \n",
        "        return agent_response, ai_message\n",
        "    \n",
        "    def get_response_parser(self) -> ResponseParser:\n",
        "        \"\"\"Get response parser.\"\"\"\n",
        "        return OpenAIResponseParser()\n",
        "\n",
        "\n",
        "print(\"QwenAgent class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qwen Agent Result:\n",
            "  Success: True\n",
            "  Steps: 4\n"
          ]
        }
      ],
      "source": [
        "### Example 2.2: Using the Custom QwenAgent\n",
        "\n",
        "async def example_qwen_agent():\n",
        "    \"\"\"Use the custom Qwen agent.\"\"\"\n",
        "    \n",
        "    # Note: You need DASHSCOPE_API_KEY set in environment\n",
        "    # os.environ[\"DASHSCOPE_API_KEY\"] = \"your-key-here\"\n",
        "    \n",
        "    try:\n",
        "        agent = QwenAgent(\n",
        "            model=\"qwen-plus\",\n",
        "            temperature=0.1\n",
        "        )\n",
        "        \n",
        "        task = Task(\n",
        "            prompt=\"List all available projects\",\n",
        "            mcp=MCPConfig(\n",
        "                name=\"jira\",\n",
        "                url=\"http://localhost:8015/mcp\",\n",
        "                transport=\"streamable_http\"\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        result = await agent.run(task)\n",
        "        \n",
        "        print(f\"Qwen Agent Result:\")\n",
        "        print(f\"  Success: {result.success}\")\n",
        "        print(f\"  Steps: {len(result.messages)}\")\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    except EnvironmentError as e:\n",
        "        print(f\"Cannot run Qwen agent: {e}\")\n",
        "        print(\"Set DASHSCOPE_API_KEY to use Qwen models\")\n",
        "\n",
        "# Uncomment to execute (requires DASHSCOPE_API_KEY):\n",
        "result = await example_qwen_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating Custom Verifiers\n",
        "\n",
        "Verifiers check that your agent's execution produced the expected results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom verifier defined!\n",
            "Verifier: CustomAPIVerifier\n",
            "  Success: False\n",
            "  Expected: completed\n",
            "  Actual: None\n",
            "  Error: [Errno 8] nodename nor servname provided, or not known\n"
          ]
        }
      ],
      "source": [
        "### Example 3.1: Custom API Verifier\n",
        "\n",
        "import httpx\n",
        "\n",
        "class CustomAPIVerifier(Verifier):\n",
        "    \"\"\"Verify state via custom API calls.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        api_url: str,\n",
        "        expected_status: str,\n",
        "        name: str = \"CustomAPIVerifier\",\n",
        "    ):\n",
        "        super().__init__(name)\n",
        "        self.api_url = api_url\n",
        "        self.expected_status = expected_status\n",
        "    \n",
        "    async def verify(self) -> VerifierResult:\n",
        "        \"\"\"Execute verification.\"\"\"\n",
        "        try:\n",
        "            async with httpx.AsyncClient() as client:\n",
        "                response = await client.get(self.api_url)\n",
        "                response.raise_for_status()\n",
        "                \n",
        "                data = response.json()\n",
        "                actual_status = data.get(\"status\")\n",
        "                \n",
        "                success = actual_status == self.expected_status\n",
        "                \n",
        "                return VerifierResult(\n",
        "                    name=self.name,\n",
        "                    success=success,\n",
        "                    expected_value=self.expected_status,\n",
        "                    actual_value=actual_status,\n",
        "                    comparison_type=\"equals\",\n",
        "                    error=None if success else f\"Status mismatch\",\n",
        "                    metadata={\"response\": data},\n",
        "                )\n",
        "        \n",
        "        except Exception as e:\n",
        "            return VerifierResult(\n",
        "                name=self.name,\n",
        "                success=False,\n",
        "                expected_value=self.expected_status,\n",
        "                actual_value=None,\n",
        "                comparison_type=\"equals\",\n",
        "                error=str(e),\n",
        "            )\n",
        "\n",
        "\n",
        "async def example_custom_verifier():\n",
        "    \"\"\"Demonstrate custom verifier.\"\"\"\n",
        "    \n",
        "    verifier = CustomAPIVerifier(\n",
        "        api_url=\"https://api.example.com/status\",\n",
        "        expected_status=\"completed\",\n",
        "    )\n",
        "    \n",
        "    result = await verifier.verify()\n",
        "    \n",
        "    print(f\"Verifier: {result.name}\")\n",
        "    print(f\"  Success: {result.success}\")\n",
        "    print(f\"  Expected: {result.expected_value}\")\n",
        "    print(f\"  Actual: {result.actual_value}\")\n",
        "    if result.error:\n",
        "        print(f\"  Error: {result.error}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"Custom verifier defined!\")\n",
        "# Uncomment to execute:\n",
        "result = await example_custom_verifier()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database Verifier: Bug Count Check\n",
            "  Query: SELECT COUNT(*) FROM issues\n",
            "  Expected: 0\n",
            "  Actual: 3\n",
            "  Comparison: equals\n",
            "  Success: False\n",
            "  Error: Comparison failed\n"
          ]
        }
      ],
      "source": [
        "### Example 3.2: Using Built-in Database Verifier\n",
        "\n",
        "async def example_database_verifier():\n",
        "    \"\"\"Use the built-in database verifier.\"\"\"\n",
        "    \n",
        "    # Create a database verifier\n",
        "    verifier = DatabaseVerifier(\n",
        "        query=\"SELECT COUNT(*) FROM issues\",\n",
        "        expected_value=0,\n",
        "        mcp_url=\"http://localhost:8015/mcp\",\n",
        "        database_id=\"test-db-123\",\n",
        "        comparison=\"equals\",\n",
        "        name=\"Bug Count Check\"\n",
        "    )\n",
        "    \n",
        "    result = await verifier.verify()\n",
        "    \n",
        "    print(f\"Database Verifier: {result.name}\")\n",
        "    if result.metadata:\n",
        "        print(f\"  Query: {result.metadata.get('query')}\")\n",
        "    print(f\"  Expected: {result.expected_value}\")\n",
        "    print(f\"  Actual: {result.actual_value}\")\n",
        "    print(f\"  Comparison: {result.comparison_type}\")\n",
        "    print(f\"  Success: {result.success}\")\n",
        "    \n",
        "    if result.error:\n",
        "        print(f\"  Error: {result.error}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Uncomment to execute:\n",
        "result = await example_database_verifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Using the Test Harness for Benchmarks\n",
        "\n",
        "The TestHarness orchestrates running multiple scenarios across multiple models with built-in parallelization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 scenario(s)\n",
            "Running benchmarks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ipriyam26/Programming/agent_harness/packages/mcp_benchmark_sdk/src/mcp_benchmark_sdk/harness/agent_factory.py:97: UserWarning: Claude thinking mode requires temperature=1.0. Your temperature=0.1 will be overridden. Set temperature=1.0 or enable_thinking=False to suppress this warning.\n",
            "  return ClaudeAgent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "gpt-4o - pm_review_task_1: âœ— FAIL\n",
            "  Error: Verifiers failed: DatabaseVerifier, DatabaseVerifier, DatabaseVerifier\n",
            "  Conversation: 12 messages\n",
            "  Verifiers:\n",
            "    âœ— DatabaseVerifier\n",
            "    âœ— DatabaseVerifier\n",
            "    âœ— DatabaseVerifier\n",
            "    âœ“ DatabaseVerifier\n",
            "    âœ“ DatabaseVerifier\n",
            "    âœ“ DatabaseVerifier\n",
            "claude-sonnet-4-5 - pm_review_task_1: âœ— FAIL\n",
            "  Error: Verifiers failed: DatabaseVerifier\n",
            "  Conversation: 40 messages\n",
            "  Verifiers:\n",
            "    âœ“ DatabaseVerifier\n",
            "    âœ“ DatabaseVerifier\n",
            "    âœ“ DatabaseVerifier\n",
            "    âœ— DatabaseVerifier\n",
            "    âœ“ DatabaseVerifier\n",
            "    âœ“ DatabaseVerifier\n",
            "\n",
            "Total: 2 | Passed: 0 | Failed: 2\n"
          ]
        }
      ],
      "source": [
        "### Example 4.1: Running Benchmarks with Test Harness\n",
        "\n",
        "async def example_test_harness():\n",
        "    \"\"\"Run benchmarks using the test harness.\"\"\"\n",
        "    \n",
        "    # Configure MCP server\n",
        "    mcp_config = MCPConfig(\n",
        "        name=\"jira\",\n",
        "        url=\"http://localhost:8015/mcp\",\n",
        "        transport=\"streamable_http\"\n",
        "    )\n",
        "    \n",
        "    # Create test harness\n",
        "    # Note: You'll need a harness JSON file (see next cell for format)\n",
        "    harness_path = Path(\"../../../9_tasks/task1.json\")\n",
        "    \n",
        "    if not harness_path.exists():\n",
        "        print(f\"Harness file not found: {harness_path}\")\n",
        "        print(\"Create a harness JSON file or adjust the path\")\n",
        "        return\n",
        "    \n",
        "    harness = TestHarness(\n",
        "        harness_path=harness_path,\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=mcp_config,\n",
        "            max_steps=1000,\n",
        "            tool_call_limit=1000,\n",
        "            temperature=0.1,\n",
        "            runs_per_scenario=1,  # Run each scenario once\n",
        "            max_concurrent_runs=3,  # Run 3 in parallel\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    print(f\"Loaded {len(harness.scenarios)} scenario(s)\")\n",
        "    print(\"Running benchmarks...\")\n",
        "    \n",
        "    # Run benchmarks across models\n",
        "    results = await harness.run(\n",
        "        models=[\"gpt-4o\", \"claude-sonnet-4-5\"],\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    \n",
        "    # Analyze results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for result in results:\n",
        "        status = \"âœ“ PASS\" if result.success else \"âœ— FAIL\"\n",
        "        print(f\"{result.model} - {result.scenario_id}: {status}\")\n",
        "        \n",
        "        if result.error:\n",
        "            print(f\"  Error: {result.error}\")\n",
        "        \n",
        "        conversation = result.get_conversation_history()\n",
        "        print(f\"  Conversation: {len(conversation)} messages\")\n",
        "        \n",
        "        if result.verifier_results:\n",
        "            print(f\"  Verifiers:\")\n",
        "            for v in result.verifier_results:\n",
        "                v_status = \"âœ“\" if v.success else \"âœ—\"\n",
        "                print(f\"    {v_status} {v.name}\")\n",
        "    \n",
        "    successful = sum(1 for r in results if r.success)\n",
        "    print(f\"\\nTotal: {len(results)} | Passed: {successful} | Failed: {len(results) - successful}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Uncomment to execute:\n",
        "results = await example_test_harness()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Harness JSON File Format\n",
        "\n",
        "A harness JSON file defines scenarios, prompts, and verifiers:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"scenarios\": [\n",
        "    {\n",
        "      \"id\": \"create_bug_issue\",\n",
        "      \"description\": \"Create a bug issue in DEMO project\",\n",
        "      \"prompts\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Create a high-priority bug about login failures\"\n",
        "        }\n",
        "      ],\n",
        "      \"verifiers\": [\n",
        "        {\n",
        "          \"verifier_type\": \"database_state\",\n",
        "          \"name\": \"Issue Created\",\n",
        "          \"validation_config\": {\n",
        "            \"query\": \"SELECT COUNT(*) FROM issues WHERE type = 'Bug'\",\n",
        "            \"expected_value\": 1,\n",
        "            \"comparison_type\": \"equals\"\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Saving and Analyzing Results\n",
        "\n",
        "Results can be exported to JSON for analysis and storage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved result to: artifacts/result_28fad97d.json\n",
            "  Success: True\n",
            "  Conversation: 4 messages\n",
            "  File size: 1951 bytes\n",
            "\n",
            "First 2 conversation messages:\n",
            "  1. human: List all projects...\n",
            "  2. ai: [{'signature': 'EusDCkYICRgCKkD5McyQEN1ve5R1X0oy7fYGOLBVUpZRbcvLBDLRUATplA18hqw6lNRmedy4EOOKSwvKyTpm...\n"
          ]
        }
      ],
      "source": [
        "### Example 5.1: Saving Results to JSON\n",
        "\n",
        "import json\n",
        "\n",
        "async def example_save_results():\n",
        "    \"\"\"Save benchmark results to JSON files.\"\"\"\n",
        "    \n",
        "    # Run a simple benchmark first\n",
        "    agent = ClaudeAgent(model=\"claude-sonnet-4-5\")\n",
        "    task = Task(\n",
        "        prompt=\"List all projects\",\n",
        "        mcp=MCPConfig(\n",
        "            name=\"jira\",\n",
        "            url=\"http://localhost:8015/mcp\",\n",
        "            transport=\"streamable_http\"\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    result = await agent.run(task)\n",
        "    \n",
        "    # Create artifacts directory\n",
        "    artifacts_dir = Path(\"artifacts\")\n",
        "    artifacts_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Export to dict (includes full conversation history)\n",
        "    result_dict = {\n",
        "        \"success\": result.success,\n",
        "        \"error\": result.error,\n",
        "        \"database_id\": result.database_id,\n",
        "        \"conversation\": [\n",
        "            {\n",
        "                \"type\": msg.type,\n",
        "                \"content\": str(msg.content)[:500]  # Truncate for display\n",
        "            }\n",
        "            for msg in result.messages\n",
        "        ],\n",
        "        \"metadata\": result.metadata,\n",
        "    }\n",
        "    \n",
        "    # Save to file\n",
        "    db_id_short = result.database_id[:8] if result.database_id else \"unknown\"\n",
        "    filename = f\"result_{db_id_short}.json\"\n",
        "    filepath = artifacts_dir / filename\n",
        "    \n",
        "    with open(filepath, \"w\") as f:\n",
        "        json.dump(result_dict, f, indent=2)\n",
        "    \n",
        "    print(f\"Saved result to: {filepath}\")\n",
        "    print(f\"  Success: {result_dict['success']}\")\n",
        "    print(f\"  Conversation: {len(result_dict['conversation'])} messages\")\n",
        "    print(f\"  File size: {filepath.stat().st_size} bytes\")\n",
        "    \n",
        "    # Access conversation programmatically\n",
        "    print(f\"\\nFirst 2 conversation messages:\")\n",
        "    for i, msg in enumerate(result_dict['conversation'][:2]):\n",
        "        print(f\"  {i+1}. {msg['type']}: {msg['content'][:100]}...\")\n",
        "    \n",
        "    return result_dict\n",
        "\n",
        "# Uncomment to execute:\n",
        "result_dict = await example_save_results()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Advanced Patterns\n",
        "\n",
        "Additional SDK features for power users.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gw/zj2q1xwn18nd9922p394wrb00000gn/T/ipykernel_52548/2892652760.py:14: UserWarning: Claude thinking mode requires temperature=1.0. Your temperature=0.1 will be overridden. Set temperature=1.0 or enable_thinking=False to suppress this warning.\n",
            "  agent = ClaudeAgent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent with custom system prompt:\n",
            "  Success: True\n",
            "  Steps: 3\n"
          ]
        }
      ],
      "source": [
        "### Example 6.1: Custom System Prompts\n",
        "\n",
        "async def example_custom_system_prompt():\n",
        "    \"\"\"Use a custom system prompt.\"\"\"\n",
        "    \n",
        "    CUSTOM_PROMPT = \"\"\"You are an expert project manager assistant.\n",
        "You are precise, thorough, and always verify your work.\n",
        "When creating issues, always include:\n",
        "1. Clear title\n",
        "2. Detailed description\n",
        "3. Priority level\n",
        "4. Assignee if known\"\"\"\n",
        "    \n",
        "    agent = ClaudeAgent(\n",
        "        model=\"claude-sonnet-4-5\",\n",
        "        system_prompt=CUSTOM_PROMPT,\n",
        "        temperature=0.1,\n",
        "        tool_call_limit=500,\n",
        "    )\n",
        "    \n",
        "    task = Task(\n",
        "        prompt=\"Create a bug issue about the login page crashing\",\n",
        "        mcp=MCPConfig(\n",
        "            name=\"jira\",\n",
        "            url=\"http://localhost:8015/mcp\",\n",
        "            transport=\"streamable_http\"\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    result = await agent.run(task)\n",
        "    \n",
        "    print(\"Agent with custom system prompt:\")\n",
        "    print(f\"  Success: {result.success}\")\n",
        "    print(f\"  Steps: {len(result.messages)}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Uncomment to execute:\n",
        "result = await example_custom_system_prompt()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Example 6.2: LangSmith Tracing (Optional)\n",
        "\n",
        "# Set environment variables for LangSmith:\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = \"your-key\"\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_PROJECT\"] = \"mcp-benchmarks\"\n",
        "\n",
        "from mcp_benchmark_sdk import configure_langsmith, with_tracing, is_tracing_enabled\n",
        "\n",
        "async def example_langsmith_tracing():\n",
        "    \"\"\"Enable LangSmith tracing for observability.\"\"\"\n",
        "    \n",
        "    # Configure LangSmith (if env vars are set)\n",
        "    # Environment variables handle configuration:\n",
        "    # LANGSMITH_API_KEY, LANGSMITH_TRACING, LANGSMITH_PROJECT\n",
        "    \n",
        "    if not is_tracing_enabled():\n",
        "        print(\"LangSmith tracing not enabled (API key not set)\")\n",
        "        print(\"Set LANGSMITH_API_KEY and LANGSMITH_TRACING=true to enable\")\n",
        "        return\n",
        "    \n",
        "    # Wrap agent with tracing\n",
        "    agent = ClaudeAgent(model=\"claude-sonnet-4-5\")\n",
        "    traced_agent = with_tracing(agent)\n",
        "    \n",
        "    task = Task(\n",
        "        prompt=\"List projects\",\n",
        "        mcp=MCPConfig(\n",
        "            name=\"jira\",\n",
        "            url=\"http://localhost:8015/mcp\",\n",
        "            transport=\"streamable_http\"\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    result = await traced_agent.run(task)\n",
        "    \n",
        "    print(\"Traced agent execution:\")\n",
        "    print(f\"  Success: {result.success}\")\n",
        "    print(f\"  Tracing enabled: {is_tracing_enabled()}\")\n",
        "    print(\"  Check LangSmith dashboard for traces!\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Uncomment to execute (requires LANGSMITH_API_KEY):\n",
        "# result = await example_langsmith_tracing()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the key usage patterns of the MCP Benchmark SDK:\n",
        "\n",
        "### âœ… What We Covered\n",
        "\n",
        "1. **Standalone Agents**: Using built-in agents (GPT, Claude, Gemini, Grok) without the harness\n",
        "2. **Custom Agents**: Creating custom agents by subclassing `Agent` (Qwen example)\n",
        "3. **Custom Verifiers**: Building verification logic for agent execution results\n",
        "4. **Test Harness**: Running benchmarks across multiple models and scenarios\n",
        "5. **Observers**: Real-time monitoring of agent execution\n",
        "6. **Advanced Features**: Custom system prompts, LangSmith tracing, result persistence\n",
        "\n",
        "### ðŸ“– Next Steps\n",
        "\n",
        "- Explore the `examples/` directory for more complete examples\n",
        "- Read the SDK source code - it's well-documented!\n",
        "- Check out existing agents in `src/mcp_benchmark_sdk/agents/`\n",
        "- Review the README for detailed documentation\n",
        "\n",
        "### ðŸš€ Quick Reference\n",
        "\n",
        "```python\n",
        "# Simple agent usage\n",
        "agent = ClaudeAgent(model=\"claude-sonnet-4-5\")\n",
        "result = await agent.run(task)\n",
        "\n",
        "# With observer\n",
        "async with RunContext() as ctx:\n",
        "    ctx.add_observer(MyObserver())\n",
        "    result = await agent.run(task, run_context=ctx)\n",
        "\n",
        "# Test harness\n",
        "harness = TestHarness(harness_path=path, config=config)\n",
        "results = await harness.run(models=[\"gpt-4o\"], agent_factory=create_agent)\n",
        "```\n",
        "\n",
        "### ðŸ”— Resources\n",
        "\n",
        "- GitHub: [Your repository]\n",
        "- Documentation: See package README\n",
        "- MCP Protocol: https://modelcontextprotocol.io/\n",
        "\n",
        "**Happy benchmarking! ðŸŽ‰**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
