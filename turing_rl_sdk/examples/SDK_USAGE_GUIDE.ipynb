{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCP Benchmark SDK - Complete Usage Guide\n",
        "\n",
        "This notebook demonstrates **all usage patterns** for the MCP Benchmark SDK, from the simplest harness to custom agents and verifiers.\n",
        "\n",
        "**Table of Contents:**\n",
        "1. [Setup](#setup)\n",
        "2. [Pattern 1: Simplest Harness](#pattern-1-simplest-harness)\n",
        "3. [Pattern 2: Using Agents Without Harness](#pattern-2-using-agents-without-harness)\n",
        "4. [Pattern 3: Custom Agent (Qwen Example)](#pattern-3-custom-agent-qwen-example)\n",
        "5. [Pattern 4: Custom Verifier](#pattern-4-custom-verifier)\n",
        "6. [Pattern 5: Full Workflow with Harness + Custom Components](#pattern-5-full-workflow-with-harness--custom-components)\n",
        "7. [Pattern 6: Multiple Models Comparison](#pattern-6-multiple-models-comparison)\n",
        "8. [Pattern 7: Observers and Progress Tracking](#pattern-7-observers-and-progress-tracking)\n",
        "\n",
        "**Focus**: The SDK is **harness-first**. The harness orchestrates everything - you define scenarios and let it run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Reference\n",
        "\n",
        "**Minimal harness usage** - The simplest way to get started:\n",
        "\n",
        "```python\n",
        "from pathlib import Path\n",
        "from turing_rl_sdk import TestHarness, TestHarnessConfig, MCPConfig, create_agent\n",
        "\n",
        "harness = TestHarness(\n",
        "    harness_path=Path(\"simple_task.json\"),\n",
        "    config=TestHarnessConfig(\n",
        "        mcp=MCPConfig(name=\"jira\", url=\"http://localhost:8015/mcp\", transport=\"streamable_http\")\n",
        "    )\n",
        ")\n",
        "\n",
        "results = await harness.run(models=[\"gpt-4o\"], agent_factory=create_agent)\n",
        "```\n",
        "\n",
        "**That's it!** The harness handles the rest: agent creation, MCP connections, execution, verification, and metrics.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the SDK and set up API keys:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Install SDK (if not already installed)\n",
        "# !pip install turing-rl-sdk\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "# Set up API keys (replace with your actual keys or load from .env)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
        "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"your-google-key\"\n",
        "# os.environ[\"DASHSCOPE_API_KEY\"] = \"your-dashscope-key\"  # For Qwen\n",
        "\n",
        "print(\"‚úì Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 1: Simplest Harness\n",
        "\n",
        "**The recommended way to use the SDK.** Create a harness file and let the SDK handle everything.\n",
        "\n",
        "### Step 1: Create a Simple Harness File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Created simple_task.json\n"
          ]
        }
      ],
      "source": [
        "# Create a simple harness file\n",
        "simple_harness = {\n",
        "    \"scenarios\": [\n",
        "        {\n",
        "            \"scenario_id\": \"create_bug\",\n",
        "            \"name\": \"Create Bug Issue\",\n",
        "            \"description\": \"Test if agent can create a bug issue\",\n",
        "            \"prompts\": [\n",
        "                {\n",
        "                    \"prompt_text\": \"Create a bug issue in project MOB with summary 'Login button not working' and description 'Users cannot click the login button'\",\n",
        "                    \"expected_tools\": [\"create_issue\"],\n",
        "                    \"verifier\": {\n",
        "                        \"verifier_type\": \"database_state\",\n",
        "                        \"validation_config\": {\n",
        "                            \"query\": \"SELECT COUNT(*) FROM issue WHERE summary = 'Login button not working'\",\n",
        "                            \"expected_value\": 1,\n",
        "                            \"comparison_type\": \"equals\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ],\n",
        "            \"metadata\": {\"difficulty\": \"easy\"},\n",
        "            \"conversation_mode\": False\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "with open(\"simple_task.json\", \"w\") as f:\n",
        "    json.dump(simple_harness, f, indent=2)\n",
        "\n",
        "print(\"‚úì Created simple_task.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Run the Harness\n",
        "\n",
        "This is the **core pattern** - everything else is built on top of this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 scenario(s)\n",
            "ResultBundle(harness_name='simple_task', run_results=[RunResult(model='gpt-5', scenario_id='create_bug', scenario_name='Create Bug Issue', run_number=1, success=True, result=Result(success=True, messages=[HumanMessage(content=\"Create a bug issue in project MOB with summary 'Login button not working' and description 'Users cannot click the login button'\", additional_kwargs={}, response_metadata={}), AIMessage(content=[{'id': 'rs_0a27a7bebc050be500691b489d7bb481978a635dcd1d06091d', 'summary': [], 'type': 'reasoning', 'encrypted_content': 'gAAAAABpG0itxjMkWHp6B29m2n7BCST0f3TBvZVz8iH4mUE99BMT7rJJlTPfVoiljHk2lDg0MgseNlLnSmNqahDtzH8e1FdidpP2uq-NdL7HPlJICyDU_WfDlaVs7cx_XSzCW-pkLSq20gd65rtJweU82aWT__56CUMMvFFWPpNUj2mWPTM48gkTpp-QBTg9uaMcAWEbMKM-CY8h_5BmRwq21r1pmAa7cFCBkLfWql_EEdjRWeYZ45K43WuUqbJRluPU6cutr10mUDuG3xODR_qvpIF8VDcU7fFTLl5ESKYETZ6ADpnSx7jkSTN1yxhcTHV-WX53kvxl4ZLj2AB6mC84rXUOqy1dTWymGfCuK0GF-fttqenxYxFvVmHoTrFAKVd2BPtp-Egi5TLg9SPrPn53kQSjAosTUNHKrYcYAAa7qtlK7mAKw-jtogGd72WE9NazMz-qbuFRxACycmm1HMSm8pyp3d4icvEP0kj8jB2OZ6-2R6z1nDm91BktNV20kTxx3wBAVE_j2J9sT9O91MERI28PvxVV5F6YJ_9ujpK-6lye1kc3hd0GfI3lhAYyiyllMMYosOyUcuQVPwxQyFZ195Eenqlx_a4xMDwyprziN71Yv7eo9dQ5Fwkcl5nf4LFvoMCyhKY2M0ZnlLhEePoyGDHACtsPknfvlSsnpUcjoVRQZvZtFcrWsoJN8QB_4b__RNlcC6pHuWw7AZnkEQDIFCOPerx1Xnsv2vdrwU2OMxar9S2bNs0TgU3EZBkIAdAYqfqKSixmKEwvpo38J_LAmsUVSDHA5boGNRpgmZP8wQuz-CziyhUG9jb4TDuBfl0DsKfLbL_lqCIjxcmwNCnnDyx_xnbLSUrfEEH9MaGlAfMMNqZtOur62mISswrBKhfgFBNj56TtU5aTJb1C4P7j1a3J7mIhlpPUgJ8X0K3650BQrddx9zPFOfKVhwLhwas_4OxtmUMcdYcc0nhsegGNXwopymLAnM_tXVegNX7RtTaOQyTtPBj_eS5YXVNNPZagdWJOwG9Fz1iKrhj6xaZl3lkZDV8PNKYrhfyVtxJzDk66V8vQlFd-RD_Tv6Nk55D9ZX9gLo6yGQCCa3Wz1BVe3waPQZ7WtjtasR9_E9QVJbFT5Et1Q97qdez5j1DEmB3swWB0wJrIWd_yHyDDaFbHc5JAf-oveOKZPLhsPqrxwEImo9DI2pvEHc2NSwoDTk0LQbdYqFIyhZtQFr8DCRTspJjd1RbtY27DCWRrnVBzloFCIDnpgKTeMhX22NKJnBsxFso5PmGrm369Y9_7NWGq3ucfpmUDOXNDEqQDaTHkv8cO8K-gc03G42s1lpdcf356faP5HVAdrrS-TdGalLhGx-sGIu4R4cXnFeY-NgY7fwS5BwQB4RZEE72MBb-Y7j6FgXPVSbBTwuJBynrY408s8NfQbQ4I56_nDpjfo9Jhn7ExM5tgapGDp-c2pGzPFif6h5RyAxKW4UcXMNbm8I-7BC76w41EqFXGdPP7dZIC3ktFuFbKUOEHuaWSgzwbL9CzYdmWwE9hrGfsWB6xsfw03XX5YI2flvACQqDV5IfAiUCGcxYsR5xP0g6n63e5DS-1itXeGAQsCwrsiBlrdve7YhECZQ9LDRpeTtQ3WqwPaLUn3mBAX56DKYoPlLkQ0qamKpuOmeHZ4dI0BaRgwwYw4wiHZq5iRaX91uRiJ3hA9DzNyv_UsfL3UfR8Yyz1lRMa7U23uHK6jj3ioR0BZTg0ksDOn0Zz2g_gIS2WacNNe8vl0_mV4RR46fJxce6bzrvXl5wE7PonppBy3t8btsSSZo4mn9a7EsvyVFSwkE30rmjaVuzxg2-xIiZNTNDzXqu94mIDF9P21EWgDLOdSs_VyDtUs6mg64Gn12jWS9CUmcIkJo-IV1xSEx7pSayObEW3QdyMDzLOgyNnkyp-Een79QxfOPZa7C5ZkUlibQXDPTEfuSh1IdIeB8mqCTxIj7cARalNSm1PpjLYsuPU1MZ9hND6RtuL5Bf39L6z-2xRIWxEM_gz9Mv0MhOt_MTv1pg5qJLqMSRoBMiCNpb81-3GZMxebnX0QGbW1tFePD03XH5vc48Td5Yxv4e3Ktka-jMNO4PQOsOyhdBCM_EVXD_vl29qC0887k6SYhgysXiTU3pbmF3dwZ_KLHXLfSMsokE6hRJ0EfNkhaOuZLXrGlkdJp-e24l-8nDNwE_G0PukNPSf5eU-75CyxxsdHOSEBqG9oSYlYK0EebzWT6eJ-RNc-9ln-FabQhlctg_gURiISHo9h4vhVt6BmyAChSOUZgfItOpacOmyYXI3-WoN9pPCr8xG_ub27632-Zp1VtAJHdPkKr2J4vS9FetPuXeim9U39WCoH6Mr8dCWwMT0tOahL9_qbwKXXSs4AqkNhfZRt1c9PHhv5DmEA6JqmNdP4r3vfqqHHpyPJCKcTRUf-XFQ3QwNnB7ms7iumDqR2sNY1qNX1qkHFRm5jzKlZPdHmXGPufn78nCRNquaIoSzhAzWsfoYe42_aCAKj60PmP-A09qGtdu8x-xZ5q6LwCtZr2NHgkZeTmkTwGMnzjFY1YBNzj6J18uvmp1biX7nbP-k8SroiJi_2keCEdS2ufDmI9N5PyoUkEXh-psGmhFPh7FNX_ozxotMB40M_rejVJJYrDQeIFe8l6ORwEzHfPtHPdbtyeVnHKPw5kAv_s0L3UoNgAZonZYSShpG9YXb4VAP5nUay5qVLaFcBzzj_0U2KnYqyOt2s1TipUPMW0Tzqxj4U81PkwzdE7mAS7RQ0mbDXpihXp5lyXuix9VVmPbXmtLjlae8dxvQtaHYLwAawt2Znobm8BwTaWoSfp4Nyho5smEndXKAbcX7fulzCjKMwVRRVz61wcUimIPctXxpdsBe82ppB-3uIYtkSNFrxEESgsBqowuL_hsJMHIVzlrWgrpS6Bj3xEoQlqZ0PR57x4daB6oCoBPtuYN8iLuBpvQSrSI50qdTlHgOkLxkUXFIDz-VG9Coa_59vQkeR3yBV20S9uMVEBB40yoJSksnuLPW24cNz93hRyPNabV9lDCLrTDm-DXoNKaUQ-UmMdmuXBCenrrXCkaubseotugNJRVK3eESh42Tse7PP3kQYpZEG0GBKKxWqIwGcX2u7YCGhz__VFaNVRXJ91EHaD-gO3bM76851zRjCkShQDGRuXNgiNQ3FTWXm90McjSDND5Me8TavBCfZ_NxfuZU1hDZ9NRIb3k030s0NyMsJl9ylE0N9F27EeQOHZ-AiS_j3j7W00ntYIgcnIQqQ9wTGcsSsSagNjLhw_GWtmxKfuzPmEcpwMa4G5yacisiNOk7Lyb6xcpSfnnAURPdErXTfHZfFK-EmFLpLufFjAJGy6UAmOgR8N336aDKcHE7pONl2ftK1R4PS86cUMh_Q2ptd28UYpNY79UXIU4v1M_YQJbwzIR5sV4-2-MQe-cjAu3zaqbW4yLhywuF9yyoF0XYIl47fg2QDzT7uzRiI8pgYz0Qk5Wgb3giST1dGmUnRCAZnt5GrKHNLQlS8K5xIgp1Uv1lwaxfz3t8Wy4ndXKsLc9EwdU0Qmv2mrCZIHa4YJ49QMYDGhLI_faii7IXWwuDhIIdv5SVF32NvI9Z6G8ybsNErT0gA6Eg8Hnetagav98jgRBS1tebvHU3zfv0Ty3jAfpEGI4IKV6jBqUV-TRWVnOT0L7mFxWEOlQhckHhUV2sYaxdrBAxbzk_FoJrmlu_JinSPEk7kBNlYqzslvFFRGXYJ63pss7KkXhiVLeb4cQfz_8TEtUM3XSOTpz3D8hEuwLorqyep3A8YN5oOCMXi8ehtEoXBES2RS65l1gmKr7WRp7KwU-QgeBo5Tfmh2aegfOuq0wLOne_ARi0wuA61LY8KLllCtMQeE31ML36BK0UVUc4nUwSzOvbQATUWQ=='}, {'arguments': '{}', 'call_id': 'call_kybPhUtrbkUi5ttuca6enUTa', 'name': 'create_issue', 'type': 'function_call', 'id': 'fc_0a27a7bebc050be500691b48ac5d948197b89d7591a37e0edf', 'status': 'completed'}], additional_kwargs={}, response_metadata={'id': 'resp_0a27a7bebc050be500691b489c3da88197a8ebc10e4b9a4cc5', 'created_at': 1763395740.0, 'metadata': {}, 'model': 'gpt-5-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07'}, id='resp_0a27a7bebc050be500691b489c3da88197a8ebc10e4b9a4cc5', tool_calls=[{'name': 'create_issue', 'args': {}, 'id': 'call_kybPhUtrbkUi5ttuca6enUTa', 'type': 'tool_call'}], usage_metadata={'input_tokens': 14033, 'output_tokens': 464, 'total_tokens': 14497, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 448}}), ToolMessage(content=\"Tool 'create_issue' failed: ToolException('Validation Error: fields: Field required')\", name='create_issue', tool_call_id='call_kybPhUtrbkUi5ttuca6enUTa'), AIMessage(content=[{'id': 'rs_0a27a7bebc050be500691b48b0f2d8819794f7d83f5ec3b392', 'summary': [], 'type': 'reasoning', 'encrypted_content': 'gAAAAABpG0i0AGsArRn9NsqPHCJc0eMhRZD0Sq9inLptzGuHI2qTbuc2mHULQSKWEdmAPxieVGqiKacJzfIOhcJdVKG8-spnQDR9Bcixlzr6Fy5NmnjKKPGXqGthBoTI2j5FLdkEwb9JNGRo0ZpWkiyxJJR9Strk13lJ6v-LvFkbCs3oJ_7Ve7ML0J1Kk2bxDMSdY1bwgFNEtprHVFZyXUIJd0XtYWcZukDImaUX8WQJRFdpJMQrDE6VX1PiMJvwu8kR4STIshrHjCq_kZkHsGv_qCSBP1BmZyns0MoRmhdiHOAK0uH0Fr7rfXgzwh4uMBu0CGyLJcPMF0g8rybXr21VF2Y2k0QHCAosPJY02UepjLjG3FGJpGjZ8lAEl101PZ6nA9Hea7jHxMbgduptKyaivx2i10bmZ17rmKZME0ssHqDOmukGbLwsGx8pFnOM6BZDhSeAGUJcv0Pufv-u0N2KqQjrpxdkL02DrUamxv_yLmCj0gYQ4xIlZvN_HnpIx7EejrT0GrUNSGv2Uf9zZP3wuPingkgqaJiNZ3O08Lhfjl75l3QPZROdntP-sALO4-_XTOeSNT1548x05tgtxfkyme4BEF95m5LlBav1Y3bfOixFEqMy8VA6FYPMnTL0JAq1-P0qRdRGY-JJMtYAUP3ovlWR1gocrrgCOSJ1u3JVbL1rHo2f5vssqhqFSZKlMX0Whxw39gj7QXGt46aIeJmeYqSCbSuACCxhvtuh-_guVnjSK2fSbQgKmwHS5I9v9HLyHxIMYl9wFz8fG3j1CVHOrq4yLskkeaB7e0U5plMFulLfSNVjTXxyKIssdeGsNXlx8c8LeJX2Mc8peLnmfCWqHAt07amciP8wQO2wpXB3GkLD8ggMvaDVi0w0DqoDd3ooNzoR3q3ZgajJSxErcPhmkEoeaOSyFtuIkagzQDtsXBt3Ur9FYjPd5RQ6PG_nguUriMaKK_oonOOhOMQuXHpqY6wBU8DeVMNYxeLwKTzO3aPlDnOcVnNktFMCA7a7bnQfikx6nMVoN2fUVEU8SYb193QaB-2rlE_7A42hdsLXTSgz8pzxN63So-kTiVVfzdsWbYIzr9oclEyQPc-cA_-Tf1XWQmqM_N5JOiXhzrvV8KlgKlOw0GRXI4Nbhk8Dbfy5HKyegnJZxdZe7-BzXjXRc_teO_8tJNtoSGcs9lppEPzt6GqvMKAuRXWHXoXyZbnvixm7Wi_p5Irok2tnjOQHJbNuRsfsgwNhmqjuexv5X6n-Si040AS_IJZe46X3OdU8No3ow1Wy3nIe1KRFvTQBGphgcI5dUOfjoSq_NZggdLd8ciJN6CjTe4_vz09m8I_PZQ9yiTvJqsFD8A-O_3-9gCU_1npbL7e2uuZqTNmNRBOBpqQsgjy9BX4KuB2r0hfxtsnmoQPenD0x7hbTSNbkuFq6IKYxpQ=='}, {'arguments': '{\\n  \"fields\": {\\n    \"project\": {\\n      \"key\": \"MOB\"\\n    },\\n    \"issuetype\": {\\n      \"name\": \"Bug\"\\n    },\\n    \"summary\": \"Login button not working\",\\n    \"description\": \"Users cannot click the login button\"\\n  }\\n}', 'call_id': 'call_4phCgtK2mjYbyp6dS4qEuhHa', 'name': 'create_issue', 'type': 'function_call', 'id': 'fc_0a27a7bebc050be500691b48b2afd88197820e82e80ca49d61', 'status': 'completed'}], additional_kwargs={}, response_metadata={'id': 'resp_0a27a7bebc050be500691b48afb53081978e65148c0d0857d1', 'created_at': 1763395759.0, 'metadata': {}, 'model': 'gpt-5-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07'}, id='resp_0a27a7bebc050be500691b48afb53081978e65148c0d0857d1', tool_calls=[{'name': 'create_issue', 'args': {'fields': {'project': {'key': 'MOB'}, 'issuetype': {'name': 'Bug'}, 'summary': 'Login button not working', 'description': 'Users cannot click the login button'}}, 'id': 'call_4phCgtK2mjYbyp6dS4qEuhHa', 'type': 'tool_call'}], usage_metadata={'input_tokens': 14555, 'output_tokens': 140, 'total_tokens': 14695, 'input_token_details': {'cache_read': 14464}, 'output_token_details': {'reasoning': 64}}), ToolMessage(content='{\"id\": \"18\", \"key\": \"MOB-4\", \"self\": \"http://localhost:8015/rest/api/3/issue/18\", \"transition\": null}', name='create_issue', tool_call_id='call_4phCgtK2mjYbyp6dS4qEuhHa'), AIMessage(content=[{'type': 'text', 'text': 'Bug created: MOB-4\\nSummary: Login button not working\\nDescription: Users cannot click the login button\\n\\nAnything else you‚Äôd like to add (priority, assignee, labels)?', 'annotations': [], 'id': 'msg_0a27a7bebc050be500691b48b7f9a88197a7ecee6d3600debe'}], additional_kwargs={}, response_metadata={'id': 'resp_0a27a7bebc050be500691b48b5debc8197895730dc10c944c0', 'created_at': 1763395766.0, 'metadata': {}, 'model': 'gpt-5-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07'}, id='resp_0a27a7bebc050be500691b48b5debc8197895730dc10c944c0', usage_metadata={'input_tokens': 14759, 'output_tokens': 42, 'total_tokens': 14801, 'input_token_details': {'cache_read': 14592}, 'output_token_details': {'reasoning': 0}})], metadata={'steps': 3}, database_id='168586d5-698d-4105-9daa-efe38b0b61e8', reasoning_traces=[], error=None, langsmith_url=None, langfuse_url=None), verifier_results=[VerifierResult(name='DatabaseVerifier', success=True, expected_value=1, actual_value=1, comparison_type='equals', error=None, metadata={'query': \"SELECT COUNT(*) FROM issue WHERE summary = 'Login button not working'\", 'verifier_type': 'database_state'})], error=None, metadata={'file_stem': 'simple_task', 'temperature': 0.1}, prompt_text=\"Create a bug issue in project MOB with summary 'Login button not working' and description 'Users cannot click the login button'\", execution_time_ms=32402)], model_names=['gpt-5'])\n",
            "\n",
            "gpt-5 - create_bug: ‚úì PASS\n",
            "  Steps: 3\n",
            "  Database ID: 168586d5-698d-4105-9daa-efe38b0b61e8\n",
            "  Verifiers:\n",
            "    ‚úì DatabaseVerifier: Expected 1, Got 1\n",
            "\n",
            "‚úì Harness completed! Pass rate: 1/1\n"
          ]
        }
      ],
      "source": [
        "from turing_rl_sdk import TestHarness, TestHarnessConfig, MCPConfig, create_agent\n",
        "\n",
        "async def run_simple_harness():\n",
        "    # Configure MCP server\n",
        "    mcp_config = MCPConfig(\n",
        "        name=\"jira\",\n",
        "        url=\"http://localhost:8015/mcp\",\n",
        "        transport=\"streamable_http\"\n",
        "    )\n",
        "    \n",
        "    # Create harness\n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(\"simple_task.json\"),\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=mcp_config,\n",
        "            max_steps=50,\n",
        "            tool_call_limit=100,\n",
        "            runs_per_scenario=1,\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    print(f\"Loaded {len(harness.scenarios)} scenario(s)\")\n",
        "    \n",
        "    # Run benchmarks\n",
        "    results = await harness.run(\n",
        "        models=[\"gpt-5\"],\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    print(results)\n",
        "    # Print results\n",
        "    for result in results:\n",
        "        status = \"‚úì PASS\" if result.success else \"‚úó FAIL\"\n",
        "        print(f\"\\n{result.model} - {result.scenario_id}: {status}\")\n",
        "        print(f\"  Steps: {result.result.metadata.get('steps')}\")\n",
        "        print(f\"  Database ID: {result.result.database_id}\")\n",
        "        \n",
        "        if result.error:\n",
        "            print(f\"  Error: {result.error}\")\n",
        "        \n",
        "        # Show verifier results\n",
        "        if result.verifier_results:\n",
        "            print(f\"  Verifiers:\")\n",
        "            for v in result.verifier_results:\n",
        "                v_status = \"‚úì\" if v.success else \"‚úó\"\n",
        "                print(f\"    {v_status} {v.name}: Expected {v.expected_value}, Got {v.actual_value}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run it\n",
        "results = await run_simple_harness()\n",
        "print(f\"\\n‚úì Harness completed! Pass rate: {sum(r.success for r in results)}/{len(results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**That's it!** The harness handled:\n",
        "- Loading the scenario\n",
        "- Creating the agent\n",
        "- Connecting to MCP\n",
        "- Running the task\n",
        "- Verifying the result\n",
        "- Collecting metrics\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 2: Using Agents Without Harness\n",
        "\n",
        "**Best for:** One-off tasks, interactive testing, custom workflows\n",
        "\n",
        "You can use agents directly without the harness:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success: True\n",
            "Steps: 2\n",
            "Database ID: 1fac81ef-24c5-4add-bf01-af700646020e\n",
            "\n",
            "Conversation (5 entries):\n",
            "  1. MessageRole.USER: Create a bug issue in project MOB titled 'Homepage not loadi...\n",
            "  2. MessageRole.ASSISTANT: I'll create a bug issue in project MOB with the specified ti...\n",
            "  3. Tool: bulk_create_issues\n",
            "  4. Tool result: bulk_create_issues\n",
            "  5. MessageRole.ASSISTANT: Perfect! I've successfully created the bug issue in project ...\n",
            "\n",
            "‚úì Direct agent execution complete!\n"
          ]
        }
      ],
      "source": [
        "from turing_rl_sdk import ClaudeAgent, Task, MCPConfig\n",
        "\n",
        "async def run_agent_directly():\n",
        "    # Create agent\n",
        "    agent = ClaudeAgent(\n",
        "        model=\"claude-sonnet-4-5\",\n",
        "        temperature=1,\n",
        "        tool_call_limit=100,\n",
        "    )\n",
        "    \n",
        "    # Define task\n",
        "    task = Task(\n",
        "        prompt=\"Create a bug issue in project MOB titled 'Homepage not loading' with description 'Users report 500 error'\",\n",
        "        mcp=MCPConfig(\n",
        "            name=\"jira\",\n",
        "            url=\"http://localhost:8015/mcp\",\n",
        "            transport=\"streamable_http\"\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Run task\n",
        "    result = await agent.run(task, max_steps=50)\n",
        "    \n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Steps: {result.metadata.get('steps')}\")\n",
        "    print(f\"Database ID: {result.database_id}\")\n",
        "    \n",
        "    if result.error:\n",
        "        print(f\"Error: {result.error}\")\n",
        "    \n",
        "    # Access conversation history\n",
        "    conversation = result.get_conversation_history()\n",
        "    print(f\"\\nConversation ({len(conversation)} entries):\")\n",
        "    for i, entry in enumerate(conversation[:5]):  # Show first 5\n",
        "        if entry[\"type\"] == \"message\":\n",
        "            print(f\"  {i+1}. {entry['role']}: {entry['content'][:60]}...\")\n",
        "        elif entry[\"type\"] == \"tool_call\":\n",
        "            print(f\"  {i+1}. Tool: {entry['tool']}\")\n",
        "        elif entry[\"type\"] == \"tool_result\":\n",
        "            print(f\"  {i+1}. Tool result: {entry['tool']}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "result = await run_agent_directly()\n",
        "print(\"\\n‚úì Direct agent execution complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manual Verification\n",
        "\n",
        "When using agents directly, you can manually verify results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified: True\n",
            "Expected: 1, Got: 1\n",
            "\n",
            "‚úì Verification complete!\n"
          ]
        }
      ],
      "source": [
        "from turing_rl_sdk import DatabaseVerifier\n",
        "\n",
        "async def verify_result(result):\n",
        "    # Create verifier\n",
        "    verifier = DatabaseVerifier(\n",
        "        query=\"SELECT COUNT(*) FROM issue WHERE summary = 'Homepage not loading'\",\n",
        "        expected_value=1,\n",
        "        mcp_url=\"http://localhost:8015/mcp\",\n",
        "        database_id=result.database_id,  # Use same database as task\n",
        "        comparison=\"equals\"\n",
        "    )\n",
        "    \n",
        "    # Run verification\n",
        "    verifier_result = await verifier.verify()\n",
        "    \n",
        "    print(f\"Verified: {verifier_result.success}\")\n",
        "    print(f\"Expected: {verifier_result.expected_value}, Got: {verifier_result.actual_value}\")\n",
        "    if verifier_result.error:\n",
        "        print(f\"Error: {verifier_result.error}\")\n",
        "    \n",
        "    return verifier_result\n",
        "\n",
        "# Verify the previous result\n",
        "verifier_result = await verify_result(result)\n",
        "print(\"\\n‚úì Verification complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì QwenAgent class defined!\n"
          ]
        }
      ],
      "source": [
        "from turing_rl_sdk import Agent, AgentResponse\n",
        "from turing_rl_sdk.agents.parsers import OpenAIResponseParser, ResponseParser\n",
        "from turing_rl_sdk.agents.utils import retry_with_backoff\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import BaseMessage, AIMessage\n",
        "\n",
        "class QwenAgent(Agent):\n",
        "    \"\"\"Custom agent for Alibaba Cloud Qwen models.\n",
        "    \n",
        "    Uses OpenAI-compatible API from DashScope.\n",
        "    Requires DASHSCOPE_API_KEY environment variable.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"qwen-plus\",\n",
        "        temperature: float = 0.1,\n",
        "        max_output_tokens: int | None = None,\n",
        "        tool_call_limit: int = 1000,\n",
        "        system_prompt: str | None = None,\n",
        "    ):\n",
        "        super().__init__(system_prompt=system_prompt, tool_call_limit=tool_call_limit)\n",
        "        \n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_output_tokens = max_output_tokens\n",
        "        \n",
        "        # Model mapping\n",
        "        model_map = {\n",
        "            \"qwen-14b\": \"qwen3-14b\",\n",
        "            \"qwen-plus\": \"qwen-plus\",\n",
        "            \"qwen-turbo\": \"qwen-turbo\",\n",
        "            \"qwen-max\": \"qwen-max\",\n",
        "        }\n",
        "        self.actual_model = model_map.get(model.lower(), model)\n",
        "        \n",
        "        # Get base URL and API key\n",
        "        self.base_url = os.environ.get(\n",
        "            \"DASHSCOPE_BASE_URL\",\n",
        "            \"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
        "        )\n",
        "        self.api_key = os.environ.get(\"DASHSCOPE_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            print(\"‚ö†Ô∏è  DASHSCOPE_API_KEY not set - agent will fail at runtime\")\n",
        "    \n",
        "    def _build_llm(self):\n",
        "        \"\"\"Build LLM client (called during agent.initialize()).\"\"\"\n",
        "        config = {\n",
        "            \"model\": self.actual_model,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"timeout\": None,\n",
        "            \"max_retries\": 3,\n",
        "            \"base_url\": self.base_url,\n",
        "            \"api_key\": self.api_key,\n",
        "        }\n",
        "        \n",
        "        if self.max_output_tokens is not None:\n",
        "            config[\"max_completion_tokens\"] = self.max_output_tokens\n",
        "        \n",
        "        # Disable thinking mode for non-streaming\n",
        "        config[\"extra_body\"] = {\"enable_thinking\": False}\n",
        "        \n",
        "        llm = ChatOpenAI(**config)\n",
        "        # Bind tools to LLM (self._tools is set during initialize())\n",
        "        return llm.bind_tools(self._tools) if self._tools else llm\n",
        "    \n",
        "    async def get_response(self, messages: list[BaseMessage]) -> tuple[AgentResponse, AIMessage]:\n",
        "        \"\"\"Get model response with retry logic.\"\"\"\n",
        "        if not self._llm:\n",
        "            raise RuntimeError(\"LLM not initialized. Call initialize() first.\")\n",
        "        \n",
        "        # Call LLM with retry logic\n",
        "        async def _invoke():\n",
        "            return await self._llm.ainvoke(messages)\n",
        "        \n",
        "        ai_message = await retry_with_backoff(\n",
        "            _invoke,\n",
        "            max_retries=2,\n",
        "            timeout_seconds=600.0,\n",
        "            on_retry=lambda attempt, exc, delay: None,\n",
        "        )\n",
        "        \n",
        "        # Parse response using OpenAI parser (compatible format)\n",
        "        parser = self.get_response_parser()\n",
        "        parsed = parser.parse(ai_message)\n",
        "        \n",
        "        # Convert to AgentResponse\n",
        "        agent_response = AgentResponse(\n",
        "            content=parsed.content,\n",
        "            tool_calls=parsed.tool_calls,\n",
        "            reasoning=\"\\n\".join(parsed.reasoning) if parsed.reasoning else None,\n",
        "            done=not bool(parsed.tool_calls),\n",
        "        )\n",
        "        \n",
        "        return agent_response, ai_message\n",
        "    \n",
        "    def get_response_parser(self) -> ResponseParser:\n",
        "        \"\"\"Get OpenAI-compatible response parser.\"\"\"\n",
        "        return OpenAIResponseParser()\n",
        "\n",
        "print(\"‚úì QwenAgent class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-4o: ‚úó FAIL\n",
            "‚úì Custom agent factory ready! Now the harness can use ANY agent you create.\n"
          ]
        }
      ],
      "source": [
        "def custom_agent_factory(model: str, **kwargs):\n",
        "    \"\"\"Factory function for creating agents (including custom ones).\"\"\"\n",
        "    if model.startswith(\"qwen\"):\n",
        "        return QwenAgent(model=model, **kwargs)\n",
        "    else:\n",
        "        # Fall back to built-in agents\n",
        "        return create_agent(model, **kwargs)\n",
        "\n",
        "async def run_harness_with_custom_agent():\n",
        "    \"\"\"Run harness with custom agent factory.\"\"\"\n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(\"simple_task.json\"),\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=MCPConfig(\n",
        "                name=\"jira\",\n",
        "                url=\"http://localhost:8015/mcp\",\n",
        "                transport=\"streamable_http\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Run with both custom (Qwen) and built-in (GPT) agents\n",
        "    results = await harness.run(\n",
        "        models=[\"gpt-4o\"],  # Add \"qwen-plus\" if you have DASHSCOPE_API_KEY\n",
        "        agent_factory=custom_agent_factory,\n",
        "    )\n",
        "    \n",
        "    for result in results:\n",
        "        status = \"‚úì PASS\" if result.success else \"‚úó FAIL\"\n",
        "        print(f\"{result.model}: {status}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run it\n",
        "results = await run_harness_with_custom_agent()\n",
        "print(\"‚úì Custom agent factory ready! Now the harness can use ANY agent you create.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 4: Custom Verifier\n",
        "\n",
        "**Best for:** Complex validation logic, custom result checks\n",
        "\n",
        "Beyond database queries, you can create verifiers for any validation logic:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Check User ID: True\n",
            "  Expected: 1, Got: 1\n",
            "\n",
            "‚úì Custom verifier works!\n"
          ]
        }
      ],
      "source": [
        "from turing_rl_sdk import Verifier, VerifierResult\n",
        "import httpx\n",
        "\n",
        "class APIResponseVerifier(Verifier):\n",
        "    \"\"\"Verify that an API endpoint returns expected data.\n",
        "    \n",
        "    Example: Check if an issue was created by querying the API directly.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        endpoint: str,\n",
        "        expected_field: str,\n",
        "        expected_value: any,\n",
        "        name: str | None = None\n",
        "    ):\n",
        "        super().__init__(name or \"APIResponseVerifier\")\n",
        "        self.endpoint = endpoint\n",
        "        self.expected_field = expected_field\n",
        "        self.expected_value = expected_value\n",
        "    \n",
        "    async def verify(self) -> VerifierResult:\n",
        "        \"\"\"Execute verification.\"\"\"\n",
        "        try:\n",
        "            async with httpx.AsyncClient() as client:\n",
        "                response = await client.get(self.endpoint)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                \n",
        "                actual_value = data.get(self.expected_field)\n",
        "                success = actual_value == self.expected_value\n",
        "                \n",
        "                return VerifierResult(\n",
        "                    name=self.name,\n",
        "                    success=success,\n",
        "                    expected_value=self.expected_value,\n",
        "                    actual_value=actual_value,\n",
        "                    comparison_type=\"equals\",\n",
        "                    error=None if success else \"Value mismatch\",\n",
        "                )\n",
        "        except Exception as exc:\n",
        "            return VerifierResult(\n",
        "                name=self.name,\n",
        "                success=False,\n",
        "                expected_value=self.expected_value,\n",
        "                actual_value=None,\n",
        "                comparison_type=\"equals\",\n",
        "                error=str(exc),\n",
        "            )\n",
        "\n",
        "# Test it\n",
        "async def test_custom_verifier():\n",
        "    verifier = APIResponseVerifier(\n",
        "        endpoint=\"https://jsonplaceholder.typicode.com/todos/1\",\n",
        "        expected_field=\"userId\",\n",
        "        expected_value=1,\n",
        "        name=\"Check User ID\"\n",
        "    )\n",
        "    \n",
        "    result = await verifier.verify()\n",
        "    print(f\"‚úì {result.name}: {result.success}\")\n",
        "    print(f\"  Expected: {result.expected_value}, Got: {result.actual_value}\")\n",
        "\n",
        "await test_custom_verifier()\n",
        "print(\"\\n‚úì Custom verifier works!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 5: Observers for Progress Tracking\n",
        "\n",
        "**Best for:** Real-time monitoring, debugging, custom logging\n",
        "\n",
        "Add observers to track execution in real-time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì DetailedObserver class defined!\n"
          ]
        }
      ],
      "source": [
        "from turing_rl_sdk.agents.runtime.events import RunObserver\n",
        "\n",
        "class DetailedObserver(RunObserver):\n",
        "    \"\"\"Observer that tracks everything with nice formatting.\"\"\"\n",
        "    \n",
        "    def __init__(self, label: str):\n",
        "        self.label = label\n",
        "        self.stats = {\n",
        "            \"messages\": 0,\n",
        "            \"tool_calls\": 0,\n",
        "            \"tool_errors\": 0,\n",
        "        }\n",
        "    \n",
        "    async def on_message(self, role: str, content: str, metadata=None):\n",
        "        self.stats[\"messages\"] += 1\n",
        "        if role == \"assistant\":\n",
        "            print(f\"[{self.label}] üí¨ Agent: {content[:80]}...\")\n",
        "    \n",
        "    async def on_tool_call(self, tool_name, arguments, result, is_error=False):\n",
        "        self.stats[\"tool_calls\"] += 1\n",
        "        if is_error:\n",
        "            self.stats[\"tool_errors\"] += 1\n",
        "        status = \"‚úó\" if is_error else \"‚úì\"\n",
        "        print(f\"[{self.label}] üîß Tool {status}: {tool_name}\")\n",
        "    \n",
        "    async def on_status(self, message: str, level: str = \"info\"):\n",
        "        emoji = {\"info\": \"‚ÑπÔ∏è\", \"warning\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\"}.get(level, \"‚ÑπÔ∏è\")\n",
        "        print(f\"[{self.label}] {emoji} {message}\")\n",
        "    \n",
        "    def print_stats(self):\n",
        "        print(f\"\\nüìä [{self.label}] Statistics:\")\n",
        "        print(f\"   Messages: {self.stats['messages']}\")\n",
        "        print(f\"   Tool calls: {self.stats['tool_calls']}\")\n",
        "        print(f\"   Tool errors: {self.stats['tool_errors']}\")\n",
        "\n",
        "print(\"‚úì DetailedObserver class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Observer with Harness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Harness path does not exist: simple_task.json",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Run with tracking\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m run_with_observer()\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úì Observer pattern ready!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrun_with_observer\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_with_observer\u001b[39m():\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run harness with detailed progress tracking.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     harness = \u001b[43mTestHarness\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mharness_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimple_task.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTestHarnessConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmcp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMCPConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjira\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://localhost:8015/mcp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstreamable_http\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Create observer (one per run)\u001b[39;00m\n\u001b[32m     16\u001b[39m     observer = DetailedObserver(\u001b[33m\"\u001b[39m\u001b[33mbenchmark\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/rl-gym-tooling/turing_rl_sdk/src/turing_rl_sdk/harness/orchestrator.py:351\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, harness_path, config)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize test harness.\u001b[39;00m\n\u001b[32m    345\u001b[39m \n\u001b[32m    346\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    347\u001b[39m \u001b[33;03m    harness_path: Path to harness file or directory\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    config: Test harness configuration\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;28mself\u001b[39m.harness_path = harness_path\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m    352\u001b[39m \u001b[38;5;28mself\u001b[39m.scenarios: \u001b[38;5;28mlist\u001b[39m[Scenario] = []\n\u001b[32m    353\u001b[39m \u001b[38;5;28mself\u001b[39m.file_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Scenario]] = {}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/rl-gym-tooling/turing_rl_sdk/src/turing_rl_sdk/harness/orchestrator.py:356\u001b[39m, in \u001b[36m_load_scenarios\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28mself\u001b[39m.file_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Scenario]] = {}\n\u001b[32m    354\u001b[39m \u001b[38;5;28mself\u001b[39m.observer_factories: \u001b[38;5;28mlist\u001b[39m[Callable[[], RunObserver]] = []\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m \u001b[38;5;28mself\u001b[39m._load_scenarios()\n",
            "\u001b[31mValueError\u001b[39m: Harness path does not exist: simple_task.json"
          ]
        }
      ],
      "source": [
        "async def run_with_observer():\n",
        "    \"\"\"Run harness with detailed progress tracking.\"\"\"\n",
        "    \n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(\"simple_task.json\"),\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=MCPConfig(\n",
        "                name=\"jira\",\n",
        "                url=\"http://localhost:8015/mcp\",\n",
        "                transport=\"streamable_http\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Create observer (one per run)\n",
        "    observer = DetailedObserver(\"benchmark\")\n",
        "    harness.add_observer_factory(lambda: observer)\n",
        "    \n",
        "    print(\"üöÄ Starting benchmark with observer...\\n\")\n",
        "    \n",
        "    # Run\n",
        "    results = await harness.run(\n",
        "        models=[\"gpt-5\"],\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    \n",
        "    # Print statistics\n",
        "    observer.print_stats()\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nüìä Results:\")\n",
        "    for result in results:\n",
        "        status = \"‚úì PASS\" if result.success else \"‚úó FAIL\"\n",
        "        print(f\"  {result.scenario_id}: {status}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run with tracking\n",
        "results = await run_with_observer()\n",
        "print(\"‚úì Observer pattern ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 6: Multiple Models Comparison\n",
        "\n",
        "**Best for:** Benchmarking across different LLM providers\n",
        "\n",
        "Compare how different models perform on the same scenarios:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèÅ Comparing 4 models...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ipriyam26/Programming/rl-gym-tooling/turing_rl_sdk/src/turing_rl_sdk/harness/agent_factory.py:109: UserWarning: Claude thinking mode requires temperature=1.0. Your temperature=0.1 will be overridden. Set temperature=1.0 or enable_thinking=False to suppress this warning.\n",
            "  return ClaudeAgent(\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "async def compare_models():\n",
        "    \"\"\"Compare multiple models on the same scenarios.\"\"\"\n",
        "    \n",
        "    harness = TestHarness(\n",
        "        harness_path=Path(\"simple_task.json\"),\n",
        "        config=TestHarnessConfig(\n",
        "            mcp=MCPConfig(\n",
        "                name=\"jira\",\n",
        "                url=\"http://localhost:8015/mcp\",\n",
        "                transport=\"streamable_http\"\n",
        "            ),\n",
        "            runs_per_scenario=3,  # Run each 3 times for reliability\n",
        "            max_concurrent_runs=10,\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Compare multiple models\n",
        "    models = [\n",
        "        \"gpt-4o\",\n",
        "        \"gpt-4o-mini\",\n",
        "        \"claude-sonnet-4-5\",\n",
        "        \"gemini-2.0-flash-exp\",\n",
        "    ]\n",
        "    \n",
        "    print(f\"üèÅ Comparing {len(models)} models...\\n\")\n",
        "    \n",
        "    results = await harness.run(\n",
        "        models=models,\n",
        "        agent_factory=create_agent,\n",
        "    )\n",
        "    \n",
        "    # Aggregate by model\n",
        "    model_stats = defaultdict(lambda: {\"total\": 0, \"passed\": 0, \"steps\": []})\n",
        "    \n",
        "    for result in results:\n",
        "        model_stats[result.model][\"total\"] += 1\n",
        "        if result.success:\n",
        "            model_stats[result.model][\"passed\"] += 1\n",
        "        model_stats[result.model][\"steps\"].append(result.result.metadata.get(\"steps\", 0))\n",
        "    \n",
        "    # Print comparison\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä MODEL COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"{'Model':<30} {'Pass Rate':<15} {'Avg Steps':<10}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    for model, stats in sorted(model_stats.items()):\n",
        "        pass_rate = stats[\"passed\"] / stats[\"total\"] * 100\n",
        "        avg_steps = sum(stats[\"steps\"]) / len(stats[\"steps\"]) if stats[\"steps\"] else 0\n",
        "        print(f\"{model:<30} {pass_rate:>6.1f}%          {avg_steps:>6.1f}\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return results, model_stats\n",
        "\n",
        "# Run comparison\n",
        "results, stats = await compare_models()\n",
        "print(\"‚úì Model comparison ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pattern 7: Export Results\n",
        "\n",
        "**Best for:** Analysis, reporting, debugging\n",
        "\n",
        "Export results to JSON for further analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_results(results, filename=\"results.json\"):\n",
        "    \"\"\"Export results to JSON file with full details.\"\"\"\n",
        "    data = results.build_model_reports()\n",
        "    \n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump([mf.payload for mf in data], f, indent=2)\n",
        "    \n",
        "    print(f\"‚úì Exported {len(results)} results to {filename}\")\n",
        "    \n",
        "    # Show what's included\n",
        "#     if results:\n",
        "#         sample = results[0].to_dict()\n",
        "#         print(f\"\\nüì¶ Each result includes:\")\n",
        "#         print(f\"   - model, scenario_id, scenario_name\")\n",
        "#         print(f\"   - success, error\")\n",
        "#         print(f\"   - steps, database_id\")\n",
        "#         print(f\"   - conversation ({len(sample.get('conversation', []))} entries)\")\n",
        "#         print(f\"   - verifier_results\")\n",
        "#         print(f\"   - reasoning_traces (if available)\")\n",
        "        \n",
        "#         # Show conversation structure\n",
        "#         if sample.get('conversation'):\n",
        "#             print(f\"\\nüí¨ Conversation format:\")\n",
        "#             entry = sample['conversation'][0]\n",
        "#             print(f\"   Type: {entry.get('type')}\")\n",
        "#             print(f\"   Keys: {list(entry.keys())}\")\n",
        "\n",
        "# # Example usage:\n",
        "async def save_results():\n",
        "    results = await run_simple_harness()\n",
        "    export_results(results, \"benchmark_results.json\")\n",
        "    return results\n",
        "\n",
        "await save_results()\n",
        "\n",
        "print(\"‚úì Export function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**üéâ You've learned all the core patterns!**\n",
        "\n",
        "### The SDK Philosophy: **Harness-First**\n",
        "\n",
        "The TestHarness is the main component. It orchestrates:\n",
        "- ‚úÖ Agent creation\n",
        "- ‚úÖ MCP connections\n",
        "- ‚úÖ Task execution\n",
        "- ‚úÖ Result verification\n",
        "- ‚úÖ Metrics collection\n",
        "\n",
        "### Usage Patterns Recap\n",
        "\n",
        "1. **Simplest Harness** ‚≠ê - Start here!\n",
        "   - Create JSON file with scenarios\n",
        "   - Run with `harness.run(models, agent_factory)`\n",
        "   - Get results with verification\n",
        "\n",
        "2. **Direct Agent Usage** - For one-off tasks\n",
        "   - `agent.run(task)` without harness\n",
        "   - Manual verification with `DatabaseVerifier`\n",
        "\n",
        "3. **Custom Agents** - Integrate any LLM\n",
        "   - Subclass `Agent`\n",
        "   - Implement `_build_llm()`, `get_response()`, `get_response_parser()`\n",
        "   - Use with harness via custom factory\n",
        "\n",
        "4. **Custom Verifiers** - Complex validation\n",
        "   - Subclass `Verifier`\n",
        "   - Implement `verify()` method\n",
        "   - Use programmatically (harness integration requires extending loader)\n",
        "\n",
        "5. **Observers** - Real-time tracking\n",
        "   - Subclass `RunObserver`\n",
        "   - Track messages, tool calls, status updates\n",
        "   - Add to harness with `add_observer_factory()`\n",
        "\n",
        "6. **Model Comparison** - Systematic benchmarking\n",
        "   - Pass multiple models to `harness.run()`\n",
        "   - Aggregate and compare results\n",
        "   - Statistical analysis (multiple runs per scenario)\n",
        "\n",
        "7. **Export Results** - Analysis and reporting\n",
        "   - `result.to_dict()` for JSON serialization\n",
        "   - Includes conversation, verifiers, reasoning traces\n",
        "   - Ready for pandas, matplotlib, etc.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "‚úÖ **Start with the harness** - It's the recommended approach  \n",
        "‚úÖ **The harness orchestrates everything** - Agents, MCP, verification  \n",
        "‚úÖ **Custom agents integrate seamlessly** - Just implement 3 methods  \n",
        "‚úÖ **Observers provide visibility** - Real-time progress tracking  \n",
        "‚úÖ **Results are export-ready** - JSON with full details  \n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Read the full README for API reference\n",
        "2. Create your own harness files (see `9_tasks/task1.json` for examples)\n",
        "3. Build custom agents for your LLM providers\n",
        "4. Create custom verifiers for your use cases\n",
        "5. Run large-scale benchmarks!\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- **README.md** - Complete API reference\n",
        "- **simple_harness_example.py** - Basic example script\n",
        "- **9_tasks/** - Real benchmark scenarios\n",
        "- **QwenAgent** - Example custom agent implementation\n",
        "\n",
        "Happy benchmarking! üéØ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
