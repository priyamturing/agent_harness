{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50c44d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 160\n"
     ]
    }
   ],
   "source": [
    "# Load all result files from Sample-Harness_1\n",
    "from glob import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "files = glob(\"results/Sample-Harness_1/run_*.json\")\n",
    "print(f\"Total files found: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fe87442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per model:\n",
      "  gemini-2.5-pro: 40\n",
      "  claude-sonnet-4-5: 40\n",
      "  grok-4: 40\n",
      "  gpt-5: 40\n"
     ]
    }
   ],
   "source": [
    "# Group files by model\n",
    "collection = defaultdict(list)\n",
    "for file in files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    collection[str(data[\"model\"] or \"unknown\")].append(file)\n",
    "\n",
    "print(\"Files per model:\")\n",
    "for model, file_list in collection.items():\n",
    "    print(f\"  {model}: {len(file_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6684822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define success checker function\n",
    "def did_model_succeed(f):\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    is_success = all(scenario[\"success\"] for scenario in data[\"scenarios\"][0][\"verifiers\"])\n",
    "    return is_success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "779d89c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOTAL SUCCESS RATES BY MODEL\n",
      "================================================================================\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  Total runs: 40\n",
      "  Successful: 18\n",
      "  Failed: 22\n",
      "  Success rate: 45.00%\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  Total runs: 40\n",
      "  Successful: 40\n",
      "  Failed: 0\n",
      "  Success rate: 100.00%\n",
      "\n",
      "grok-4:\n",
      "  Total runs: 40\n",
      "  Successful: 33\n",
      "  Failed: 7\n",
      "  Success rate: 82.50%\n",
      "\n",
      "gpt-5:\n",
      "  Total runs: 40\n",
      "  Successful: 30\n",
      "  Failed: 10\n",
      "  Success rate: 75.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gemini-2.5-pro': {'total': 40,\n",
       "  'success': 18,\n",
       "  'failed': 22,\n",
       "  'success_rate': 45.0},\n",
       " 'claude-sonnet-4-5': {'total': 40,\n",
       "  'success': 40,\n",
       "  'failed': 0,\n",
       "  'success_rate': 100.0},\n",
       " 'grok-4': {'total': 40, 'success': 33, 'failed': 7, 'success_rate': 82.5},\n",
       " 'gpt-5': {'total': 40, 'success': 30, 'failed': 10, 'success_rate': 75.0}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate total success rate per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TOTAL SUCCESS RATES BY MODEL\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "model_success = {}\n",
    "for model, files_list in collection.items():\n",
    "    total_runs = len(files_list)\n",
    "    successful_runs = sum(1 for f in files_list if did_model_succeed(f))\n",
    "    success_rate = (successful_runs / total_runs * 100) if total_runs > 0 else 0\n",
    "    \n",
    "    model_success[model] = {\n",
    "        \"total\": total_runs,\n",
    "        \"success\": successful_runs,\n",
    "        \"failed\": total_runs - successful_runs,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    \n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Total runs: {total_runs}\")\n",
    "    print(f\"  Successful: {successful_runs}\")\n",
    "    print(f\"  Failed: {total_runs - successful_runs}\")\n",
    "    print(f\"  Success rate: {success_rate:.2f}%\")\n",
    "    print()\n",
    "\n",
    "model_success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "711f8412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file name extraction (first 3 files per model):\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  run_task4-google-gemini-2.5-pro-6.json\n",
      "    -> Test: task4-google, Run: 6\n",
      "  run_task2-google-gemini-2.5-pro-1.json\n",
      "    -> Test: task2-google, Run: 1\n",
      "  run_task5-google-gemini-2.5-pro-5.json\n",
      "    -> Test: task5-google, Run: 5\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  run_task5-anthropic-claude-sonnet-4-5-2.json\n",
      "    -> Test: task5-anthropic, Run: 2\n",
      "  run_task4-anthropic-claude-sonnet-4-5-8.json\n",
      "    -> Test: task4-anthropic, Run: 8\n",
      "  run_task3-anthropic-claude-sonnet-4-5-3.json\n",
      "    -> Test: task3-anthropic, Run: 3\n"
     ]
    }
   ],
   "source": [
    "# File naming convention and extraction function\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_test_info(filepath, model):\n",
    "    \"\"\"Extract test name and run number from filepath.\n",
    "    \n",
    "    Pattern: run_{test_name}-{model_name}-{run_number}.json\n",
    "    Example: run_new_sys_task10_3_c1_p1_r8_v2_harness-claude-sonnet-4-5-1.json\n",
    "    \n",
    "    The challenge is that model names contain hyphens (grok-4, gpt-5, gemini-2.5-pro, claude-sonnet-4-5)\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filepath)\n",
    "    \n",
    "    # Remove 'run_' prefix\n",
    "    if basename.startswith('run_'):\n",
    "        basename = basename[4:]\n",
    "    \n",
    "    # Remove '.json' suffix\n",
    "    if basename.endswith('.json'):\n",
    "        basename = basename[:-5]\n",
    "    \n",
    "    # Strategy: find the model name followed by a hyphen and single/double digit number\n",
    "    # Escape special chars in model name for regex\n",
    "    model_escaped = re.escape(model)\n",
    "    \n",
    "    # Pattern: anything, then model, then hyphen, then 1-2 digits at the end\n",
    "    pattern = f'^(.+)-{model_escaped}-(\\\\d+)$'\n",
    "    match = re.match(pattern, basename)\n",
    "    \n",
    "    if match:\n",
    "        test_name = match.group(1)\n",
    "        run_number = match.group(2)\n",
    "        return test_name, run_number\n",
    "    \n",
    "    # Fallback: just extract last number\n",
    "    match = re.match(r'(.+)-(\\d+)$', basename)\n",
    "    if match:\n",
    "        test_name = match.group(1)\n",
    "        run_number = match.group(2)\n",
    "        return test_name, run_number\n",
    "    \n",
    "    return basename, \"unknown\"\n",
    "\n",
    "# Test the extraction\n",
    "print(\"Sample file name extraction (first 3 files per model):\")\n",
    "for model, files_list in list(collection.items())[:2]:\n",
    "    print(f\"\\n{model}:\")\n",
    "    for f in files_list[:3]:\n",
    "        test_name, run_num = extract_test_info(f, model)\n",
    "        print(f\"  {os.path.basename(f)}\")\n",
    "        print(f\"    -> Test: {test_name}, Run: {run_num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d662d070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUCCESS RATES PER TASK PER MODEL\n",
      "================================================================================\n",
      "\n",
      "Summary: Found unique tasks per model:\n",
      "  gemini-2.5-pro: 5 unique tasks\n",
      "  claude-sonnet-4-5: 5 unique tasks\n",
      "  grok-4: 5 unique tasks\n",
      "  gpt-5: 5 unique tasks\n",
      "\n",
      "================================================================================\n",
      "Checking run counts per task:\n",
      "================================================================================\n",
      "\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  5 tasks with 8 run(s)\n",
      "  \n",
      "  Examples of tasks with multiple runs:\n",
      "    - task1-google: 8 runs\n",
      "    - task2-google: 8 runs\n",
      "    - task3-google: 8 runs\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  5 tasks with 8 run(s)\n",
      "  \n",
      "  Examples of tasks with multiple runs:\n",
      "    - task1-anthropic: 8 runs\n",
      "    - task2-anthropic: 8 runs\n",
      "    - task3-anthropic: 8 runs\n",
      "\n",
      "grok-4:\n",
      "  5 tasks with 8 run(s)\n",
      "  \n",
      "  Examples of tasks with multiple runs:\n",
      "    - task1-xai: 8 runs\n",
      "    - task2-xai: 8 runs\n",
      "    - task3-xai: 8 runs\n",
      "\n",
      "gpt-5:\n",
      "  5 tasks with 8 run(s)\n",
      "  \n",
      "  Examples of tasks with multiple runs:\n",
      "    - task1-openai: 8 runs\n",
      "    - task2-openai: 8 runs\n",
      "    - task3-openai: 8 runs\n"
     ]
    }
   ],
   "source": [
    "# Group runs by task and analyze success per task per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"SUCCESS RATES PER TASK PER MODEL\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Structure: task_results[model][task_name] = {\"total\": X, \"success\": Y, \"runs\": [...]}\n",
    "def create_task_stats():\n",
    "    return {\"total\": 0, \"success\": 0, \"runs\": []}\n",
    "\n",
    "task_results = defaultdict(lambda: defaultdict(create_task_stats))\n",
    "\n",
    "for model, files_list in collection.items():\n",
    "    for file_path in files_list:\n",
    "        test_name, run_num = extract_test_info(file_path, model)\n",
    "        is_success = did_model_succeed(file_path)\n",
    "        \n",
    "        task_results[model][test_name][\"total\"] += 1\n",
    "        if is_success:\n",
    "            task_results[model][test_name][\"success\"] += 1\n",
    "        task_results[model][test_name][\"runs\"].append({\n",
    "            \"run_number\": run_num,\n",
    "            \"success\": is_success,\n",
    "            \"file\": file_path\n",
    "        })\n",
    "\n",
    "# Display summary\n",
    "print(f\"Summary: Found unique tasks per model:\")\n",
    "for model in task_results.keys():\n",
    "    unique_tasks = len(task_results[model])\n",
    "    print(f\"  {model}: {unique_tasks} unique tasks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Checking run counts per task:\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for model, tasks in task_results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    \n",
    "    # Group by run count\n",
    "    run_counts = defaultdict(int)\n",
    "    for task_name, stats in tasks.items():\n",
    "        run_counts[stats[\"total\"]] += 1\n",
    "    \n",
    "    for count in sorted(run_counts.keys(), reverse=True):\n",
    "        print(f\"  {run_counts[count]} tasks with {count} run(s)\")\n",
    "    \n",
    "    # Show examples of non-standard counts\n",
    "    non_standard = []\n",
    "    for task_name, stats in sorted(tasks.items()):\n",
    "        if stats[\"total\"] != 1:  # Assuming sample data has 1 run per task\n",
    "            non_standard.append((task_name, stats[\"total\"]))\n",
    "    \n",
    "    if non_standard:\n",
    "        print(f\"  \\n  Examples of tasks with multiple runs:\")\n",
    "        for task_name, count in non_standard[:3]:\n",
    "            print(f\"    - {task_name}: {count} runs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8a04937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAILED TASK SUCCESS RATES\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CLAUDE-SONNET-4-5\n",
      "================================================================================\n",
      "\n",
      "✅ task1-anthropic\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task2-anthropic\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task3-anthropic\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task4-anthropic\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task5-anthropic\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GEMINI-2.5-PRO\n",
      "================================================================================\n",
      "\n",
      "❌ task2-google\n",
      "   Success: 0/8 (0.0%)\n",
      "   Failed runs: 1, 7, 6, 5, 4, 8, 3, 2\n",
      "\n",
      "⚠️  task3-google\n",
      "   Success: 4/8 (50.0%)\n",
      "   Failed runs: 3, 6, 7, 1\n",
      "\n",
      "⚠️  task4-google\n",
      "   Success: 4/8 (50.0%)\n",
      "   Failed runs: 6, 7, 2, 5\n",
      "\n",
      "⚠️  task1-google\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 6, 4, 5\n",
      "\n",
      "⚠️  task5-google\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 8, 3, 2\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GPT-5\n",
      "================================================================================\n",
      "\n",
      "❌ task4-openai\n",
      "   Success: 3/8 (37.5%)\n",
      "   Failed runs: 1, 6, 5, 4, 2\n",
      "\n",
      "⚠️  task1-openai\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 7, 8, 2\n",
      "\n",
      "⚠️  task2-openai\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 7\n",
      "\n",
      "⚠️  task5-openai\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 1\n",
      "\n",
      "✅ task3-openai\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GROK-4\n",
      "================================================================================\n",
      "\n",
      "⚠️  task1-xai\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 4, 5, 1\n",
      "\n",
      "⚠️  task4-xai\n",
      "   Success: 5/8 (62.5%)\n",
      "   Failed runs: 4, 8, 6\n",
      "\n",
      "⚠️  task2-xai\n",
      "   Success: 7/8 (87.5%)\n",
      "   Failed runs: 8\n",
      "\n",
      "✅ task3-xai\n",
      "   Success: 8/8 (100.0%)\n",
      "\n",
      "✅ task5-xai\n",
      "   Success: 8/8 (100.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed success rate per task per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"DETAILED TASK SUCCESS RATES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for model in sorted(task_results.keys()):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    tasks = task_results[model]\n",
    "    \n",
    "    # Sort tasks by success rate (lowest first to highlight problems)\n",
    "    sorted_tasks = sorted(tasks.items(), key=lambda x: (x[1][\"success\"] / x[1][\"total\"], x[0]))\n",
    "    \n",
    "    for task_name, stats in sorted_tasks:\n",
    "        success_rate = (stats[\"success\"] / stats[\"total\"] * 100) if stats[\"total\"] > 0 else 0\n",
    "        \n",
    "        # Use different symbols based on success rate\n",
    "        if success_rate == 100:\n",
    "            symbol = \"✅\"\n",
    "        elif success_rate >= 50:\n",
    "            symbol = \"⚠️ \"\n",
    "        else:\n",
    "            symbol = \"❌\"\n",
    "        \n",
    "        print(f\"{symbol} {task_name}\")\n",
    "        print(f\"   Success: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
    "        \n",
    "        # Show which specific runs failed\n",
    "        failed_runs = [r for r in stats[\"runs\"] if not r[\"success\"]]\n",
    "        if failed_runs:\n",
    "            failed_run_nums = [r[\"run_number\"] for r in failed_runs]\n",
    "            print(f\"   Failed runs: {', '.join(failed_run_nums)}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "462a3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING RESULTS TO CSV\n",
      "================================================================================\n",
      "\n",
      "✅ Exported: sample_model_summary.csv\n",
      "✅ Exported: sample_task_results_detailed.csv\n",
      "✅ Exported: sample_run_details.csv\n",
      "\n",
      "================================================================================\n",
      "EXPORT COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  1. sample_model_summary.csv - Overall model statistics\n",
      "  2. sample_task_results_detailed.csv - Success rates per task per model\n",
      "  3. sample_run_details.csv - Individual run details\n"
     ]
    }
   ],
   "source": [
    "# Export results to CSV for analysis\n",
    "import csv\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"EXPORTING RESULTS TO CSV\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Export 1: Summary by model\n",
    "with open(\"sample_model_summary.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Total Runs\", \"Successful\", \"Failed\", \"Success Rate %\"])\n",
    "    \n",
    "    for model, stats in sorted(model_success.items()):\n",
    "        writer.writerow([\n",
    "            model,\n",
    "            stats[\"total\"],\n",
    "            stats[\"success\"],\n",
    "            stats[\"failed\"],\n",
    "            f\"{stats['success_rate']:.2f}\"\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: sample_model_summary.csv\")\n",
    "\n",
    "# Export 2: Detailed task results\n",
    "with open(\"sample_task_results_detailed.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Task\", \"Total Runs\", \"Successful\", \"Failed\", \"Success Rate %\", \"Failed Run Numbers\"])\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        for task_name, stats in sorted(task_results[model].items()):\n",
    "            success_rate = (stats[\"success\"] / stats[\"total\"] * 100) if stats[\"total\"] > 0 else 0\n",
    "            failed_runs = [r for r in stats[\"runs\"] if not r[\"success\"]]\n",
    "            failed_run_nums = \", \".join([r[\"run_number\"] for r in failed_runs])\n",
    "            \n",
    "            writer.writerow([\n",
    "                model,\n",
    "                task_name,\n",
    "                stats[\"total\"],\n",
    "                stats[\"success\"],\n",
    "                stats[\"total\"] - stats[\"success\"],\n",
    "                f\"{success_rate:.1f}\",\n",
    "                failed_run_nums\n",
    "            ])\n",
    "\n",
    "print(\"✅ Exported: sample_task_results_detailed.csv\")\n",
    "\n",
    "# Export 3: Per-run details\n",
    "with open(\"sample_run_details.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Task\", \"Run Number\", \"Success\", \"File Path\"])\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        for task_name, stats in sorted(task_results[model].items()):\n",
    "            for run in stats[\"runs\"]:\n",
    "                writer.writerow([\n",
    "                    model,\n",
    "                    task_name,\n",
    "                    run[\"run_number\"],\n",
    "                    \"Yes\" if run[\"success\"] else \"No\",\n",
    "                    run[\"file\"]\n",
    "                ])\n",
    "\n",
    "print(\"✅ Exported: sample_run_details.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPORT COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  1. sample_model_summary.csv - Overall model statistics\")\n",
    "print(f\"  2. sample_task_results_detailed.csv - Success rates per task per model\")\n",
    "print(f\"  3. sample_run_details.csv - Individual run details\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ae7685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE DATA ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Dataset: results/Sample-Harness_1/\n",
      "Total files analyzed: 160\n",
      "\n",
      "Models analyzed: 4\n",
      "  - claude-sonnet-4-5\n",
      "  - gemini-2.5-pro\n",
      "  - gpt-5\n",
      "  - grok-4\n",
      "\n",
      "================================================================================\n",
      "SUCCESS RATES BY MODEL (sorted)\n",
      "================================================================================\n",
      "\n",
      "claude-sonnet-4-5: 100.00% (40/40)\n",
      "grok-4: 82.50% (33/40)\n",
      "gpt-5: 75.00% (30/40)\n",
      "gemini-2.5-pro: 45.00% (18/40)\n",
      "\n",
      "================================================================================\n",
      "KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  - Total unique tasks: 5\n",
      "  - Tasks with at least one failure: 0\n",
      "  - Perfect tasks (all runs passed): 5\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  - Total unique tasks: 5\n",
      "  - Tasks with at least one failure: 5\n",
      "  - Perfect tasks (all runs passed): 0\n",
      "\n",
      "gpt-5:\n",
      "  - Total unique tasks: 5\n",
      "  - Tasks with at least one failure: 4\n",
      "  - Perfect tasks (all runs passed): 1\n",
      "\n",
      "grok-4:\n",
      "  - Total unique tasks: 5\n",
      "  - Tasks with at least one failure: 3\n",
      "  - Perfect tasks (all runs passed): 2\n",
      "\n",
      "================================================================================\n",
      "All CSV files have been exported with 'sample_' prefix.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"SAMPLE DATA ANALYSIS SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Dataset: results/Sample-Harness_1/\")\n",
    "print(f\"Total files analyzed: {len(files)}\\n\")\n",
    "\n",
    "print(f\"Models analyzed: {len(task_results)}\")\n",
    "for model in sorted(task_results.keys()):\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUCCESS RATES BY MODEL (sorted)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for model, stats in sorted(model_success.items(), key=lambda x: x[1]['success_rate'], reverse=True):\n",
    "    print(f\"{model}: {stats['success_rate']:.2f}% ({stats['success']}/{stats['total']})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"KEY FINDINGS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Count tasks with issues per model\n",
    "for model in sorted(task_results.keys()):\n",
    "    failed_tasks = sum(1 for stats in task_results[model].values() if stats[\"success\"] < stats[\"total\"])\n",
    "    total_tasks = len(task_results[model])\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  - Total unique tasks: {total_tasks}\")\n",
    "    print(f\"  - Tasks with at least one failure: {failed_tasks}\")\n",
    "    print(f\"  - Perfect tasks (all runs passed): {total_tasks - failed_tasks}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"All CSV files have been exported with 'sample_' prefix.\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "386979cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT DIFFICULTY CATEGORIZATION BY MODEL\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CLAUDE-SONNET-4-5\n",
      "================================================================================\n",
      "\n",
      "✅ EASY (0-1 failures): 5 prompts\n",
      "⚠️  MEDIUM (2-4 failures): 0 prompts\n",
      "❌ HARD (5+ failures): 0 prompts\n",
      "\n",
      "Total prompts: 5\n",
      "\n",
      "Distribution:\n",
      "  Easy: 100.0%\n",
      "  Medium: 0.0%\n",
      "  Hard: 0.0%\n",
      "\n",
      "================================================================================\n",
      "GEMINI-2.5-PRO\n",
      "================================================================================\n",
      "\n",
      "✅ EASY (0-1 failures): 0 prompts\n",
      "⚠️  MEDIUM (2-4 failures): 4 prompts\n",
      "❌ HARD (5+ failures): 1 prompts\n",
      "\n",
      "Total prompts: 5\n",
      "\n",
      "Distribution:\n",
      "  Easy: 0.0%\n",
      "  Medium: 80.0%\n",
      "  Hard: 20.0%\n",
      "\n",
      "❌ HARD prompts (up to 5):\n",
      "  - task2-google\n",
      "    Failures: 8/8\n",
      "\n",
      "⚠️  MEDIUM prompts (up to 3):\n",
      "  - task4-google\n",
      "    Failures: 4/8\n",
      "  - task3-google\n",
      "    Failures: 4/8\n",
      "  - task5-google\n",
      "    Failures: 3/8\n",
      "\n",
      "================================================================================\n",
      "GPT-5\n",
      "================================================================================\n",
      "\n",
      "✅ EASY (0-1 failures): 3 prompts\n",
      "⚠️  MEDIUM (2-4 failures): 1 prompts\n",
      "❌ HARD (5+ failures): 1 prompts\n",
      "\n",
      "Total prompts: 5\n",
      "\n",
      "Distribution:\n",
      "  Easy: 60.0%\n",
      "  Medium: 20.0%\n",
      "  Hard: 20.0%\n",
      "\n",
      "❌ HARD prompts (up to 5):\n",
      "  - task4-openai\n",
      "    Failures: 5/8\n",
      "\n",
      "⚠️  MEDIUM prompts (up to 3):\n",
      "  - task1-openai\n",
      "    Failures: 3/8\n",
      "\n",
      "================================================================================\n",
      "GROK-4\n",
      "================================================================================\n",
      "\n",
      "✅ EASY (0-1 failures): 3 prompts\n",
      "⚠️  MEDIUM (2-4 failures): 2 prompts\n",
      "❌ HARD (5+ failures): 0 prompts\n",
      "\n",
      "Total prompts: 5\n",
      "\n",
      "Distribution:\n",
      "  Easy: 60.0%\n",
      "  Medium: 40.0%\n",
      "  Hard: 0.0%\n",
      "\n",
      "⚠️  MEDIUM prompts (up to 3):\n",
      "  - task4-xai\n",
      "    Failures: 3/8\n",
      "  - task1-xai\n",
      "    Failures: 3/8\n",
      "\n",
      "================================================================================\n",
      "SUMMARY ACROSS ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "Model                     Easy       Medium     Hard       Total     \n",
      "-----------------------------------------------------------------\n",
      "claude-sonnet-4-5         5          0          0          5         \n",
      "gemini-2.5-pro            0          4          1          5         \n",
      "gpt-5                     3          1          1          5         \n",
      "grok-4                    3          2          0          5         \n"
     ]
    }
   ],
   "source": [
    "# Categorize prompts by difficulty based on failure rate per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"PROMPT DIFFICULTY CATEGORIZATION BY MODEL\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Categories:\n",
    "# Easy: 0-1 failures (close to perfect success)\n",
    "# Medium: 2-4 failures (50-80% success range)\n",
    "# Hard: 5+ failures (more failures than successes)\n",
    "\n",
    "def categorize_difficulty(success_count, total_runs):\n",
    "    \"\"\"Categorize based on number of failures\"\"\"\n",
    "    failures = total_runs - success_count\n",
    "    if failures <= 1:\n",
    "        return \"easy\"\n",
    "    elif failures <= 4:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"hard\"\n",
    "\n",
    "# Structure: difficulty_by_model[model][category] = [list of tasks]\n",
    "difficulty_by_model = {}\n",
    "\n",
    "for model in sorted(task_results.keys()):\n",
    "    difficulty_by_model[model] = {\n",
    "        \"easy\": [],\n",
    "        \"medium\": [],\n",
    "        \"hard\": []\n",
    "    }\n",
    "    \n",
    "    for task_name, stats in task_results[model].items():\n",
    "        category = categorize_difficulty(stats[\"success\"], stats[\"total\"])\n",
    "        difficulty_by_model[model][category].append({\n",
    "            \"task\": task_name,\n",
    "            \"success\": stats[\"success\"],\n",
    "            \"total\": stats[\"total\"],\n",
    "            \"failures\": stats[\"total\"] - stats[\"success\"]\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "for model in sorted(difficulty_by_model.keys()):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    categories = difficulty_by_model[model]\n",
    "    \n",
    "    print(f\"✅ EASY (0-1 failures): {len(categories['easy'])} prompts\")\n",
    "    print(f\"⚠️  MEDIUM (2-4 failures): {len(categories['medium'])} prompts\")\n",
    "    print(f\"❌ HARD (5+ failures): {len(categories['hard'])} prompts\")\n",
    "    \n",
    "    total_prompts = len(categories['easy']) + len(categories['medium']) + len(categories['hard'])\n",
    "    print(f\"\\nTotal prompts: {total_prompts}\")\n",
    "    \n",
    "    if total_prompts > 0:\n",
    "        easy_pct = len(categories['easy']) / total_prompts * 100\n",
    "        medium_pct = len(categories['medium']) / total_prompts * 100\n",
    "        hard_pct = len(categories['hard']) / total_prompts * 100\n",
    "        \n",
    "        print(f\"\\nDistribution:\")\n",
    "        print(f\"  Easy: {easy_pct:.1f}%\")\n",
    "        print(f\"  Medium: {medium_pct:.1f}%\")\n",
    "        print(f\"  Hard: {hard_pct:.1f}%\")\n",
    "    \n",
    "    # Show some examples from each category\n",
    "    if categories['hard']:\n",
    "        print(f\"\\n❌ HARD prompts (up to 5):\")\n",
    "        for item in sorted(categories['hard'], key=lambda x: x['failures'], reverse=True)[:5]:\n",
    "            print(f\"  - {item['task']}\")\n",
    "            print(f\"    Failures: {item['failures']}/{item['total']}\")\n",
    "    \n",
    "    if categories['medium']:\n",
    "        print(f\"\\n⚠️  MEDIUM prompts (up to 3):\")\n",
    "        for item in sorted(categories['medium'], key=lambda x: x['failures'], reverse=True)[:3]:\n",
    "            print(f\"  - {item['task']}\")\n",
    "            print(f\"    Failures: {item['failures']}/{item['total']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY ACROSS ALL MODELS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Aggregate summary\n",
    "summary_table = []\n",
    "for model in sorted(difficulty_by_model.keys()):\n",
    "    categories = difficulty_by_model[model]\n",
    "    total = len(categories['easy']) + len(categories['medium']) + len(categories['hard'])\n",
    "    summary_table.append({\n",
    "        \"model\": model,\n",
    "        \"easy\": len(categories['easy']),\n",
    "        \"medium\": len(categories['medium']),\n",
    "        \"hard\": len(categories['hard']),\n",
    "        \"total\": total\n",
    "    })\n",
    "\n",
    "# Print as table\n",
    "print(f\"{'Model':<25} {'Easy':<10} {'Medium':<10} {'Hard':<10} {'Total':<10}\")\n",
    "print(f\"{'-'*65}\")\n",
    "for row in summary_table:\n",
    "    print(f\"{row['model']:<25} {row['easy']:<10} {row['medium']:<10} {row['hard']:<10} {row['total']:<10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08ebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PERFECT TASKS ANALYSIS (All runs passing)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  Tasks with perfect success: 5/5\n",
      "  Percentage: 100.0%\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  Tasks with perfect success: 0/5\n",
      "  Percentage: 0.0%\n",
      "\n",
      "gpt-5:\n",
      "  Tasks with perfect success: 1/5\n",
      "  Percentage: 20.0%\n",
      "\n",
      "grok-4:\n",
      "  Tasks with perfect success: 2/5\n",
      "  Percentage: 40.0%\n",
      "\n",
      "================================================================================\n",
      "SUMMARY TABLE\n",
      "================================================================================\n",
      "\n",
      "Model                     Perfect Tasks        Total Tasks     Perfect %      \n",
      "---------------------------------------------------------------------------\n",
      "claude-sonnet-4-5         5                    5               100.0%\n",
      "gemini-2.5-pro            0                    5               0.0%\n",
      "gpt-5                     1                    5               20.0%\n",
      "grok-4                    2                    5               40.0%\n",
      "\n",
      "================================================================================\n",
      "TASKS PERFECT FOR ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "Tasks with perfect success across ALL models: 0/20\n",
      "Percentage: 0.0%\n",
      "\n",
      "No tasks are perfect across all models.\n",
      "\n",
      "================================================================================\n",
      "TASKS WITH MIXED PERFECT SCORES\n",
      "================================================================================\n",
      "\n",
      "Tasks with mixed perfect scores: 0\n",
      "\n",
      "No tasks with mixed perfect scores found.\n"
     ]
    }
   ],
   "source": [
    "# Find tasks with perfect success rate per model\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"PERFECT TASKS ANALYSIS (All runs passing)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "perfect_tasks_by_model = {}\n",
    "\n",
    "for model in sorted(task_results.keys()):\n",
    "    perfect_tasks = []\n",
    "    \n",
    "    for task_name, stats in task_results[model].items():\n",
    "        # Perfect means all runs succeeded\n",
    "        if stats[\"success\"] == stats[\"total\"] and stats[\"total\"] > 0:\n",
    "            perfect_tasks.append(task_name)\n",
    "    \n",
    "    perfect_tasks_by_model[model] = perfect_tasks\n",
    "    \n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  Tasks with perfect success: {len(perfect_tasks)}/{len(task_results[model])}\")\n",
    "    if len(task_results[model]) > 0:\n",
    "        print(f\"  Percentage: {len(perfect_tasks)/len(task_results[model])*100:.1f}%\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY TABLE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"{'Model':<25} {'Perfect Tasks':<20} {'Total Tasks':<15} {'Perfect %':<15}\")\n",
    "print(f\"{'-'*75}\")\n",
    "\n",
    "for model in sorted(perfect_tasks_by_model.keys()):\n",
    "    perfect_count = len(perfect_tasks_by_model[model])\n",
    "    total_count = len(task_results[model])\n",
    "    perfect_pct = perfect_count / total_count * 100 if total_count > 0 else 0\n",
    "    \n",
    "    print(f\"{model:<25} {perfect_count:<20} {total_count:<15} {perfect_pct:.1f}%\")\n",
    "\n",
    "# Compare which tasks are perfect across all models\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TASKS PERFECT FOR ALL MODELS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "all_tasks_set = set()\n",
    "for model in task_results.keys():\n",
    "    all_tasks_set.update(task_results[model].keys())\n",
    "\n",
    "perfect_for_all = []\n",
    "for task in all_tasks_set:\n",
    "    is_perfect_for_all = True\n",
    "    \n",
    "    for model in task_results.keys():\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            if stats[\"success\"] != stats[\"total\"] or stats[\"total\"] == 0:\n",
    "                is_perfect_for_all = False\n",
    "                break\n",
    "        else:\n",
    "            is_perfect_for_all = False\n",
    "            break\n",
    "    \n",
    "    if is_perfect_for_all:\n",
    "        perfect_for_all.append(task)\n",
    "\n",
    "print(f\"Tasks with perfect success across ALL models: {len(perfect_for_all)}/{len(all_tasks_set)}\")\n",
    "if len(all_tasks_set) > 0:\n",
    "    print(f\"Percentage: {len(perfect_for_all)/len(all_tasks_set)*100:.1f}%\\n\")\n",
    "\n",
    "if perfect_for_all:\n",
    "    print(f\"These tasks are perfect for all models:\")\n",
    "    for i, task in enumerate(sorted(perfect_for_all), 1):\n",
    "        print(f\"  {i}. {task}\")\n",
    "else:\n",
    "    print(\"No tasks are perfect across all models.\")\n",
    "\n",
    "# Find tasks that are perfect for some models but not all\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TASKS WITH MIXED PERFECT SCORES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "mixed_perfect = {}\n",
    "for task in all_tasks_set:\n",
    "    perfect_models = []\n",
    "    imperfect_models = []\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            if stats[\"success\"] == stats[\"total\"] and stats[\"total\"] > 0:\n",
    "                perfect_models.append(model)\n",
    "            else:\n",
    "                imperfect_models.append(model)\n",
    "    \n",
    "    # Only include if some (but not all) models have perfect score\n",
    "    if perfect_models and imperfect_models:\n",
    "        mixed_perfect[task] = {\n",
    "            \"perfect\": perfect_models,\n",
    "            \"imperfect\": imperfect_models\n",
    "        }\n",
    "\n",
    "print(f\"Tasks with mixed perfect scores: {len(mixed_perfect)}\\n\")\n",
    "\n",
    "if mixed_perfect:\n",
    "    # Show top examples (tasks where most models are perfect)\n",
    "    mixed_sorted = sorted(mixed_perfect.items(), \n",
    "                         key=lambda x: len(x[1][\"perfect\"]), \n",
    "                         reverse=True)\n",
    "    \n",
    "    print(f\"Top examples (most models perfect):\")\n",
    "    for i, (task, data) in enumerate(mixed_sorted[:10], 1):\n",
    "        print(f\"\\n{i}. {task}\")\n",
    "        print(f\"   Perfect for ({len(data['perfect'])}/{len(task_results)}): {', '.join(data['perfect'])}\")\n",
    "        print(f\"   Not perfect for: {', '.join(data['imperfect'])}\")\n",
    "else:\n",
    "    print(\"No tasks with mixed perfect scores found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1034f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE FAILURE ANALYSIS (All runs failing)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "claude-sonnet-4-5:\n",
      "  Tasks with complete failure: 0/5\n",
      "  Percentage: 0.0%\n",
      "\n",
      "gemini-2.5-pro:\n",
      "  Tasks with complete failure: 1/5\n",
      "  Percentage: 20.0%\n",
      "\n",
      "  Failed tasks:\n",
      "    - task2-google\n",
      "      All 8 run(s) failed\n",
      "\n",
      "gpt-5:\n",
      "  Tasks with complete failure: 0/5\n",
      "  Percentage: 0.0%\n",
      "\n",
      "grok-4:\n",
      "  Tasks with complete failure: 0/5\n",
      "  Percentage: 0.0%\n",
      "\n",
      "================================================================================\n",
      "COMPLETE FAILURE SUMMARY TABLE\n",
      "================================================================================\n",
      "\n",
      "Model                     Failed Tasks         Total Tasks     Failure %      \n",
      "---------------------------------------------------------------------------\n",
      "claude-sonnet-4-5         0                    5               0.0%\n",
      "gemini-2.5-pro            1                    5               20.0%\n",
      "gpt-5                     0                    5               0.0%\n",
      "grok-4                    0                    5               0.0%\n",
      "\n",
      "================================================================================\n",
      "TASKS THAT FAIL FOR ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "Tasks that fail across ALL models: 0/20\n",
      "Percentage: 0.0%\n",
      "\n",
      "✅ No tasks fail across all models.\n",
      "\n",
      "================================================================================\n",
      "TASKS WITH MIXED FAILURE PATTERNS\n",
      "================================================================================\n",
      "\n",
      "Tasks with mixed failure patterns: 0\n",
      "\n",
      "No tasks with mixed failure patterns found.\n"
     ]
    }
   ],
   "source": [
    "# Find tasks where ALL runs fail (complete failure)\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"COMPLETE FAILURE ANALYSIS (All runs failing)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "failed_tasks_by_model = {}\n",
    "\n",
    "for model in sorted(task_results.keys()):\n",
    "    failed_tasks = []\n",
    "    \n",
    "    for task_name, stats in task_results[model].items():\n",
    "        # Complete failure means zero successes\n",
    "        if stats[\"success\"] == 0 and stats[\"total\"] > 0:\n",
    "            failed_tasks.append({\n",
    "                \"task\": task_name,\n",
    "                \"runs\": stats[\"total\"]\n",
    "            })\n",
    "    \n",
    "    failed_tasks_by_model[model] = failed_tasks\n",
    "    \n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  Tasks with complete failure: {len(failed_tasks)}/{len(task_results[model])}\")\n",
    "    if len(task_results[model]) > 0:\n",
    "        print(f\"  Percentage: {len(failed_tasks)/len(task_results[model])*100:.1f}%\")\n",
    "    \n",
    "    if failed_tasks:\n",
    "        print(f\"\\n  Failed tasks:\")\n",
    "        for item in sorted(failed_tasks, key=lambda x: x['runs'], reverse=True):\n",
    "            print(f\"    - {item['task']}\")\n",
    "            print(f\"      All {item['runs']} run(s) failed\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPLETE FAILURE SUMMARY TABLE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"{'Model':<25} {'Failed Tasks':<20} {'Total Tasks':<15} {'Failure %':<15}\")\n",
    "print(f\"{'-'*75}\")\n",
    "\n",
    "for model in sorted(failed_tasks_by_model.keys()):\n",
    "    failed_count = len(failed_tasks_by_model[model])\n",
    "    total_count = len(task_results[model])\n",
    "    failed_pct = failed_count / total_count * 100 if total_count > 0 else 0\n",
    "    \n",
    "    print(f\"{model:<25} {failed_count:<20} {total_count:<15} {failed_pct:.1f}%\")\n",
    "\n",
    "# Find tasks that fail for ALL models\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TASKS THAT FAIL FOR ALL MODELS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "all_tasks_set = set()\n",
    "for model in task_results.keys():\n",
    "    all_tasks_set.update(task_results[model].keys())\n",
    "\n",
    "failed_for_all = []\n",
    "for task in all_tasks_set:\n",
    "    is_failed_for_all = True\n",
    "    \n",
    "    for model in task_results.keys():\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            if stats[\"success\"] > 0 or stats[\"total\"] == 0:\n",
    "                is_failed_for_all = False\n",
    "                break\n",
    "        else:\n",
    "            is_failed_for_all = False\n",
    "            break\n",
    "    \n",
    "    if is_failed_for_all:\n",
    "        failed_for_all.append(task)\n",
    "\n",
    "print(f\"Tasks that fail across ALL models: {len(failed_for_all)}/{len(all_tasks_set)}\")\n",
    "if len(all_tasks_set) > 0:\n",
    "    print(f\"Percentage: {len(failed_for_all)/len(all_tasks_set)*100:.1f}%\\n\")\n",
    "\n",
    "if failed_for_all:\n",
    "    print(f\"⚠️  These tasks fail for ALL models:\")\n",
    "    for i, task in enumerate(sorted(failed_for_all), 1):\n",
    "        print(f\"  {i}. {task}\")\n",
    "        # Show how many runs failed for each model\n",
    "        print(f\"     Failures per model:\")\n",
    "        for model in sorted(task_results.keys()):\n",
    "            if task in task_results[model]:\n",
    "                total = task_results[model][task][\"total\"]\n",
    "                print(f\"       - {model}: {total}/{total} runs failed\")\n",
    "else:\n",
    "    print(\"✅ No tasks fail across all models.\")\n",
    "\n",
    "# Find tasks that fail for some models\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TASKS WITH MIXED FAILURE PATTERNS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "mixed_failure = {}\n",
    "for task in all_tasks_set:\n",
    "    failed_models = []\n",
    "    passed_models = []\n",
    "    \n",
    "    for model in sorted(task_results.keys()):\n",
    "        if task in task_results[model]:\n",
    "            stats = task_results[model][task]\n",
    "            if stats[\"success\"] == 0 and stats[\"total\"] > 0:\n",
    "                failed_models.append(model)\n",
    "            elif stats[\"success\"] > 0:\n",
    "                passed_models.append(model)\n",
    "    \n",
    "    # Only include if some (but not all) models have complete failure\n",
    "    if failed_models and passed_models:\n",
    "        mixed_failure[task] = {\n",
    "            \"failed\": failed_models,\n",
    "            \"passed\": passed_models\n",
    "        }\n",
    "\n",
    "print(f\"Tasks with mixed failure patterns: {len(mixed_failure)}\\n\")\n",
    "\n",
    "if mixed_failure:\n",
    "    # Show tasks that fail for most models\n",
    "    mixed_sorted = sorted(mixed_failure.items(), \n",
    "                         key=lambda x: len(x[1][\"failed\"]), \n",
    "                         reverse=True)\n",
    "    \n",
    "    print(f\"Tasks that fail for multiple models (showing up to 10):\")\n",
    "    for i, (task, data) in enumerate(mixed_sorted[:10], 1):\n",
    "        print(f\"\\n{i}. {task}\")\n",
    "        print(f\"   ❌ Failed for ({len(data['failed'])}/{len(task_results)}): {', '.join(data['failed'])}\")\n",
    "        print(f\"   ✅ Has successes for: {', '.join(data['passed'])}\")\n",
    "else:\n",
    "    print(\"No tasks with mixed failure patterns found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b7501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING ADDITIONAL ANALYSIS TO CSV\n",
      "================================================================================\n",
      "\n",
      "✅ Exported: sample_difficulty_summary.csv\n",
      "✅ Exported: sample_difficulty_detailed.csv\n",
      "✅ Exported: sample_perfect_tasks.csv\n",
      "✅ Exported: sample_perfect_tasks_list.csv\n",
      "✅ Exported: sample_complete_failures.csv\n",
      "✅ Exported: sample_failed_tasks_list.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mixed_sorted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 134\u001b[39m\n\u001b[32m    131\u001b[39m     writer.writerow([\u001b[33m\"\u001b[39m\u001b[33m=== MIXED PATTERNS ===\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    132\u001b[39m     writer.writerow([\u001b[33m\"\u001b[39m\u001b[33mType\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPerfect Models\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mImperfect/Failed Models\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m task, data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmixed_sorted\u001b[49m:\n\u001b[32m    135\u001b[39m         writer.writerow([\n\u001b[32m    136\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMixed perfect\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    137\u001b[39m             task,\n\u001b[32m    138\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(data[\u001b[33m\"\u001b[39m\u001b[33mperfect\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    139\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(data[\u001b[33m\"\u001b[39m\u001b[33mimperfect\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    140\u001b[39m         ])\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Exported: sample_universal_analysis.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'mixed_sorted' is not defined"
     ]
    }
   ],
   "source": [
    "# Export additional analysis to CSV\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"EXPORTING ADDITIONAL ANALYSIS TO CSV\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Export 1: Difficulty categorization summary\n",
    "with open(\"sample_difficulty_summary.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Easy (0-1 failures)\", \"Medium (2-4 failures)\", \"Hard (5+ failures)\", \"Total Prompts\", \"Easy %\", \"Medium %\", \"Hard %\"])\n",
    "    \n",
    "    for model in sorted(difficulty_by_model.keys()):\n",
    "        categories = difficulty_by_model[model]\n",
    "        total = len(categories['easy']) + len(categories['medium']) + len(categories['hard'])\n",
    "        \n",
    "        easy_pct = len(categories['easy']) / total * 100 if total > 0 else 0\n",
    "        medium_pct = len(categories['medium']) / total * 100 if total > 0 else 0\n",
    "        hard_pct = len(categories['hard']) / total * 100 if total > 0 else 0\n",
    "        \n",
    "        writer.writerow([\n",
    "            model,\n",
    "            len(categories['easy']),\n",
    "            len(categories['medium']),\n",
    "            len(categories['hard']),\n",
    "            total,\n",
    "            f\"{easy_pct:.1f}\",\n",
    "            f\"{medium_pct:.1f}\",\n",
    "            f\"{hard_pct:.1f}\"\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: sample_difficulty_summary.csv\")\n",
    "\n",
    "# Export 2: Detailed difficulty per task\n",
    "with open(\"sample_difficulty_detailed.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Task\", \"Difficulty\", \"Successes\", \"Failures\", \"Total Runs\", \"Success Rate %\"])\n",
    "    \n",
    "    for model in sorted(difficulty_by_model.keys()):\n",
    "        for category in ['easy', 'medium', 'hard']:\n",
    "            for item in sorted(difficulty_by_model[model][category], key=lambda x: x['failures'], reverse=True):\n",
    "                success_rate = (item['success'] / item['total'] * 100) if item['total'] > 0 else 0\n",
    "                writer.writerow([\n",
    "                    model,\n",
    "                    item['task'],\n",
    "                    category.upper(),\n",
    "                    item['success'],\n",
    "                    item['failures'],\n",
    "                    item['total'],\n",
    "                    f\"{success_rate:.1f}\"\n",
    "                ])\n",
    "\n",
    "print(\"✅ Exported: sample_difficulty_detailed.csv\")\n",
    "\n",
    "# Export 3: Perfect tasks summary\n",
    "with open(\"sample_perfect_tasks.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Perfect Tasks\", \"Total Tasks\", \"Perfect %\", \"Imperfect Tasks\"])\n",
    "    \n",
    "    for model in sorted(perfect_tasks_by_model.keys()):\n",
    "        perfect_count = len(perfect_tasks_by_model[model])\n",
    "        total_count = len(task_results[model])\n",
    "        imperfect_count = total_count - perfect_count\n",
    "        perfect_pct = perfect_count / total_count * 100 if total_count > 0 else 0\n",
    "        \n",
    "        writer.writerow([\n",
    "            model,\n",
    "            perfect_count,\n",
    "            total_count,\n",
    "            f\"{perfect_pct:.1f}\",\n",
    "            imperfect_count\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: sample_perfect_tasks.csv\")\n",
    "\n",
    "# Export 4: Perfect tasks list\n",
    "with open(\"sample_perfect_tasks_list.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Task Name\"])\n",
    "    \n",
    "    for model in sorted(perfect_tasks_by_model.keys()):\n",
    "        for task in sorted(perfect_tasks_by_model[model]):\n",
    "            writer.writerow([model, task])\n",
    "\n",
    "print(\"✅ Exported: sample_perfect_tasks_list.csv\")\n",
    "\n",
    "# Export 5: Complete failure summary\n",
    "with open(\"sample_complete_failures.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Failed Tasks\", \"Total Tasks\", \"Failure %\"])\n",
    "    \n",
    "    for model in sorted(failed_tasks_by_model.keys()):\n",
    "        failed_count = len(failed_tasks_by_model[model])\n",
    "        total_count = len(task_results[model])\n",
    "        failed_pct = failed_count / total_count * 100 if total_count > 0 else 0\n",
    "        \n",
    "        writer.writerow([\n",
    "            model,\n",
    "            failed_count,\n",
    "            total_count,\n",
    "            f\"{failed_pct:.1f}\"\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: sample_complete_failures.csv\")\n",
    "\n",
    "# Export 6: Failed tasks list\n",
    "with open(\"sample_failed_tasks_list.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Task Name\", \"Total Runs Failed\"])\n",
    "    \n",
    "    for model in sorted(failed_tasks_by_model.keys()):\n",
    "        for item in sorted(failed_tasks_by_model[model], key=lambda x: x['runs'], reverse=True):\n",
    "            writer.writerow([model, item['task'], item['runs']])\n",
    "\n",
    "print(\"✅ Exported: sample_failed_tasks_list.csv\")\n",
    "\n",
    "# Export 7: Universal perfect and failed tasks\n",
    "with open(\"sample_universal_analysis.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow([\"Category\", \"Task Name\", \"Details\"])\n",
    "    \n",
    "    writer.writerow([\"=== PERFECT FOR ALL MODELS ===\", \"\", \"\"])\n",
    "    for task in sorted(perfect_for_all):\n",
    "        writer.writerow([\"Perfect for all\", task, \"All models succeeded\"])\n",
    "    \n",
    "    writer.writerow([\"\", \"\", \"\"])\n",
    "    writer.writerow([\"=== FAILED FOR ALL MODELS ===\", \"\", \"\"])\n",
    "    for task in sorted(failed_for_all):\n",
    "        writer.writerow([\"Failed for all\", task, \"All models failed\"])\n",
    "    \n",
    "    writer.writerow([\"\", \"\", \"\"])\n",
    "    writer.writerow([\"=== MIXED PATTERNS ===\", \"\", \"\"])\n",
    "    writer.writerow([\"Type\", \"Task\", \"Perfect Models\", \"Imperfect/Failed Models\"])\n",
    "    \n",
    "    for task, data in mixed_sorted:\n",
    "        writer.writerow([\n",
    "            \"Mixed perfect\",\n",
    "            task,\n",
    "            \", \".join(data[\"perfect\"]),\n",
    "            \", \".join(data[\"imperfect\"])\n",
    "        ])\n",
    "\n",
    "print(\"✅ Exported: sample_universal_analysis.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL EXPORTS COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Generated additional analysis files:\")\n",
    "print(f\"  1. sample_difficulty_summary.csv - Difficulty distribution per model\")\n",
    "print(f\"  2. sample_difficulty_detailed.csv - Detailed difficulty categorization\")\n",
    "print(f\"  3. sample_perfect_tasks.csv - Perfect task summary\")\n",
    "print(f\"  4. sample_perfect_tasks_list.csv - List of all perfect tasks\")\n",
    "print(f\"  5. sample_complete_failures.csv - Complete failure summary\")\n",
    "print(f\"  6. sample_failed_tasks_list.csv - List of all failed tasks\")\n",
    "print(f\"  7. sample_universal_analysis.csv - Universal patterns across models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c57e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Final Summary\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"COMPREHENSIVE SAMPLE DATA ANALYSIS SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Dataset: results/Sample-Harness_1/\")\n",
    "print(f\"Total files analyzed: {len(files)}\\n\")\n",
    "\n",
    "print(f\"Models analyzed: {len(task_results)}\")\n",
    "for model in sorted(task_results.keys()):\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUCCESS RATES BY MODEL (sorted)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for model, stats in sorted(model_success.items(), key=lambda x: x[1]['success_rate'], reverse=True):\n",
    "    print(f\"{model}: {stats['success_rate']:.2f}% ({stats['success']}/{stats['total']})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"KEY FINDINGS BY MODEL\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for model in sorted(task_results.keys()):\n",
    "    total_tasks = len(task_results[model])\n",
    "    failed_tasks = sum(1 for stats in task_results[model].values() if stats[\"success\"] < stats[\"total\"])\n",
    "    perfect_tasks = len(perfect_tasks_by_model[model])\n",
    "    complete_failures = len(failed_tasks_by_model[model])\n",
    "    \n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  - Total unique tasks: {total_tasks}\")\n",
    "    print(f\"  - Perfect tasks (100% success): {perfect_tasks}\")\n",
    "    print(f\"  - Tasks with partial failures: {failed_tasks - complete_failures}\")\n",
    "    print(f\"  - Complete failures (0% success): {complete_failures}\")\n",
    "    \n",
    "    # Difficulty breakdown\n",
    "    easy = len(difficulty_by_model[model]['easy'])\n",
    "    medium = len(difficulty_by_model[model]['medium'])\n",
    "    hard = len(difficulty_by_model[model]['hard'])\n",
    "    print(f\"  - Difficulty: Easy={easy}, Medium={medium}, Hard={hard}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"UNIVERSAL PATTERNS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Tasks perfect for ALL models: {len(perfect_for_all)}/{len(all_tasks_set)}\")\n",
    "print(f\"Tasks failed for ALL models: {len(failed_for_all)}/{len(all_tasks_set)}\")\n",
    "print(f\"Tasks with mixed patterns: {len(mixed_perfect)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPORTED CSV FILES (all with 'sample_' prefix)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Basic Analysis:\")\n",
    "print(f\"  1. sample_model_summary.csv - Overall model statistics\")\n",
    "print(f\"  2. sample_task_results_detailed.csv - Per-task success rates\")\n",
    "print(f\"  3. sample_run_details.csv - Individual run details\")\n",
    "\n",
    "print(f\"\\nDifficulty Analysis:\")\n",
    "print(f\"  4. sample_difficulty_summary.csv - Difficulty distribution per model\")\n",
    "print(f\"  5. sample_difficulty_detailed.csv - Detailed difficulty categorization\")\n",
    "\n",
    "print(f\"\\nPerfect Tasks Analysis:\")\n",
    "print(f\"  6. sample_perfect_tasks.csv - Perfect task summary\")\n",
    "print(f\"  7. sample_perfect_tasks_list.csv - List of all perfect tasks\")\n",
    "\n",
    "print(f\"\\nFailure Analysis:\")\n",
    "print(f\"  8. sample_complete_failures.csv - Complete failure summary\")\n",
    "print(f\"  9. sample_failed_tasks_list.csv - List of all failed tasks\")\n",
    "\n",
    "print(f\"\\nCross-Model Patterns:\")\n",
    "print(f\"  10. sample_universal_analysis.csv - Universal patterns across models\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✅ ANALYSIS COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697542e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
